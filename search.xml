<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>archlinux-easy-setup-for-deep-learning</title>
    <url>/2022/11/19/System/archlinux-easy-setup-for-deep-learning/</url>
    <content><![CDATA[<h1 id="My-setup"><a href="#My-setup" class="headerlink" title="My setup"></a>My setup</h1><ul>
<li>CPU: AMD Ryzen 9 5900X</li>
<li>GPU: NVIDIA GeForce RTX 3090 FE</li>
<li>Archlinux 6.0.8-arch1-1</li>
<li>WM i3</li>
</ul>
<h1 id="Update-pacman-source"><a href="#Update-pacman-source" class="headerlink" title="Update pacman source"></a>Update pacman source</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo pacman -Syu</span><br></pre></td></tr></table></figure>

<h1 id="Nvidia-driver"><a href="#Nvidia-driver" class="headerlink" title="Nvidia driver"></a>Nvidia driver</h1><p><strong>Skip</strong> if you’ve installed properitary nvidia driver</p>
<ul>
<li>for those need to switch nouveu to nvidia, please check archlinux wiki for more information</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo pacman -S nvidia nvidia-utils</span><br><span class="line"><span class="comment"># or nvidia-dkms and linux-header for custom kernel</span></span><br></pre></td></tr></table></figure>

<h1 id="CUDA-amp-CUDNN"><a href="#CUDA-amp-CUDNN" class="headerlink" title="CUDA &amp; CUDNN"></a>CUDA &amp; CUDNN</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo pacman -S cuda cudnn</span><br></pre></td></tr></table></figure>

<p>If you havn’t set the path for dynamic&#x2F;shared library, it’s time to set it. Otherwise errors like <code># Unimplemented: DNN library is not found</code> might present when running CNN</p>
<p>Add the following line to either <code>~/.bashrc</code> or <code>~/.bash_profile</code> or <code>.profile</code> (last two are preferred)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">&quot;<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>:/usr/lib&quot;</span></span><br></pre></td></tr></table></figure>

<p>Verify the installation</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/cuda/extras/demo_suite</span><br><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: &quot;NVIDIA GeForce RTX 3090&quot;</span><br><span class="line">  CUDA Driver Version / Runtime Version          11.8 / 11.8</span><br><span class="line">  CUDA Capability Major/Minor version number:    8.6</span><br><span class="line">  Total amount of global memory:                 24265 MBytes (25443893248 bytes)</span><br><span class="line">  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores</span><br><span class="line">  GPU Max Clock rate:                            1695 MHz (1.70 GHz)</span><br><span class="line">  Memory Clock rate:                             9751 Mhz</span><br><span class="line">  Memory Bus Width:                              384-bit</span><br><span class="line">  L2 Cache Size:                                 6291456 bytes</span><br><span class="line">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class="line">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class="line">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class="line">  Total amount of constant memory:               65536 bytes</span><br><span class="line">  Total amount of shared memory per block:       49152 bytes</span><br><span class="line">  Total number of registers available per block: 65536</span><br><span class="line">  Warp size:                                     32</span><br><span class="line">  Maximum number of threads per multiprocessor:  1536</span><br><span class="line">  Maximum number of threads per block:           1024</span><br><span class="line">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">  Maximum memory pitch:                          2147483647 bytes</span><br><span class="line">  Texture alignment:                             512 bytes</span><br><span class="line">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">  Run time limit on kernels:                     Yes</span><br><span class="line">  Integrated GPU sharing Host Memory:            No</span><br><span class="line">  Support host page-locked memory mapping:       Yes</span><br><span class="line">  Alignment requirement for Surfaces:            Yes</span><br><span class="line">  Device has ECC support:                        Disabled</span><br><span class="line">  Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">  Device supports Compute Preemption:            Yes</span><br><span class="line">  Supports Cooperative Kernel Launch:            Yes</span><br><span class="line">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class="line">  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0</span><br><span class="line">  Compute Mode:</span><br><span class="line">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class="line"></span><br><span class="line">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090</span><br><span class="line">Result = PASS</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow-amp-Keras"><a href="#Tensorflow-amp-Keras" class="headerlink" title="Tensorflow &amp; Keras"></a>Tensorflow &amp; Keras</h1><p><code>python-tensorflow-opt-cuda</code> vs <code>python-tensorflow-cuda</code></p>
<ul>
<li>opt might be slightly faster as the package is optimized for certain intel cpu</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo pacman -S python-tensorflow-cuda keras</span><br></pre></td></tr></table></figure>

<h1 id="Try-it-out"><a href="#Try-it-out" class="headerlink" title="Try it out!"></a>Try it out!</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers  </span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models  </span><br><span class="line">model = models.Sequential()  </span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))  </span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))  </span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))  </span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))  </span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))  </span><br><span class="line">model.add(layers.Flatten())  </span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>))  </span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))  </span><br><span class="line"><span class="built_in">print</span>(model.summary())  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist  </span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical  </span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  </span><br><span class="line">  </span><br><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))  </span><br><span class="line">train_images = train_images.astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255</span>  </span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))  </span><br><span class="line">test_images = test_images.astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255</span>  </span><br><span class="line">train_labels = to_categorical(train_labels)  </span><br><span class="line">test_labels = to_categorical(test_labels)  </span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;rmsprop&#x27;</span>,  </span><br><span class="line">	loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,  </span><br><span class="line">	metrics=[<span class="string">&#x27;accuracy&#x27;</span>])  </span><br><span class="line">model.fit(train_images, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">64</span>)  </span><br><span class="line">  </span><br><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels)  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test_acc:&#x27;</span>, test_acc)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class="line">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class="line">2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</span><br><span class="line">2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class="line">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class="line">2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6</span><br><span class="line">2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.</span><br><span class="line">Model: &quot;sequential&quot;</span><br><span class="line">_________________________________________________________________</span><br><span class="line"> Layer (type)                Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line"> conv2d (Conv2D)             (None, 26, 26, 32)        320       </span><br><span class="line">                                                                 </span><br><span class="line"> max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         </span><br><span class="line"> )                                                               </span><br><span class="line">                                                                 </span><br><span class="line"> conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     </span><br><span class="line">                                                                 </span><br><span class="line"> max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         </span><br><span class="line"> 2D)                                                             </span><br><span class="line">                                                                 </span><br><span class="line"> conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     </span><br><span class="line">                                                                 </span><br><span class="line"> flatten (Flatten)           (None, 576)               0         </span><br><span class="line">                                                                 </span><br><span class="line"> dense (Dense)               (None, 64)                36928     </span><br><span class="line">                                                                 </span><br><span class="line"> dense_1 (Dense)             (None, 10)                650       </span><br><span class="line">                                                                 </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 93,322</span><br><span class="line">Trainable params: 93,322</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">_________________________________________________________________</span><br><span class="line">None</span><br><span class="line">Epoch 1/5</span><br><span class="line">2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600</span><br><span class="line">2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.</span><br><span class="line">938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469</span><br><span class="line">Epoch 2/5</span><br><span class="line">938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853</span><br><span class="line">Epoch 3/5</span><br><span class="line">938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902</span><br><span class="line">Epoch 4/5</span><br><span class="line">938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919</span><br><span class="line">Epoch 5/5</span><br><span class="line">938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940</span><br><span class="line">313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880</span><br><span class="line">test_acc: 0.9879999756813049</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>archlinux</tag>
        <tag>installation</tag>
        <tag>deep-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Emoji Not Showing Properly on Archlinux.md</title>
    <url>/2022/06/03/System/archlinux-emoji-display-problem/</url>
    <content><![CDATA[<p>The issue persisted for a long time. </p>
<p>Emojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today </p>
<p><a href="https://flammie.github.io/dotfiles/fontconfig.html">Original Solution</a></p>
<p>Insert the following lines to <code>/etc/font/fonts.conf</code> inside the <code>&lt;font-config&gt;</code> tag</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">match</span> <span class="attr">target</span>=<span class="string">&quot;font&quot;</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">test</span> <span class="attr">name</span>=<span class="string">&quot;family&quot;</span> <span class="attr">compare</span>=<span class="string">&quot;contains&quot;</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">string</span>&gt;</span>Emoji<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">test</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">edit</span> <span class="attr">name</span>=<span class="string">&quot;hinting&quot;</span> <span class="attr">mode</span>=<span class="string">&quot;assign&quot;</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">bool</span>&gt;</span>true<span class="tag">&lt;/<span class="name">bool</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">edit</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">edit</span> <span class="attr">name</span>=<span class="string">&quot;hintstyle&quot;</span> <span class="attr">mode</span>=<span class="string">&quot;assign&quot;</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">const</span>&gt;</span>hintslight<span class="tag">&lt;/<span class="name">const</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">edit</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">edit</span> <span class="attr">name</span>=<span class="string">&quot;embeddedbitmap&quot;</span> <span class="attr">mode</span>=<span class="string">&quot;assign&quot;</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">bool</span>&gt;</span>true<span class="tag">&lt;/<span class="name">bool</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">edit</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">match</span>&gt;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>System</category>
      </categories>
  </entry>
  <entry>
    <title>Missing or Oversized Zoom Window on Arch</title>
    <url>/2022/06/02/System/archlinux-zoom-oversized-window/</url>
    <content><![CDATA[<p>The zoom client installed from AUR (and also the one from flatpak) had two weird issues:</p>
<ol>
<li>the window didn’t show up</li>
<li>the window is oversize and cannot be rescaled.</li>
</ol>
<p>checked the terminal output (there’s no output) and the log file under <code>$HOME/.zoom/log</code> (nothing suspicious).</p>
<h1 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h1><p>After few google searches, I found a workaround</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">vim <span class="variable">$HOME</span>/.config/zoomus.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight toml"><table><tr><td class="code"><pre><span class="line"><span class="attr">autoScale</span>=<span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>I reinstalled the zoom (restart also works) and it worked</p>
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>archlinux</tag>
        <tag>zoom</tag>
        <tag>solved</tag>
      </tags>
  </entry>
  <entry>
    <title>Lazy Cat</title>
    <url>/2022/11/19/Gallery/Cat/Funny/</url>
    <content><![CDATA[<p><img data-src="/2022/11/18/Gallery/Cat/Funny/cateye.jpg"><br><img data-src="/2022/11/18/Gallery/Cat/Funny/greasycat.jpg"></p>
]]></content>
      <categories>
        <category>Gallery</category>
        <category>Cat</category>
      </categories>
  </entry>
  <entry>
    <title>Very Naive Neural Network in C++ Part 1 Matrix</title>
    <url>/2021/01/05/Code/Naive-Neural-Network-in-Cpp-1/</url>
    <content><![CDATA[<h1 id="Why-even-bother-writing-any-neural-networks-in-C"><a href="#Why-even-bother-writing-any-neural-networks-in-C" class="headerlink" title="Why even bother writing any neural networks in C++?"></a>Why even bother writing any neural networks in C++?</h1><p>For production purposes, C++ implementations are assumed to be faster since it can utilize the  hardware to their limit if coded properly. But in most cases, python is powerful enough, especially when the python community is much larger.<br>But sometime I wonder if I rely on those beautiful toolkits a bit too much. Imagine someday the python community collapses (purely fictional). Will it be awesome if you know how to code things in those “ancient” languages?</p>
<h1 id="A-2D-Matrix-Class"><a href="#A-2D-Matrix-Class" class="headerlink" title="A 2D Matrix Class"></a>A 2D Matrix Class</h1><p>The fundamental of any higher dimensional computation is matrix. There’re actually a lot of implementation, but I still want to reinvent the wheel just for fun.</p>
<p>Let’s create one </p>
<ol>
<li>to reduce the typing workload, I will overload most of the algebraic operators.</li>
<li>I need the dot operation and transpose</li>
<li>I also need a few getters and setters</li>
<li>I will put everything in a 2D <code>std::vector</code> <span id="more"></span>
<blockquote>
<p>It might be to avoid std container if I know how to handle the power of dynamic memory.</p>
</blockquote>
</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Matrix</span> &#123;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Matrix</span><span class="params">(std::vector&lt;std::vector&lt;<span class="type">double</span>&gt;&gt;)</span></span>;  </span><br><span class="line">    <span class="built_in">Matrix</span>(<span class="type">int</span> rows, <span class="type">int</span> cols);  </span><br><span class="line">    <span class="built_in">Matrix</span>(Matrix <span class="type">const</span> &amp;matrix) <span class="keyword">noexcept</span>;  </span><br><span class="line">    ~<span class="built_in">Matrix</span>();  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Operators overloading  </span></span><br><span class="line">    <span class="function"><span class="type">double</span> &amp;<span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> row, <span class="type">int</span> col)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> row, <span class="type">int</span> col)</span> <span class="type">const</span></span>;  </span><br><span class="line">  </span><br><span class="line">    Matrix <span class="keyword">operator</span>+(<span class="type">const</span> Matrix &amp;other) <span class="type">const</span>;  </span><br><span class="line">    Matrix <span class="keyword">operator</span>+(<span class="type">double</span> scalar) <span class="type">const</span>;  </span><br><span class="line">    Matrix <span class="keyword">operator</span>-(<span class="type">const</span> Matrix &amp;other) <span class="type">const</span>;  </span><br><span class="line">    Matrix <span class="keyword">operator</span>*(<span class="type">const</span> Matrix &amp;other) <span class="type">const</span>;  </span><br><span class="line">    Matrix <span class="keyword">operator</span>*(<span class="type">double</span> scalar) <span class="type">const</span>;  </span><br><span class="line">    Matrix <span class="keyword">operator</span>/(<span class="type">double</span> scalar) <span class="type">const</span>;  </span><br><span class="line">    Matrix &amp;<span class="keyword">operator</span>+=(<span class="type">const</span> Matrix &amp;other);  </span><br><span class="line">    Matrix &amp;<span class="keyword">operator</span>-=(<span class="type">const</span> Matrix &amp;other);  </span><br><span class="line">    Matrix &amp;<span class="keyword">operator</span>*=(<span class="type">double</span> scalar);  </span><br><span class="line">    Matrix &amp;<span class="keyword">operator</span>/=(<span class="type">double</span> scalar);  </span><br><span class="line">    Matrix &amp;<span class="keyword">operator</span>=(<span class="type">const</span> Matrix &amp;other);  </span><br><span class="line">    Matrix &amp;<span class="keyword">operator</span>=(Matrix &amp;&amp;other) <span class="keyword">noexcept</span>; </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Other computations  </span></span><br><span class="line">    <span class="function">Matrix <span class="title">dot</span><span class="params">(<span class="type">const</span> Matrix &amp;other)</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function">Matrix <span class="title">transpose</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="comment">//Fetching Operations  </span></span><br><span class="line">    <span class="function">Matrix <span class="title">getRow</span><span class="params">(<span class="type">int</span> row)</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function">Matrix <span class="title">getCol</span><span class="params">(<span class="type">int</span> col)</span> <span class="type">const</span></span>; </span><br><span class="line">    <span class="comment">// Getter  </span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">size</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">rows</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">cols</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">sum</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">mean</span><span class="params">()</span> <span class="type">const</span></span>; </span><br><span class="line">    <span class="function">std::pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; <span class="title">shape</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isScalar</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Setter  </span></span><br><span class="line">    <span class="function">Matrix <span class="title">changeAt</span><span class="params">(<span class="type">int</span> row, <span class="type">int</span> col, <span class="type">double</span> value)</span></span>;  </span><br><span class="line">    <span class="comment">// Manipulation  </span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addRow</span><span class="params">(<span class="type">const</span> Matrix &amp;row)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setRow</span><span class="params">(<span class="type">int</span> row, <span class="type">const</span> Matrix &amp;newRow)</span></span>;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Static functions  </span></span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">EmptyMatrix</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> &#123;<span class="number">0</span>, <span class="number">0</span>&#125;; &#125;;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Print  </span></span><br><span class="line">    <span class="keyword">friend</span> std::ostream &amp;<span class="keyword">operator</span>&lt;&lt;(std::ostream &amp;os, <span class="type">const</span> Matrix &amp;matrix);  </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">print</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function">std::string <span class="title">printableShape</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span>:  </span><br><span class="line">    std::vector&lt;std::vector&lt;<span class="type">double</span>&gt;&gt; data;  </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">checkIfSameDimension</span><span class="params">(<span class="type">const</span> Matrix &amp;other)</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">checkForDotMultiplication</span><span class="params">(<span class="type">const</span> Matrix &amp;other)</span> <span class="type">const</span></span>;  </span><br><span class="line">  </span><br><span class="line">&#125;;  </span><br><span class="line"><span class="meta">#<span class="keyword">endif</span> <span class="comment">//NAIVE_NN_MATRIX_H</span></span></span><br></pre></td></tr></table></figure>


<h1 id="Constructors"><a href="#Constructors" class="headerlink" title="Constructors"></a>Constructors</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Matrix.h&quot;</span>  </span></span><br><span class="line">Matrix::<span class="built_in">Matrix</span>(<span class="type">int</span> rows, <span class="type">int</span> cols)&#123;  </span><br><span class="line">    <span class="comment">// initialize the vector  </span></span><br><span class="line">    <span class="keyword">this</span>-&gt;data = std::vector&lt;std::vector&lt;<span class="type">double</span>&gt;&gt;(rows, std::<span class="built_in">vector</span>&lt;<span class="type">double</span>&gt;(cols, <span class="number">0</span>));  </span><br><span class="line">  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix::<span class="built_in">Matrix</span>(std::vector&lt;std::vector&lt;<span class="type">double</span>&gt;&gt; data) &#123;  </span><br><span class="line">    <span class="comment">// Copy data from the array to the matrix  </span></span><br><span class="line">    <span class="keyword">this</span>-&gt;data = std::<span class="built_in">move</span>(data);  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Matrix::<span class="built_in">Matrix</span>(Matrix <span class="type">const</span> &amp;matrix) <span class="keyword">noexcept</span> &#123;  </span><br><span class="line">    <span class="keyword">this</span>-&gt;data = matrix.data;  </span><br><span class="line">&#125;  </span><br><span class="line">Matrix::~<span class="built_in">Matrix</span>() = <span class="keyword">default</span>;  </span><br></pre></td></tr></table></figure>

<h1 id="Operators-overloading"><a href="#Operators-overloading" class="headerlink" title="Operators overloading"></a>Operators overloading</h1><ul>
<li>For the <code>*</code> multiplication, it is the similar to the broadcasting logic in numpy</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">double</span> &amp;<span class="title">Matrix::operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> data[row][col];  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Matrix::operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> row, <span class="type">int</span> col)</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> data[row][col];  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix Matrix::<span class="keyword">operator</span>+(<span class="type">const</span> Matrix &amp;other) <span class="type">const</span> &#123;  </span><br><span class="line">    <span class="built_in">checkIfSameDimension</span>(other);  </span><br><span class="line">    <span class="comment">// Add two matrices  </span></span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(other.<span class="built_in">rows</span>(), other.<span class="built_in">cols</span>());  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; other.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; other.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][j] + <span class="built_in">other</span>(i, j);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line">Matrix Matrix::<span class="keyword">operator</span>+(<span class="type">double</span> scalar) <span class="type">const</span> &#123;  </span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][j] + scalar;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125; </span><br><span class="line">  </span><br><span class="line">Matrix Matrix::<span class="keyword">operator</span>-(<span class="type">const</span> Matrix &amp;other) <span class="type">const</span> &#123;  </span><br><span class="line">    <span class="built_in">checkIfSameDimension</span>(other);  </span><br><span class="line">  </span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(other.<span class="built_in">rows</span>(), other.<span class="built_in">cols</span>());  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; other.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; other.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][j] - <span class="built_in">other</span>(i, j);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix Matrix::<span class="keyword">operator</span>*(<span class="type">const</span> Matrix &amp;other) <span class="type">const</span> &#123;  </span><br><span class="line">    <span class="comment">//Broadcasting  </span></span><br><span class="line">    <span class="comment">// If one of the matrix is a scalar, then we can just multiply the scalar with the other matrix    if (this-&gt;isScalar()) &#123;  </span></span><br><span class="line">        <span class="keyword">return</span> other * <span class="keyword">this</span>-&gt;data[<span class="number">0</span>][<span class="number">0</span>];  </span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (other.<span class="built_in">isScalar</span>()) &#123;  </span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span> * other.data[<span class="number">0</span>][<span class="number">0</span>];  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// If the two matrices has the same dimension, then we can just multiply them  </span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() == other.<span class="built_in">rows</span>() &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() == other.<span class="built_in">cols</span>()) &#123;  </span><br><span class="line">        Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">                <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][j] * <span class="built_in">other</span>(i, j);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">return</span> result;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// If the two matrices has the same number of rows and one of them has only one column, then we can just multiply them element-wise  </span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() == other.<span class="built_in">rows</span>() &amp;&amp; other.<span class="built_in">cols</span>() == <span class="number">1</span>) &#123;  </span><br><span class="line">        Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">                <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][j] * <span class="built_in">other</span>(i, <span class="number">0</span>);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">return</span> result;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() == other.<span class="built_in">rows</span>() &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() == <span class="number">1</span>) &#123;  </span><br><span class="line">        Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), other.<span class="built_in">cols</span>());  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; other.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">                <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][<span class="number">0</span>] * <span class="built_in">other</span>(i, j);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">return</span> result;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// If the two matrices has the same number of columns and one of them has only one row, then we can just multiply them element-wise  </span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() == other.<span class="built_in">cols</span>() &amp;&amp; other.<span class="built_in">rows</span>() == <span class="number">1</span>) &#123;  </span><br><span class="line">        Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">                <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][j] * <span class="built_in">other</span>(<span class="number">0</span>, j);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">return</span> result;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() == other.<span class="built_in">cols</span>() &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() == <span class="number">1</span>) &#123;  </span><br><span class="line">        Matrix result = <span class="built_in">Matrix</span>(other.<span class="built_in">rows</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; other.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">                <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[<span class="number">0</span>][j] * <span class="built_in">other</span>(i, j);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">return</span> result;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    std::cerr &lt;&lt; <span class="string">&quot;Shape mismatch: &quot;</span> &lt;&lt; <span class="keyword">this</span>-&gt;<span class="built_in">printableShape</span>() &lt;&lt; <span class="string">&quot; and &quot;</span> &lt;&lt; other.<span class="built_in">printableShape</span>() &lt;&lt; std::endl;  </span><br><span class="line">    <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(<span class="string">&quot;The two matrices cannot be multiplied&quot;</span>);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix Matrix::<span class="keyword">operator</span>*(<span class="type">double</span> scalar) <span class="type">const</span> &#123;  </span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][j] * scalar;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix Matrix::<span class="keyword">operator</span>/(<span class="type">double</span> scalar) <span class="type">const</span> &#123;  </span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="built_in">result</span>(i, j) = <span class="keyword">this</span>-&gt;data[i][j] / scalar;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix &amp;Matrix::<span class="keyword">operator</span>+=(<span class="type">const</span> Matrix &amp;other) &#123;  </span><br><span class="line">    <span class="built_in">checkIfSameDimension</span>(other);  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; other.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; other.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="keyword">this</span>-&gt;data[i][j] += <span class="built_in">other</span>(i, j);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix &amp;Matrix::<span class="keyword">operator</span>-=(<span class="type">const</span> Matrix &amp;other) &#123;  </span><br><span class="line">    <span class="built_in">checkIfSameDimension</span>(other);  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; other.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; other.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="keyword">this</span>-&gt;data[i][j] -= <span class="built_in">other</span>(i, j);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix &amp;Matrix::<span class="keyword">operator</span>*=(<span class="type">double</span> scalar) &#123;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="keyword">this</span>-&gt;data[i][j] *= scalar;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix &amp;Matrix::<span class="keyword">operator</span>/=(<span class="type">double</span> scalar) &#123;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="keyword">this</span>-&gt;data[i][j] /= scalar;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix &amp;Matrix::<span class="keyword">operator</span>=(<span class="type">const</span> Matrix &amp;other) = <span class="keyword">default</span>;  </span><br><span class="line">  </span><br><span class="line">Matrix &amp;Matrix::<span class="keyword">operator</span>=(Matrix &amp;&amp;other) <span class="keyword">noexcept</span> &#123;  </span><br><span class="line">    <span class="keyword">this</span>-&gt;data = std::<span class="built_in">move</span>(other.data);  </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Some-helper-methods"><a href="#Some-helper-methods" class="headerlink" title="Some helper methods"></a>Some helper methods</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Matrix::checkIfSameDimension</span><span class="params">(<span class="type">const</span> Matrix &amp;other)</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() != other.<span class="built_in">rows</span>() || <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() != other.<span class="built_in">cols</span>()) &#123;  </span><br><span class="line">        <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(<span class="string">&quot;Matrix dimensions do not match&quot;</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Matrix::checkForDotMultiplication</span><span class="params">(<span class="type">const</span> Matrix &amp;other)</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() != other.<span class="built_in">rows</span>()) &#123;  </span><br><span class="line">        <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(<span class="string">&quot;Matrix dimensions do not match&quot;</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<h1 id="Dot-and-transpose"><a href="#Dot-and-transpose" class="headerlink" title="Dot and transpose"></a>Dot and transpose</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Matrix <span class="title">Matrix::dot</span><span class="params">(<span class="type">const</span> Matrix &amp;other)</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="built_in">checkForDotMultiplication</span>(other);  </span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), other.<span class="built_in">cols</span>());  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; other.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="type">double</span> sum = <span class="number">0</span>;  </span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); k++) &#123;  </span><br><span class="line">                sum += <span class="keyword">this</span>-&gt;data[i][k] * <span class="built_in">other</span>(k, j);  </span><br><span class="line">            &#125;  </span><br><span class="line">            <span class="built_in">result</span>(i, j) = sum;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125; </span><br><span class="line"><span class="function">Matrix <span class="title">Matrix::transpose</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(), <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>());  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="built_in">result</span>(j, i) = <span class="keyword">this</span>-&gt;data[i][j];  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Basic-stats"><a href="#Basic-stats" class="headerlink" title="Basic stats"></a>Basic stats</h1><ul>
<li>sum</li>
<li>mean</li>
<li>size</li>
<li>shape</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Matrix::sum</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="type">double</span> sum = <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            sum += <span class="keyword">this</span>-&gt;data[i][j];  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> sum;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Matrix::mean</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">sum</span>() / <span class="keyword">this</span>-&gt;<span class="built_in">size</span>();  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Matrix::size</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() * <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>();  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Matrix::rows</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> (<span class="type">int</span>)data.<span class="built_in">size</span>();  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Matrix::cols</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span> (data.<span class="built_in">empty</span>()) &#123;  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> (<span class="type">int</span>)data[<span class="number">0</span>].<span class="built_in">size</span>();  </span><br><span class="line">  </span><br><span class="line"><span class="function">std::pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; <span class="title">Matrix::shape</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="built_in">rows</span>(), <span class="built_in">cols</span>()&#125;;  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>
<h1 id="Row-and-column-operations"><a href="#Row-and-column-operations" class="headerlink" title="Row and column operations"></a>Row and column operations</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Matrix <span class="title">Matrix::getRow</span><span class="params">(<span class="type">int</span> row)</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(<span class="number">1</span>, <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>());  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); i++) &#123;  </span><br><span class="line">        <span class="built_in">result</span>(<span class="number">0</span>, i) = <span class="keyword">this</span>-&gt;data[row][i];  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">Matrix::getCol</span><span class="params">(<span class="type">int</span> col)</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    Matrix result = <span class="built_in">Matrix</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(), <span class="number">1</span>);  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="built_in">result</span>(i, <span class="number">0</span>) = <span class="keyword">this</span>-&gt;data[i][col];  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Matrix::addRow</span><span class="params">(<span class="type">const</span> Matrix &amp;row)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span> ((row.<span class="built_in">cols</span>() != <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() &gt; <span class="number">0</span>)|| row.<span class="built_in">rows</span>() &lt; <span class="number">1</span>) &#123;  </span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;Shape mismatch: &quot;</span> &lt;&lt; <span class="keyword">this</span>-&gt;<span class="built_in">printableShape</span>() &lt;&lt; <span class="string">&quot; and &quot;</span> &lt;&lt; row.<span class="built_in">printableShape</span>() &lt;&lt; std::endl;  </span><br><span class="line">        <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(<span class="string">&quot;Row dimensions do not match&quot;</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">this</span>-&gt;data.<span class="built_in">push_back</span>(row.data[<span class="number">0</span>]);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Matrix::setRow</span><span class="params">(<span class="type">int</span> row, <span class="type">const</span> Matrix &amp;newRow)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">if</span> (newRow.<span class="built_in">cols</span>() != <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>()) &#123;  </span><br><span class="line">        <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(<span class="string">&quot;Row dimensions do not match&quot;</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">        <span class="keyword">this</span>-&gt;data[row][j] = <span class="built_in">newRow</span>(<span class="number">0</span>, j);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">Matrix::changeAt</span><span class="params">(<span class="type">int</span> row, <span class="type">int</span> col, <span class="type">double</span> value)</span> </span>&#123;  </span><br><span class="line">    <span class="comment">// check if row and col are in range  </span></span><br><span class="line">    <span class="keyword">if</span> (row &gt;= <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() || col &gt;= <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>()) &#123;  </span><br><span class="line">        <span class="comment">// print range  </span></span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Row: &quot;</span> &lt;&lt; row &lt;&lt; <span class="string">&quot; Col: &quot;</span> &lt;&lt; col &lt;&lt; std::endl;  </span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Max Rows: &quot;</span> &lt;&lt; <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() &lt;&lt; <span class="string">&quot; Max Cols: &quot;</span> &lt;&lt; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() &lt;&lt; std::endl;  </span><br><span class="line">        <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(<span class="string">&quot;Row or column out of range&quot;</span>);  </span><br><span class="line">  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">this</span>-&gt;data[row][col] = value;  </span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<h1 id="Formatted-Output"><a href="#Formatted-Output" class="headerlink" title="Formatted Output"></a>Formatted Output</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// To string  </span></span><br><span class="line">std::ostream &amp;<span class="keyword">operator</span>&lt;&lt;(std::ostream &amp;os, <span class="type">const</span> Matrix &amp;matrix) &#123;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; matrix.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; matrix.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            os &lt;&lt; matrix.data[i][j] &lt;&lt; <span class="string">&quot;, &quot;</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">        os &lt;&lt; std::endl;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> os;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Matrix::print</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;row : data) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;col : row) &#123;  </span><br><span class="line">            std::cout &lt;&lt; col &lt;&lt; <span class="string">&quot; &quot;</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">        std::cout &lt;&lt; std::endl;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">std::string <span class="title">Matrix::printableShape</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;(&quot;</span> + std::<span class="built_in">to_string</span>(<span class="keyword">this</span>-&gt;<span class="built_in">rows</span>()) + <span class="string">&quot;, &quot;</span> + std::<span class="built_in">to_string</span>(<span class="keyword">this</span>-&gt;<span class="built_in">cols</span>()) + <span class="string">&quot;)&quot;</span>;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Simple-check"><a href="#Simple-check" class="headerlink" title="Simple check"></a>Simple check</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Matrix::isScalar</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">rows</span>() == <span class="number">1</span> &amp;&amp; <span class="keyword">this</span>-&gt;<span class="built_in">cols</span>() == <span class="number">1</span>;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<p>And we’re done here, the build of the neural network will be in part 2</p>
]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>c++</tag>
        <tag>machine-learning</tag>
        <tag>neural-network</tag>
      </tags>
  </entry>
  <entry>
    <title>Very Naive Neural Network in C++ Part 2 Activation functions</title>
    <url>/2021/01/05/Code/Naive-Neural-Network-in-Cpp-2/</url>
    <content><![CDATA[<p>Now we’ve created the <code>Matrix</code>, we should build the layer that contains the neurons. But before that we need the activation functions. I’m gonna create a class that contains some static activation functions and their derivative forms (relu, sigmoid, tanh and softmax). These activation functions will take the input matrices and output the  matrices after applying the corresponding functions.</p>
<span id="more"></span>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Matrix.h&quot;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span>  </span></span><br><span class="line"></span><br><span class="line"><span class="comment">//Let&#x27;s put everything to an enum for better organization</span></span><br><span class="line"><span class="keyword">enum</span> <span class="title class_">ActivationFunctionType</span> &#123;  </span><br><span class="line">    RELU,  </span><br><span class="line">    SIGMOID,  </span><br><span class="line">    TANH,  </span><br><span class="line">    SOFTMAX,  </span><br><span class="line">    NONE  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ActivationFunction</span> &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">	<span class="comment">//The activation and derivative methods will serve as proxies to call the required function types</span></span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">activation</span><span class="params">(<span class="type">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">derivative</span><span class="params">(<span class="type">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span>:  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">reluActivation</span><span class="params">(Matrix input)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">reluDerivative</span><span class="params">(Matrix input)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">sigmoidActivation</span><span class="params">(Matrix input)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">sigmoidDerivative</span><span class="params">(Matrix input)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">tanhActivation</span><span class="params">(Matrix input)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">tanhDerivative</span><span class="params">(Matrix input)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">softmaxActivation</span><span class="params">(Matrix input)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">softmaxDerivative</span><span class="params">(Matrix input)</span></span>;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;ActivationFunction.h&quot;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">ActivationFunction::activation</span><span class="params">(<span class="type">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span> </span>&#123;  </span><br><span class="line"><span class="comment">//    std::cout &lt;&lt; &quot;ActivationFunction::activation&quot; &lt;&lt; std::endl;  </span></span><br><span class="line">    <span class="keyword">switch</span> (activationFunctionType) &#123;  </span><br><span class="line">        <span class="keyword">case</span> RELU:  </span><br><span class="line">            <span class="keyword">return</span> ActivationFunction::<span class="built_in">reluActivation</span>(input);  </span><br><span class="line">        <span class="keyword">case</span> SIGMOID:  </span><br><span class="line">            <span class="keyword">return</span> ActivationFunction::<span class="built_in">sigmoidActivation</span>(input);  </span><br><span class="line">        <span class="keyword">case</span> TANH:  </span><br><span class="line">            <span class="keyword">return</span> ActivationFunction::<span class="built_in">tanhActivation</span>(input);  </span><br><span class="line">        <span class="keyword">case</span> SOFTMAX:  </span><br><span class="line">            <span class="keyword">return</span> ActivationFunction::<span class="built_in">softmaxActivation</span>(input);  </span><br><span class="line">        <span class="keyword">default</span>:  </span><br><span class="line">            <span class="keyword">return</span> input;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">ActivationFunction::derivative</span><span class="params">(<span class="type">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">switch</span> (activationFunctionType) &#123;  </span><br><span class="line">        <span class="keyword">case</span> RELU:  </span><br><span class="line">            <span class="keyword">return</span> ActivationFunction::<span class="built_in">reluDerivative</span>(input);  </span><br><span class="line">        <span class="keyword">case</span> SIGMOID:  </span><br><span class="line">            <span class="keyword">return</span> ActivationFunction::<span class="built_in">sigmoidDerivative</span>(input);  </span><br><span class="line">        <span class="keyword">case</span> TANH:  </span><br><span class="line">            <span class="keyword">return</span> ActivationFunction::<span class="built_in">tanhDerivative</span>(input);  </span><br><span class="line">        <span class="keyword">case</span> SOFTMAX:  </span><br><span class="line">            <span class="keyword">return</span> ActivationFunction::<span class="built_in">softmaxDerivative</span>(input);  </span><br><span class="line">        <span class="keyword">default</span>:  </span><br><span class="line">            <span class="keyword">return</span> input;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>
<h1 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Matrix <span class="title">ActivationFunction::reluActivation</span><span class="params">(Matrix input)</span> </span>&#123;  </span><br><span class="line">    Matrix output = input;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; input.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; input.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">input</span>(i, j) &lt; <span class="number">0</span>) &#123;  </span><br><span class="line">                <span class="built_in">output</span>(i, j) = <span class="number">0</span>;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> output;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">ActivationFunction::reluDerivative</span><span class="params">(Matrix input)</span> </span>&#123;  </span><br><span class="line">    Matrix output = input;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; input.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; input.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">input</span>(i, j) &lt; <span class="number">0</span>) &#123;  </span><br><span class="line">                <span class="built_in">output</span>(i, j) = <span class="number">0</span>;  </span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">                <span class="built_in">output</span>(i, j) = <span class="number">1</span>;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> output;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<h1 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Matrix <span class="title">ActivationFunction::softmaxActivation</span><span class="params">(Matrix input)</span> </span>&#123;  </span><br><span class="line">    Matrix output = input;  </span><br><span class="line">    <span class="type">double</span> sum = <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; input.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; input.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            sum += <span class="built_in">exp</span>(<span class="built_in">input</span>(i, j));  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; input.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; input.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="built_in">output</span>(i, j) = <span class="built_in">exp</span>(<span class="built_in">input</span>(i, j)) / sum;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> output;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">ActivationFunction::softmaxDerivative</span><span class="params">(Matrix input)</span> </span>&#123;  </span><br><span class="line">    Matrix output = input;  </span><br><span class="line">    <span class="type">double</span> sum = <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; input.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; input.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            sum += <span class="built_in">exp</span>(<span class="built_in">input</span>(i, j));  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; input.<span class="built_in">rows</span>(); i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; input.<span class="built_in">cols</span>(); j++) &#123;  </span><br><span class="line">            <span class="built_in">output</span>(i, j) = <span class="built_in">exp</span>(<span class="built_in">input</span>(i, j)) / sum * (<span class="number">1</span> - <span class="built_in">exp</span>(<span class="built_in">input</span>(i, j)) / sum);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> output;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>
<h1 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Matrix ActivationFunction::sigmoidActivation(Matrix input) &#123;  </span><br><span class="line">    Matrix output = input;  </span><br><span class="line">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class="line">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class="line">            output(i, j) = 1 / (1 + exp(-input(i, j)));  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    return output;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix ActivationFunction::sigmoidDerivative(Matrix input) &#123;  </span><br><span class="line">    Matrix output = input;  </span><br><span class="line">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class="line">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class="line">            output(i, j) = exp(-input(i, j)) / pow(1 + exp(-input(i, j)), 2);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    return output;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<h1 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Matrix ActivationFunction::tanhActivation(Matrix input) &#123;  </span><br><span class="line">    Matrix output = input;  </span><br><span class="line">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class="line">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class="line">            output(i, j) = (exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j)));  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    return output;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Matrix ActivationFunction::tanhDerivative(Matrix input) &#123;  </span><br><span class="line">    Matrix output = input;  </span><br><span class="line">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class="line">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class="line">            output(i, j) = 1 - pow((exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j))), 2);  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    return output;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>c++</tag>
        <tag>machine-learning</tag>
        <tag>neural-network</tag>
      </tags>
  </entry>
  <entry>
    <title>Very Naive Neural Network in C++ Part 3 Layers</title>
    <url>/2021/01/05/Code/Naive-Neural-Network-in-Cpp-3/</url>
    <content><![CDATA[<p>Okay, now we have matrices and the activation functions, it’s time to build up the layers.<br>For simplicity and purpose of the project, I will not gives the layers more abstraction by making it an interface and builds the subsequent layer above that. For now, I will put activation and non-activation layers together.</p>
<span id="more"></span>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../Math/ActivationFunction.h&quot;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;random&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Layer</span> &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">    <span class="comment">// Constructor for input layer, which has no weights and biases  </span></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Layer</span><span class="params">(<span class="type">int</span> inputSize, <span class="type">int</span> outputSize)</span></span>;  </span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Layer</span><span class="params">(<span class="type">int</span> inputSize, <span class="type">int</span> outputSize, Matrix weights, Matrix biases)</span></span>;  </span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Layer</span><span class="params">(ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class="line"></span><br><span class="line">	<span class="comment">//Forward propagation</span></span><br><span class="line">    <span class="function">Matrix <span class="title">forwardPropagation</span><span class="params">(<span class="type">const</span> Matrix&amp; inputData)</span></span>;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Backward propagation  </span></span><br><span class="line">    <span class="function">Matrix <span class="title">backwardPropagation</span><span class="params">(<span class="type">const</span> Matrix&amp; error, <span class="type">double</span> learningRate)</span></span>;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Getters  </span></span><br><span class="line">    <span class="function">Matrix <span class="title">getWeights</span><span class="params">()</span></span>;  </span><br><span class="line">    <span class="function">Matrix <span class="title">getBiases</span><span class="params">()</span></span>;  </span><br><span class="line">    <span class="function">Matrix <span class="title">getInput</span><span class="params">()</span></span>;  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getInputSize</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getOutputSize</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">isActivationLayer</span><span class="params">()</span> <span class="type">const</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">setActivationType</span><span class="params">(ActivationFunctionType activationType)</span></span>;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span>:  </span><br><span class="line">    Matrix weights;  </span><br><span class="line">    Matrix biases;  </span><br><span class="line">    ActivationFunctionType activationFunctionType;  </span><br><span class="line">    Matrix input;  </span><br><span class="line">    <span class="type">int</span> inputSize;  </span><br><span class="line">    <span class="type">int</span> outputSize;  </span><br><span class="line">  </span><br><span class="line">    <span class="function">ActivationFunctionType <span class="title">getActivationType</span><span class="params">(ActivationFunctionType activationType)</span></span>;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="Initialization-of-weights-and-bias"><a href="#Initialization-of-weights-and-bias" class="headerlink" title="Initialization of weights and bias"></a>Initialization of weights and bias</h1><p>So here we will generate the weights and bias randomly by a normal distribution<br>The initial values are actually crucial to our naive design</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Layer.h&quot;</span>  </span></span><br><span class="line">Layer::<span class="built_in">Layer</span>(ActivationFunctionType activationFunctionType) : <span class="built_in">weights</span>(Matrix::<span class="built_in">EmptyMatrix</span>()),  </span><br><span class="line">                                                              <span class="built_in">biases</span>(Matrix::<span class="built_in">EmptyMatrix</span>()),  </span><br><span class="line">                                                              <span class="built_in">input</span>(Matrix::<span class="built_in">EmptyMatrix</span>()), <span class="built_in">inputSize</span>(<span class="number">0</span>), <span class="built_in">outputSize</span>(<span class="number">0</span>),  </span><br><span class="line">                                                              <span class="built_in">activationFunctionType</span>(activationFunctionType) &#123;&#125;  </span><br><span class="line">  </span><br><span class="line">Layer::<span class="built_in">Layer</span>(<span class="type">int</span> inputSize, <span class="type">int</span> outputSize) : <span class="built_in">weights</span>(inputSize, outputSize),  </span><br><span class="line">                                              <span class="built_in">biases</span>(<span class="number">1</span>, outputSize),  </span><br><span class="line">                                              <span class="built_in">input</span>(Matrix::<span class="built_in">EmptyMatrix</span>()),  </span><br><span class="line">                                              <span class="built_in">inputSize</span>(inputSize),  </span><br><span class="line">                                              <span class="built_in">outputSize</span>(outputSize),  </span><br><span class="line">                                              <span class="built_in">activationFunctionType</span>(ActivationFunctionType::NONE) &#123;  </span><br><span class="line">    weights = <span class="built_in">Matrix</span>(inputSize, outputSize);  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Set up random number generator  </span></span><br><span class="line">    std::random_device rd;  </span><br><span class="line">    <span class="function">std::mt19937 <span class="title">gen</span><span class="params">(rd())</span></span>;  </span><br><span class="line">    <span class="function">std::normal_distribution&lt;<span class="type">double</span>&gt; <span class="title">weights_distribution</span><span class="params">(inputSize, outputSize)</span></span>;  </span><br><span class="line">    <span class="function">std::normal_distribution&lt;<span class="type">double</span>&gt; <span class="title">bias_distribution</span><span class="params">(<span class="number">1</span>, outputSize)</span></span>;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Initialize weights and biases with random values from normal distribution  </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; inputSize; i++) &#123;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; outputSize; j++) &#123;  </span><br><span class="line">            <span class="keyword">this</span>-&gt;weights.<span class="built_in">changeAt</span>(i, j, <span class="built_in">weights_distribution</span>(gen)/ <span class="built_in">sqrt</span>(inputSize+outputSize));  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; outputSize; i++) &#123;  </span><br><span class="line">        <span class="keyword">this</span>-&gt;biases.<span class="built_in">changeAt</span>(<span class="number">0</span>, i, <span class="built_in">bias_distribution</span>(gen)/ <span class="built_in">sqrt</span>(inputSize+outputSize));  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">this</span>-&gt;weights.<span class="built_in">print</span>();  </span><br><span class="line">    <span class="keyword">this</span>-&gt;biases.<span class="built_in">print</span>();  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>
<h1 id="Forward-propagation"><a href="#Forward-propagation" class="headerlink" title="Forward propagation"></a>Forward propagation</h1><ul>
<li>For non-activation layer, we will simply sum up the product of each x and weights and plus the bias. So the information will be changed and transformed into the dimension of the inputSize times the output size<br>$$Y&#x3D;XW+B$$</li>
<li>For activation layer, we just apply the activation function and return the tensor to next layer<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Matrix <span class="title">Layer::forwardPropagation</span><span class="params">(<span class="type">const</span> Matrix &amp;inputData)</span> </span>&#123;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">this</span>-&gt;input = inputData;  </span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;activationFunctionType == ActivationFunctionType::NONE) &#123;  </span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>-&gt;biases + <span class="keyword">this</span>-&gt;input.<span class="built_in">dot</span>(<span class="keyword">this</span>-&gt;weights);  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        Matrix results = ActivationFunction::<span class="built_in">activation</span>(inputData, activationFunctionType);  </span><br><span class="line">        <span class="keyword">return</span> results;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="Backward-propagation"><a href="#Backward-propagation" class="headerlink" title="Backward propagation"></a>Backward propagation</h1><p>The clear part of the neural network is to find a set of weights for each layer that minimize the error, and the most common way is to find the gradient and move the weights and biases bits by bits based on the gradients.</p>
<p>We have to use chain rule to get the information from the previous layer (assume has <code>n</code> nodes)<br>$$<br>\frac{\partial E}{\partial W} &#x3D; \frac{\partial E}{\partial w_{ij}} &#x3D; \sum_{k &#x3D; 1}^{n} \frac{\partial E}{\partial k_j} \frac{\partial y_k}{\partial w_{ij}} &#x3D;\sum_{k &#x3D; 1}^{n} \frac{\partial E}{\partial y_j} x_{i}&#x3D;X^t \frac{\partial E}{\partial Y}<br>$$</p>
<p>$$<br>\frac{\partial E}{\partial B_i} &#x3D; \sum_{j &#x3D; 1}^{n} \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial b_i} &#x3D;\sum_{j &#x3D; 1}^{n} \frac{\partial E}{\partial y_j} &#x3D; \frac{\partial E}{\partial Y}<br>$$</p>
<p>$$<br>\frac{\partial E}{\partial X_i} &#x3D; \sum_{j &#x3D; 1}^{n} \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial x_i} &#x3D;\sum_{j &#x3D; 1}^{n} \frac{\partial E}{\partial y_j} w_{ij}<br>$$</p>
<p>$$<br>\frac{\partial E}{\partial X} &#x3D; \frac{\partial E}{\partial Y}W^t<br>$$</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// error is dE/DY</span></span><br><span class="line"><span class="function">Matrix <span class="title">Layer::backwardPropagation</span><span class="params">(<span class="type">const</span> Matrix &amp;error, <span class="type">double</span> learningRate)</span> </span>&#123;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">// If the layer is not the activation layer  </span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;activationFunctionType == ActivationFunctionType::NONE) &#123;  </span><br><span class="line">		<span class="comment">// dE/dX</span></span><br><span class="line">        Matrix previousError = error.<span class="built_in">dot</span>(<span class="keyword">this</span>-&gt;weights.<span class="built_in">transpose</span>());  </span><br><span class="line">  </span><br><span class="line">        <span class="comment">// Calculate gradient of weights and biases  </span></span><br><span class="line">        <span class="comment">// dE/dW</span></span><br><span class="line">        Matrix gradientOfWeights = <span class="keyword">this</span>-&gt;input.<span class="built_in">transpose</span>().<span class="built_in">dot</span>(error);  </span><br><span class="line">  </span><br><span class="line">        <span class="comment">// dE/dB, just for better understanding  </span></span><br><span class="line">        <span class="type">const</span> Matrix &amp;gradientOfBiases = error;  </span><br><span class="line">  </span><br><span class="line">        <span class="comment">// Update weights and biases  </span></span><br><span class="line">        <span class="keyword">this</span>-&gt;weights -= gradientOfWeights * learningRate;  </span><br><span class="line">        <span class="keyword">this</span>-&gt;biases -= gradientOfBiases * learningRate;  </span><br><span class="line">        <span class="keyword">return</span> previousError;  </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;  </span><br><span class="line">        <span class="keyword">return</span> ActivationFunction::<span class="built_in">derivative</span>(<span class="keyword">this</span>-&gt;input, <span class="keyword">this</span>-&gt;activationFunctionType) * error;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="Other-methods"><a href="#Other-methods" class="headerlink" title="Other methods"></a>Other methods</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Matrix <span class="title">Layer::getWeights</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;weights;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">Layer::getBiases</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;biases;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">Layer::getInput</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;input;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Layer::getInputSize</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> inputSize;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Layer::getOutputSize</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> outputSize;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Layer::isActivationLayer</span><span class="params">()</span> <span class="type">const</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;activationFunctionType != ActivationFunctionType::NONE;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Layer::setActivationType</span><span class="params">(ActivationFunctionType activationType)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">this</span>-&gt;activationFunctionType = activationType;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">ActivationFunctionType <span class="title">Layer::getActivationType</span><span class="params">(ActivationFunctionType activationType)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>-&gt;activationFunctionType;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">Layer::<span class="built_in">Layer</span>(<span class="type">int</span> inputSize, <span class="type">int</span> outputSize, Matrix weights, Matrix biases):  </span><br><span class="line">        <span class="built_in">weights</span>(weights), <span class="built_in">biases</span>(biases), <span class="built_in">input</span>(Matrix::<span class="built_in">EmptyMatrix</span>()), <span class="built_in">inputSize</span>(inputSize), <span class="built_in">outputSize</span>(outputSize),  </span><br><span class="line">        <span class="built_in">activationFunctionType</span>(ActivationFunctionType::NONE) &#123;&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>c++</tag>
        <tag>machine-learning</tag>
        <tag>neural-network</tag>
      </tags>
  </entry>
  <entry>
    <title>Very Naive Neural Network in C++ Part 4 Network</title>
    <url>/2021/01/05/Code/Naive-Neural-Network-in-Cpp-4%20Network/</url>
    <content><![CDATA[<p>Ok this is the final bit. We need the network to wrap things up and do the training</p>
<p>But again, we need some loss functions to determine the errors we gonna put into back propagation. I will use a simple MSE here. it’s simply the sum of square of the difference between predicted value and training value<br>$$MSE &#x3D; \frac{\sum^n_{i&#x3D;1} (Y_i - \hat Y_i)^2}{n}$$</p>
<p>And I guess you can directly see the derivative from here</p>
<span id="more"></span>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Matrix.h&quot;</span>  </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Loss</span> &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="type">static</span> <span class="type">double</span> <span class="title">meanSquaredError</span><span class="params">(<span class="type">const</span> Matrix &amp;predicted, <span class="type">const</span> Matrix &amp;actual)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">static</span> Matrix <span class="title">meanSquaredErrorDerivative</span><span class="params">(<span class="type">const</span> Matrix &amp;predicted, <span class="type">const</span> Matrix &amp;actual)</span></span>;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Loss.h&quot;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">Loss::meanSquaredError</span><span class="params">(<span class="type">const</span> Matrix&amp; predicted, <span class="type">const</span> Matrix&amp; actual)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">return</span> ((predicted - actual) * (predicted - actual)).<span class="built_in">mean</span>();  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">Matrix <span class="title">Loss::meanSquaredErrorDerivative</span><span class="params">(<span class="type">const</span> Matrix&amp; predicted, <span class="type">const</span> Matrix&amp; actual)</span> </span>&#123;  </span><br><span class="line"><span class="comment">//    std::cout&lt;&lt; &quot;Predicted: &quot; &lt;&lt; predicted &lt;&lt; std::endl;  </span></span><br><span class="line"><span class="comment">//    std::cout&lt;&lt; &quot;Actual: &quot; &lt;&lt; actual &lt;&lt; std::endl;  </span></span><br><span class="line"><span class="comment">//    std::cout&lt;&lt; &quot;Actual size&quot; &lt;&lt; actual.size() &lt;&lt; std::endl;  </span></span><br><span class="line">    <span class="keyword">return</span> (predicted - actual)*<span class="number">2</span>/actual.<span class="built_in">size</span>();  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h1><p>Now finally, the ending bits. </p>
<ul>
<li>The network is gonna take layers and stack them in order</li>
<li>The network is gonna fit(train), aka change the weights and bias, based on input data</li>
<li>and it’s need to predict</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../Layers/Layer.h&quot;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../Math/Loss.h&quot;</span>  </span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Network</span> &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">    <span class="built_in">Network</span>(): <span class="built_in">layers</span>(std::<span class="built_in">vector</span>&lt;Layer&gt;()) &#123;&#125;;  </span><br><span class="line">    ~<span class="built_in">Network</span>();  </span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addLayer</span><span class="params">(<span class="type">const</span> Layer&amp; layer)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">fit</span><span class="params">(<span class="type">const</span> Matrix &amp;xTrain, <span class="type">const</span> Matrix &amp;yTrain, <span class="type">int</span> numberOfEpoch, <span class="type">double</span> learningRate)</span></span>;  </span><br><span class="line">    <span class="function">Matrix <span class="title">predict</span><span class="params">(<span class="type">const</span> Matrix&amp; xTest)</span></span>;  </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">getOutputSize</span><span class="params">()</span></span>;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span>:  </span><br><span class="line">    std::vector&lt;Layer&gt; layers;  </span><br><span class="line">  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h1 id="Add-layers"><a href="#Add-layers" class="headerlink" title="Add layers"></a>Add layers</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Network.h&quot;</span>  </span></span><br><span class="line">  </span><br><span class="line">Network::~<span class="built_in">Network</span>() = <span class="keyword">default</span>;</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Network::addLayer</span><span class="params">(<span class="type">const</span> Layer &amp;layer)</span> </span>&#123;  </span><br><span class="line">    layers.<span class="built_in">push_back</span>(layer);  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<p>The fit method is gonna</p>
<ul>
<li>take input consisting of actual value (Y) and its label (X)</li>
<li>the training time, epoch number</li>
<li>the learning rate, how much of the gradient we apply to the weights and bias</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Network::fit</span><span class="params">(<span class="type">const</span> Matrix &amp;xTrain, <span class="type">const</span> Matrix &amp;yTrain, <span class="type">int</span> numberOfEpoch, <span class="type">double</span> learningRate)</span> </span>&#123;  </span><br><span class="line">    <span class="comment">// Check if the number of training data matches the number of labels  </span></span><br><span class="line">    <span class="type">int</span> trainingSize = xTrain.<span class="built_in">rows</span>();  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span> (trainingSize != yTrain.<span class="built_in">rows</span>()) &#123;  </span><br><span class="line">        <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(<span class="string">&quot;The number of training data does not match the number of labels&quot;</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> epoch = <span class="number">0</span>; epoch &lt; numberOfEpoch; epoch++) &#123;  </span><br><span class="line">        <span class="type">double</span> mse = <span class="number">0</span>;  </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; trainingSize; ++i) &#123;  </span><br><span class="line">            Matrix output = xTrain.<span class="built_in">getRow</span>(i);  </span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;layer: layers) &#123;  </span><br><span class="line">                output = layer.forwardPropagation(output);  </span><br><span class="line">            &#125;  </span><br><span class="line">			<span class="comment">// We actually don&#x27;t need this for computation, but we can print it to see if the training is doing properly</span></span><br><span class="line">            mse += Loss::<span class="built_in">meanSquaredError</span>(output, yTrain.<span class="built_in">getRow</span>(i));  </span><br><span class="line">	</span><br><span class="line">			<span class="comment">// The dE/dY</span></span><br><span class="line">            Matrix err = Loss::<span class="built_in">meanSquaredErrorDerivative</span>(output, yTrain.<span class="built_in">getRow</span>(i));  </span><br><span class="line"></span><br><span class="line">			<span class="comment">//remeber it&#x27;s back not forward, so we do it in reverse</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> it = layers.<span class="built_in">rbegin</span>(); it != layers.<span class="built_in">rend</span>(); ++it) &#123;  </span><br><span class="line">                err = it-&gt;<span class="built_in">backwardPropagation</span>(err, learningRate);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">		</span><br><span class="line">        mse /= trainingSize;  </span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Epoch &quot;</span> &lt;&lt; epoch &lt;&lt; <span class="string">&quot; MSE: &quot;</span> &lt;&lt; mse &lt;&lt; std::endl;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>
<h1 id="Predict"><a href="#Predict" class="headerlink" title="Predict"></a>Predict</h1><p>Prediction is rather easy, just propagate through all the layer and you will have the answer.</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Matrix <span class="title">Network::predict</span><span class="params">(<span class="type">const</span> Matrix&amp; xTest)</span> </span>&#123;  </span><br><span class="line">    <span class="type">int</span> testingSize = xTest.<span class="built_in">rows</span>();  </span><br><span class="line">    <span class="function">Matrix <span class="title">result</span><span class="params">(<span class="number">0</span>, getOutputSize())</span></span>;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; testingSize; ++i) &#123;  </span><br><span class="line">        Matrix output = xTest.<span class="built_in">getRow</span>(i);  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;layer: layers) &#123;  </span><br><span class="line">            output = layer.forwardPropagation(output);  </span><br><span class="line">        &#125;  </span><br><span class="line">  </span><br><span class="line">        result.<span class="built_in">addRow</span>(output);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> result;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Network::getOutputSize</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">    <span class="comment">// check if the layer is not an activation layer reversely  </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> it = layers.<span class="built_in">rbegin</span>(); it != layers.<span class="built_in">rend</span>(); ++it) &#123;  </span><br><span class="line">        <span class="keyword">if</span> (!it-&gt;<span class="built_in">isActivationLayer</span>()) &#123;  </span><br><span class="line">            <span class="keyword">return</span> it-&gt;<span class="built_in">getOutputSize</span>();  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Test-the-code"><a href="#Test-the-code" class="headerlink" title="Test the code"></a>Test the code</h1><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;Networks/Network.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="comment">//XOR gate</span></span><br><span class="line">	<span class="keyword">auto</span> xTrain = <span class="built_in">Matrix</span>(&#123;&#123;<span class="number">0</span>,<span class="number">0</span>&#125;, &#123;<span class="number">0</span>,<span class="number">1</span>&#125;, &#123;<span class="number">1</span>,<span class="number">0</span>&#125;, &#123;<span class="number">1</span>,<span class="number">1</span>&#125;&#125;);  </span><br><span class="line">	<span class="keyword">auto</span> yTrain = <span class="built_in">Matrix</span>(&#123;&#123;<span class="number">0</span>&#125;, &#123;<span class="number">1</span>&#125;, &#123;<span class="number">1</span>&#125;, &#123;<span class="number">0</span>&#125;&#125;);  </span><br><span class="line"></span><br><span class="line">	Network network = <span class="built_in">Network</span>();  </span><br><span class="line">	network.<span class="built_in">addLayer</span>(<span class="built_in">Layer</span>(<span class="number">2</span>, <span class="number">3</span>));  </span><br><span class="line">	network.<span class="built_in">addLayer</span>(<span class="built_in">Layer</span>(ActivationFunctionType::TANH));  </span><br><span class="line">	network.<span class="built_in">addLayer</span>(<span class="built_in">Layer</span>(<span class="number">3</span>,<span class="number">1</span>));  </span><br><span class="line">	network.<span class="built_in">addLayer</span>(<span class="built_in">Layer</span>(ActivationFunctionType::TANH));  </span><br><span class="line">	network.<span class="built_in">fit</span>(xTrain, yTrain, <span class="number">100</span>, <span class="number">0.1</span>);  </span><br><span class="line"></span><br><span class="line">	<span class="keyword">auto</span> result = network.<span class="built_in">predict</span>(xTrain);  </span><br><span class="line">	std::cout &lt;&lt; result &lt;&lt; std::endl;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The output</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Epoch 1 MSE: 0.48139</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Epoch 99 MSE: 0.0133901</span><br><span class="line">0.0314563, </span><br><span class="line">0.886822, </span><br><span class="line">0.819525, </span><br><span class="line">-0.00120216, </span><br></pre></td></tr></table></figure>

<p>Here we go, we get the fairly accurate output with a small epoch number. </p>
<p>And we can also use the model on 2D images like the famous mnist dataset. The idea is convert the 2d image into a 1D array and create the initial activation layer with input size equals to the image size. However, it requires some image processing code which we will be doing in another day.</p>
]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>deep-learning</tag>
        <tag>c++</tag>
        <tag>machine-learning</tag>
        <tag>neural-network</tag>
      </tags>
  </entry>
  <entry>
    <title>Setup Neovide in WSL Archlinux</title>
    <url>/2025/05/01/System/neovide-setup-in-wsl-archilinux/</url>
    <content><![CDATA[<h1 id="Reading-time"><a href="#Reading-time" class="headerlink" title="Reading time"></a>Reading time</h1><ul>
<li>less than 5 min</li>
</ul>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>Neovide is a one of the best GUI for Neovim. It is written in Rust, supports high refreshing rate, smooth animations and transparent&#x2F;opacity background. I tried to use Neovide as my daily editor but later walked back to VSCode for some Neovim configuration related reasons. Recently the weather is getting warmer, so is my laptop when a VSCode instance is running with minimal typing. I decided to give Neovide and Neovim another visit and this time I will be running Neovim in WSL.</p>
<h1 id="Use-Neovim-RPC"><a href="#Use-Neovim-RPC" class="headerlink" title="Use Neovim RPC"></a>Use Neovim RPC</h1><p>The first bump quickly came as I tried to run neovide with <code>neovide --wsl</code>, I have 3 instances of distributions (Ubuntu, Archlinux &amp; NixOS) and have Neovim installed on first 2 distros. But the neovide was not able to detect the Neovim installation. As a workround, I run the following command to use the Neovim <a href="https://neovim.io/doc/user/remote.html">remote</a> mode.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvim --headless --lisen localhost:6666 </span><br></pre></td></tr></table></figure>

<p>then open a powershell in windows, we connect neovide to this address</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">neovide <span class="literal">--server</span>=localhost:<span class="number">6666</span></span><br></pre></td></tr></table></figure>

<p>The 1st line will run a Neovim instance on current directory and it is a blocking foreground process and 2nd line will run the neovide and connect to the instance. This is annoying since we have to do 2 things on 2 shells.<br>A better solution will be using a bash function to handle both jobs:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> nvs &#123;</span><br><span class="line">  nvim --headless --listen localhost:6666 <span class="string">&quot;<span class="variable">$@</span>&quot;</span> &amp;</span><br><span class="line">  <span class="built_in">sleep</span> 0.5 &amp;&amp; <span class="string">&quot;/mnt/c/Program Files/Neovide/neovide.exe&quot;</span> --server=localhost:6666 &amp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>You can replace the port and neovide installation path above with your own</p>
</blockquote>
<p>By adding the above function to <code>.bashrc</code>, and source it. We now have a single command to run a headless Neovim and the corresponding Neovide.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvs something.txt</span><br></pre></td></tr></table></figure>

<h1 id="Clipboard"><a href="#Clipboard" class="headerlink" title="Clipboard"></a>Clipboard</h1><p>Checkout the clipboard post on how to setup the clipboard for neovim in archlinux</p>
]]></content>
      <categories>
        <category>System</category>
      </categories>
      <tags>
        <tag>archlinux</tag>
        <tag>wsl</tag>
        <tag>neovide</tag>
        <tag>neovim</tag>
      </tags>
  </entry>
</search>
