{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1},{"_id":"source/images/cateye.jpg","path":"images/cateye.jpg","modified":0,"renderable":0},{"_id":"node_modules/hexo-theme-next/source/js/sidebar.js","path":"js/sidebar.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","path":"js/third-party/addtoany.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","path":"js/third-party/tags/wavedrom.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/.obsidian/app.json","hash":"bfadd47324d70b4cd0b738e55227027a85f1bd77","modified":1671050501048},{"_id":"source/.obsidian/appearance.json","hash":"9439122150a0431e432676115220bc925af96094","modified":1668853278040},{"_id":"source/.obsidian/core-plugins.json","hash":"bef954208c2c5c5e253ee8738889b6d7d02793eb","modified":1668849171032},{"_id":"source/.obsidian/hotkeys.json","hash":"bf21a9e8fbc5a3846fb05b4fa0859e0917b2202f","modified":1668849170529},{"_id":"source/.obsidian/workspace.json","hash":"8a553df0206ded67234d8435eeff1b327668d2b1","modified":1671235236090},{"_id":"source/404/index.md","hash":"99a9cfc6e59ccfc68caf5a1d5c3f41d268f26d23","modified":1668855665985},{"_id":"source/categories/index.md","hash":"ea34e09c1a592f3892acde873f30539d20cbe233","modified":1668850996215},{"_id":"source/tags/index.md","hash":"3b4712581282205c08005b3bee3a4a6388c46d43","modified":1668850979668},{"_id":"source/_posts/System/archlinux-easy-setup-for-deep-learning.md","hash":"94f95db43e1e65aa696ee1b7e7021776ddc527ac","modified":1668854963495},{"_id":"source/_posts/System/archlinux-emoji-display-problem.md","hash":"0ee5db8d7e36aa9f289f2d3ec6fa8fa347464e75","modified":1668852286659},{"_id":"source/_posts/System/archlinux-zoom-oversized-window.md","hash":"65b608166c82dfa41aae6fd82cf3cc57fb832ce6","modified":1668852315799},{"_id":"source/_posts/Gallery/Cat/Funny.md","hash":"4c7c307effbcad35935d479847533b4ce1089e1d","modified":1668852469888},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1668844772641},{"_id":"source/images/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1668846721611},{"_id":"node_modules/hexo-theme-next/README.md","hash":"5dd295594b92dbe07b0376b505f29fb3884e77f9","modified":1744315703587},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"dbbd054721541f4d1f4fed4bc983bc19ff1f259a","modified":1744315703767},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"72a3de9af55fbb7831ab7a3aa6c5e3f89cad53d3","modified":1744315703768},{"_id":"node_modules/hexo-theme-next/package.json","hash":"eaf826c46804a8da9ec6ddf325c4670a3e3ecfa7","modified":1744315703571},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1668844772638},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"7d0f39e8684284a04bb9808521c87fecda8bd131","modified":1744315703769},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1668844772638},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"564bed75da6e05b11dce6164508f97a15e2fb6c2","modified":1744315703770},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"79b37df731c29665dee6cd7c90d278e1edfb6e24","modified":1744315703770},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"ba0fd79a2b1d8db01a034180556061745965ff05","modified":1744315703771},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"dffc63ef42e1266b88e0acf08994fd17a9908d53","modified":1744315703773},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"929df147f4f17d638b07de5fe52ca13e2549ab1c","modified":1744315703775},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"16d716ecfd748def2f6486ef5a82d0ab7ceb4890","modified":1744315703775},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"8ac44e58f71a38b7697a2f7f98a6971ed818cb5b","modified":1744315703774},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"f3ffc444599f4ac92d62e9ed00a1490ebc277d70","modified":1744315703774},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"3cb3687696635ec71b4ca40c5fc43b56acc8843e","modified":1744315703778},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"543222bfc516aab6c33e8534f807972ecb8943a9","modified":1744315703777},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"d345a303310c8a5f4836c3683f3580f861ebd1b4","modified":1744315703778},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"76b8576ce228d540a16b1f0af5af2cce20923194","modified":1744315703779},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"b62faaa767a45a613dd042b5f1903675eb5a8cf9","modified":1744315703779},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2d712eedf3f60d04d36c3108cf5a12e2a52e875c","modified":1744315703780},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"511726054873f6f8d7ce0d2e803f6731de0ddbe7","modified":1744315703784},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"c6d8de0ff7d8148d09993257cfd3b7aca755696c","modified":1744315703780},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"ff537047b4b4c3ca9a7b64fa7f428a9942751eeb","modified":1744315703785},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"7ebcba5e1128784195e4681dffc9d34c4e873fec","modified":1744315703786},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"741d7efe0262c9cdc2c648014b55599665d90f6b","modified":1744315703788},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"a57e4ed089b893a95f5e1ecff17ce625165f4d46","modified":1744315703785},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"b17d44bd7379c23241053a0b7fbd38c9c43cc239","modified":1744315703588},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"8eb6a9f231ce1bfa54cc54418ccf14f01dcc9a31","modified":1744315703789},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"5c0f00cdac3f4727b880dd223f622a535736fa8e","modified":1744315703789},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"af6d7570621be760536c216a56d74e40a1cceae2","modified":1744315703651},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"12a6631617695504d5cf2a94b57d87bd331bef6f","modified":1744315703579},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"285d5c4dffb97da659851fb6e53379a614619fd9","modified":1744315703583},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"313637fe3569f98fd926e8cd0fcc75d098eb6e6e","modified":1744315703653},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"21567f1810f861b0a33db6514a4fcf18fb98467d","modified":1744315703584},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1668844772641},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"fbf8232cacf0df87e88e74860be66c9f86018302","modified":1744315703627},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"952449064fcb6a5cefc281b585f9149809f857f1","modified":1744315703660},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"2a7dfffebad19b67dc9e3b2a6b2986d0630ef930","modified":1744315703638},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"7261e24287984853c8ef08cda8bbc80cacf9bd6f","modified":1744315703667},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"d83fb59f02c5e6630a7770401a05c02f6f07358b","modified":1744315703668},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"dfd7cdd6ba89f8c3deabc27726c7a350cadafd11","modified":1744315703639},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/rating.njk","hash":"1bcdbc7fde26d6d9ef4e7fa43ffcff5a9506b20e","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1671319836980},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1671319836990},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"2063aaa1db448ebcf7b0fdbbc54d3991a368bbd3","modified":1744315703544},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"4c84d45daac86396edf656d2a8abe6e7583491ea","modified":1744315703496},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"83235f2879567eb8686431c9554a4b99f14ab665","modified":1744315703473},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"6281d47c1de98eb38f3aa0f6df29bbb19d412173","modified":1744315703550},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"af3946a595f997eb43d9af87428e4898c9acbc82","modified":1744315703552},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"439c37d7177ebc407206a432168536af41dea826","modified":1744315703549},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"f57f7e09eb6220f681fa8385082b0960502ce5c4","modified":1744315703515},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"1f6aba7820f1fb58b61969485148db21846e1aa9","modified":1744315703526},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"7d7bbc4a9970bd4c5449bc71b94364a8ec61e5d2","modified":1744315703541},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1668844772624},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"ebfe0954e3931431f46f913abe08d0212e06e7c2","modified":1744315703673},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"e21309d4165ebb6645084eed8dd749846ae981f1","modified":1744315703683},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"9ba4cceafd12c6d5ba8a6b986a046ef8319a7811","modified":1744315703324},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"a38c6d92b368bfc42c72ad799ad03f3274957065","modified":1744315703763},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"6f751f5c9499a39d7c5e1d323db3260342dd9431","modified":1744315703546},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"694b271819aab37ce473b15db9e6aded971d82e5","modified":1744315703555},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"523bbaeda463e82ab0be428cc0005717038ec63e","modified":1744315703548},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"93c1d103d9d16581c944c51f3d0638f57c80ee41","modified":1744315703631},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1671319837010},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"b870aae1271f3453b71e6d8cd6fc4a1448e52064","modified":1744315703564},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-footer.njk","hash":"bde2c7356d9362972bde41cc206d5816f8ed714d","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"bfff923526d6800218f08dba6ce0bbf5c17755fd","modified":1744315703654},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"6ad43135bd3aecf933ffdd750763e27ade36f97c","modified":1744315703637},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"bc5708e38b6070dff0cab6bf9480971017ce4dda","modified":1744315703662},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1671319837014},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"1efeeda00db08a3c033798228dd0092ee532cc36","modified":1744315703650},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/gitter.njk","hash":"f8cc14b7aa949999a1faaeb7855e2f20b59a386d","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"41b28f05e6233fb37700f7151f55868be10a0965","modified":1744315703593},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"55c2468b2b7f035881d494085527d6554f37b556","modified":1744315703612},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"00af4f5f9a79eaccf051f9e372d233d65d44c8a5","modified":1744315703449},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"8a8f752260be5b8098393f9879b61ffe904465e8","modified":1744315703518},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"5942feb3f31ed3480bf50b0f5a4a305b5bdca3d6","modified":1744315703563},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"e2b4a9d6b08155735ec336eedc506763d5671821","modified":1744315703568},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"a600a98e7436edeb31e291abca359885567df3c9","modified":1744315703468},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1668844772624},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"879b49f693af0c04c285b2dd0c9cccaf77347b7c","modified":1744315703740},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"20d5c6aa136bbb55e03906d98ee90ad3fbaa80a7","modified":1744315703745},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"2c800eaab6c613e5d091be2111aaa786641aa0c2","modified":1744315703739},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"0c490e9ba82efbb8bdf8465e6b537fafd51e1ed7","modified":1671319836987},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1744315703475},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"b8445c828d78a38e2de50bdc86b3bff66285ea0f","modified":1744315703688},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"6ec500fa4005993f2e080e260194f4d5323f589f","modified":1744315703691},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"48b2dfc04df6409c6e0736ccc11462ad97d349b1","modified":1744315703740},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"f316ba87f8d3299677fbf8345e1e993c35210e2e","modified":1744315703690},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"69c66aab4651e2e7ae9e65f08600144970648c60","modified":1744315703762},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"bcbf498d8d3ecea84324f0a59b7f95f389a52b8d","modified":1744315703735},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"dafc6d23c80d6fe3e55a7711e94210d2479b629a","modified":1744315703677},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"f23c53e32d140091b819be2603d1afbbb5d66933","modified":1744315703682},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"6569a6640f79d247a8235b3914772c0e2f99ead2","modified":1744315703681},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"3fbfab591f280e2e7f3b0265901c93bc4bd137ed","modified":1744315703679},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"e31f6adbb22a451f07e4661cff9a2f12e4e99a36","modified":1744315703682},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1668844772661},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"c29a827e82d2820ed8977c92994da73721200fac","modified":1744315703685},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"dc03835e42d82eaf2633cf3b627990ad3e1f5967","modified":1744315703680},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"a92c4eb16bdb7806079467eb022ccf193bb0f794","modified":1744315703682},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"a03f16ffc7dfdbdc6053f9fd68d77257ba0c559e","modified":1744315703683},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1668844772661},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e792a6233e1d4dbc5fd2f10ae97b7a790b82568b","modified":1744315703685},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1744315703513},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1744315703466},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1744315703469},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1668844772624},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4262628e173b16c4c6c18f817173dd103fb9e9a8","modified":1744315703538},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"93d3c39aded8d0140e63e70b896bd3d34c187c68","modified":1744315703038},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"1d1b6d847215b16f26b230859d7e16501190ecc0","modified":1744315703543},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"51a97a33879289904cb523ddc2d88b5b0c60fa72","modified":1744315703712},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"026cd5735fd2a75bb60b7bf8bd09139583d602b9","modified":1744315703747},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"93f014809d6442da23f8b7d729f7375e2badba7d","modified":1744315703746},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"809bab3414b1eb1ae44444eb821126868f764414","modified":1744315703746},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"ebfba158a0a4af3d1dabcacbc58986664de52140","modified":1744315703751},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"b47fb36915962309553ff7fb1782341585ed2b76","modified":1744315703751},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"424de4f64b12c521e8c6bfbc711d7961490ab36e","modified":1744315703750},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitter.styl","hash":"35104dc6883a61c31e0e368dac8ac2f697be62fe","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"54d12e2c5d9982f7b9e5b23be5133954a8514e9d","modified":1744315703725},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"09afd355e9d55cb97bbfe55178aa3480adce8f31","modified":1744315703753},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4e967702cf4c637132346bc74ec8854426f1a68c","modified":1744315703727},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/related-posts.styl","hash":"41ed817e1eb64078074e245e771446ee041e5790","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"a3dd3edea9c01b66b28a8367185269b9dcc3bdee","modified":1744315703738},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"21acb11e397526132605eef23bde7b307aa1eab5","modified":1744315703730},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"0847400d8579b0a2dd1bf662c78954c10adf2680","modified":1744315703754},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"ce36bf1602233298e0351b4babc592315529eb26","modified":1744315703755},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"fbdb63c6a8887d19b7137325ba7d6806f728139c","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"46eece42510c2c89bb9209afb0262ad76a4b0b36","modified":1744315703755},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"24752d145c6fb8f5344dca9c7b9640839c02e009","modified":1744315703756},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"c2e354a565c8c1b32bd0ceacc972b17982758b67","modified":1744315703757},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"741566d6ac5f852b5c8dee6a8996b65e48e7c97f","modified":1744315703757},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"f634f94828620e88c3f5a8db56f7944f6ba232b0","modified":1744315703715},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"9b0217e1caecd91e05572c7e8e52d32016ca312f","modified":1744315703730},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1668844772664},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"22cd37bd5df9972d5074710896aba4424ad5161c","modified":1744315703733},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"49329a7144f3413d1c832e52a1f4954171ef11e1","modified":1744315703737},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"8213015d9cae45d2c9945f8aba9d8db39c734efc","modified":1744315703744},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"c3be8b0738f693e750486bb71769c3dbbec174cc","modified":1744315703761},{"_id":"source/_posts/Gallery/Cat/Funny/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1668848912792},{"_id":"source/_posts/Gallery/Cat/Funny/greasycat.jpg","hash":"980b1b329451b35edb669f4199ea2af14cfdbb63","modified":1668848912792},{"_id":"public/search.xml","hash":"398b1f686f3be77e1675c2a9f4841c4c9a3927e7","modified":1744317704987},{"_id":"public/sitemap.xml","hash":"9de960bb59ac35cacfda250623b4f7cdff47a2d9","modified":1744317704987},{"_id":"public/sitemap.txt","hash":"18c0cca19be8d96077df91a6213e0de026453ac5","modified":1744317704987},{"_id":"public/categories/index.html","hash":"281d5db46054e5c17975b8c46dcbc772eecd2ea5","modified":1744317704987},{"_id":"public/404/index.html","hash":"40a7fbac786f8df57b3d4f5ad86096232d399924","modified":1744317704987},{"_id":"public/tags/index.html","hash":"ad3321dfeb1510b36f5e5482f41fb66e6aec3334","modified":1744317704987},{"_id":"public/2022/11/18/Gallery/Cat/Funny/index.html","hash":"a7c1340bb97c554bc976d220fe409a2c646589d8","modified":1671320270199},{"_id":"public/2022/06/03/System/archlinux-emoji-display-problem/index.html","hash":"75cdc65ad426a6eb64a0415fa196642661eef2df","modified":1744317704987},{"_id":"public/2022/06/01/System/archlinux-zoom-oversized-window/index.html","hash":"dffc785cc4c822c60ee1725ffb48c5a52fbe6469","modified":1671320270199},{"_id":"public/categories/System/index.html","hash":"2461813359f6b2f23dad9fee547ca54a7396a3c0","modified":1744317704987},{"_id":"public/categories/Gallery/index.html","hash":"c440b6023db59eccc800a992b52115a4253be712","modified":1744317704987},{"_id":"public/categories/Gallery/Cat/index.html","hash":"419cbf7a73c1a29c6c2ddecad9beb56866b5f651","modified":1744317704987},{"_id":"public/archives/index.html","hash":"bbeed707239ab4c95854ef01dda06b92d4751075","modified":1744317704987},{"_id":"public/archives/2022/index.html","hash":"94e5b04c1b2ba53e3bb52bdb9f4ed2a627e961dc","modified":1744317704987},{"_id":"public/archives/2022/06/index.html","hash":"575f8e1eabd5f54ef83a38560b5442d605168490","modified":1744317704987},{"_id":"public/archives/2022/11/index.html","hash":"1e32b93296e6be97d9c1bb61583f46dca387131a","modified":1744317704987},{"_id":"public/tags/archlinux/index.html","hash":"dbfdba7aabc26a0b39fa8722f1ac184521993fda","modified":1744317704987},{"_id":"public/tags/installation/index.html","hash":"df9e488df64e7270d43e3a2f8a40e0cc779ca6c6","modified":1744317704987},{"_id":"public/tags/deep-learning/index.html","hash":"146722eaf670f27e82bcac74e5d17d322f7d483f","modified":1744317704987},{"_id":"public/tags/zoom/index.html","hash":"cd2e11b1d0f01040acb2055def543e13db650d0a","modified":1744317704987},{"_id":"public/tags/solved/index.html","hash":"9b539480a3fa94e3b1c3cd8d36fc705baa95c755","modified":1744317704987},{"_id":"public/2022/11/19/System/archlinux-easy-setup-for-deep-learning/index.html","hash":"f52311d33d7d87373f6bc0230254d26434231fd3","modified":1744317704987},{"_id":"public/index.html","hash":"7154b89b496dc4a3a3a5fe34429ba21c13ac792d","modified":1744317704987},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1744317704987},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1744317704987},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1744317704987},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1744317704987},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"a38c6d92b368bfc42c72ad799ad03f3274957065","modified":1744317704987},{"_id":"public/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1744317704987},{"_id":"public/css/noscript.css","hash":"4cd5301e478e0e0d4b176740ec314087ec5cb707","modified":1744317704987},{"_id":"public/js/bookmark.js","hash":"9ba4cceafd12c6d5ba8a6b986a046ef8319a7811","modified":1744317704987},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1744317704987},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1744317704987},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1744317704987},{"_id":"public/js/motion.js","hash":"6f751f5c9499a39d7c5e1d323db3260342dd9431","modified":1744317704987},{"_id":"public/js/next-boot.js","hash":"523bbaeda463e82ab0be428cc0005717038ec63e","modified":1744317704987},{"_id":"public/js/pjax.js","hash":"694b271819aab37ce473b15db9e6aded971d82e5","modified":1744317704987},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1744317704987},{"_id":"public/js/utils.js","hash":"b870aae1271f3453b71e6d8cd6fc4a1448e52064","modified":1744317704987},{"_id":"public/js/schemes/muse.js","hash":"9794bd4fc6a458322949d6a0ade89cd1026bc69f","modified":1668858639243},{"_id":"public/js/third-party/fancybox.js","hash":"819f382c561fe5ec23c67cc5fabd63dd1cc22dc1","modified":1744317704987},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1744317704987},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1744317704987},{"_id":"public/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1668858639243},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1744317704987},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"def07bcc7c17d8a0caad177fb1dd2f3a5e5b3536","modified":1744317704987},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1744317704987},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1744317704987},{"_id":"public/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1668858639243},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1744317704987},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1744317704987},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1744317704987},{"_id":"public/js/third-party/comments/disqus.js","hash":"da361917d65e5dca8362f8cdeb6c8cc0e8316cec","modified":1744317704987},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"1e826dea3f684c0515f362dc1352447a1f0eae71","modified":1744317704987},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1744317704987},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1744317704987},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1744317704987},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1744317704987},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1744317704987},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1744317704987},{"_id":"public/js/third-party/search/algolia-search.js","hash":"93d3c39aded8d0140e63e70b896bd3d34c187c68","modified":1744317704987},{"_id":"public/js/third-party/search/local-search.js","hash":"4262628e173b16c4c6c18f817173dd103fb9e9a8","modified":1744317704987},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1744317704987},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1744317704987},{"_id":"public/js/third-party/tags/mermaid.js","hash":"1d1b6d847215b16f26b230859d7e16501190ecc0","modified":1744317704987},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1744317704987},{"_id":"public/css/main.css","hash":"8526c8eea57f8bbc184cf7bf715f09927634cdaa","modified":1744317704987},{"_id":"public/images/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1744317704987},{"_id":"public/2022/11/18/Gallery/Cat/Funny/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1668858639243},{"_id":"public/2022/11/18/Gallery/Cat/Funny/greasycat.jpg","hash":"980b1b329451b35edb669f4199ea2af14cfdbb63","modified":1668858639243},{"_id":"source/_posts/.Rhistory","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671049517931},{"_id":"source/_posts/Naive-Neural-Network-in-Cpp.md","hash":"3ca27581eefe2c196cdb9797addf74d3870f3e59","modified":1671050038702},{"_id":"public/2021/01/05/Naive-Neural-Network-in-Cpp/index.html","hash":"ecddfcd4ad830dc88c658efc2b4288143d8f522a","modified":1671050041739},{"_id":"public/archives/2021/index.html","hash":"ff5218ade4392c31f2d150c6dce929f9687eedaa","modified":1671320270199},{"_id":"public/archives/2021/01/index.html","hash":"aeb14fd200258a05e4a77f3e92a3f4eb52a96f88","modified":1671320270199},{"_id":"public/categories/Code/index.html","hash":"2c259d642e62a584ae1cb9189f7eeffbee618836","modified":1671320270199},{"_id":"public/tags/c/index.html","hash":"8cf55daa23ab089f488041874c3e19af2f1ae3af","modified":1671320270199},{"_id":"public/tags/machine-learning/index.html","hash":"f7267f3e4478c86a09a473f709cdb1c1d9e21b40","modified":1671320270199},{"_id":"public/tags/neural-network/index.html","hash":"24be4def8056c581da5d33518e0baa417f4512d7","modified":1671320270199},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp.md","hash":"59cfaa1ffd474f79cdf56df39b8fa791ccf51632","modified":1671050041996},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp/index.html","hash":"4bcdc5b5568e70e932e6ab09b52635e739514879","modified":1671050061831},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp-1.md","hash":"c175a569424a260fbfc7efd56fac5852c89335f8","modified":1671233235785},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp-1/index.html","hash":"7fd40c0f49c6f9bf5ed9f0cc094056f858b98d17","modified":1671320270199},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp-2.md","hash":"af03eb55ca4710e071db98c45271d629bcd1325d","modified":1671235060452},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp-2/index.html","hash":"7a8076493a35f688aa25370ce0e68f0befa96e2a","modified":1671320270199},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp-3.md","hash":"3de59dd8045952564530489d84966648ebaa6bb6","modified":1671235128323},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp-4 Network.md","hash":"0e169ee279e0e11aa3cba7503f82c30d7abad1d7","modified":1671235097462},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp-3/index.html","hash":"c3b32ba403e23d2823c8751246f9548e0b08d275","modified":1671320270199},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp-4 Network/index.html","hash":"cdcb4b7555c56ec8e240bbd644e58bf6e8adb385","modified":1671320270199},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1671319836987},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"16696990e4ce65fc8db18c4635082a5d5d06ff07","modified":1744315703659},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"ef9f2bb7110507f1c4336800af9157d5fa9765bd","modified":1744315703652},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"3343750682fbd8535e50f8129be3003ad26015b4","modified":1744315703666},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"6829e998b39f8f143e20b276bb1f62d95a29de58","modified":1744315703783},{"_id":"source/_posts/Fun/setup_sunshine_hyperv.md","hash":"af8531ee327ef4fc2e2fe4b92484145a42ec9c3e","modified":1744317635752},{"_id":"node_modules/hexo-theme-next/languages/eo.yml","hash":"e34bb33ae827bf2f0727088599a73bc64bdad1b0","modified":1744315703771},{"_id":"node_modules/hexo-theme-next/layout/_third-party/addtoany.njk","hash":"ef64c6bfb8540cd874701236b9be47db2496e98e","modified":1744315703589},{"_id":"node_modules/hexo-theme-next/scripts/tags/wavedrom.js","hash":"b44dfeeb58b41945d469141787f3dbce4b117d08","modified":1744315703569},{"_id":"node_modules/hexo-theme-next/source/js/sidebar.js","hash":"2ee359ae48273b01ba1e0768704524e08702c7eb","modified":1744315703559},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/wavedrom.njk","hash":"02202bf563fb5eedde2ccad4d6c5b9109d30a703","modified":1744315703667},{"_id":"node_modules/hexo-theme-next/source/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1744315702932},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1744315703570},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-copyright.styl","hash":"56805b77fe236fac19e19c716a49363bcf986311","modified":1744315703756},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/fold.styl","hash":"42a0b65491ad85438596b3fe0b7f23973e4cef34","modified":1744315703717},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/wavedrom.styl","hash":"af113411ad9cca7674177be36af8dd399680834d","modified":1744315703763},{"_id":"public/2025/04/10/Fun/setup_sunshine_hyperv/index.html","hash":"268037891f3486b41568f7d9ea75f6991e5be1b5","modified":1744317704987},{"_id":"public/2022/11/19/Gallery/Cat/Funny/index.html","hash":"0eee8a7e46813622e0b42ea11fa2e16fc7a4cb8b","modified":1744317704987},{"_id":"public/2022/06/02/System/archlinux-zoom-oversized-window/index.html","hash":"c57096c18281f8aebef76280b0c5917cc9f1d698","modified":1744317704987},{"_id":"public/archives/2025/index.html","hash":"d18666a53ffeaf2153c7baf8f101621ccb516f45","modified":1744317704987},{"_id":"public/archives/2025/04/index.html","hash":"c4481e26b08b3943b476c97f00a22897b0564b51","modified":1744317704987},{"_id":"public/categories/Fun/index.html","hash":"d9519b0fd97cbeaf3df46b67fec2d03751e696bf","modified":1744317704987},{"_id":"public/tags/hyperv-sunshine-gaming/index.html","hash":"f0208a6a7000a36aa381a99329c51fef47926a2a","modified":1744317704987},{"_id":"public/js/sidebar.js","hash":"2ee359ae48273b01ba1e0768704524e08702c7eb","modified":1744317704987},{"_id":"public/js/third-party/addtoany.js","hash":"5276c8f78ee562a8965216dc67d762e59cb4a9f2","modified":1744317704987},{"_id":"public/js/third-party/tags/wavedrom.js","hash":"40dcd10df6edf124088c329346e0cc0bdac74ef1","modified":1744317704987},{"_id":"public/2022/11/19/Gallery/Cat/Funny/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1744317704987},{"_id":"public/2022/11/19/Gallery/Cat/Funny/greasycat.jpg","hash":"980b1b329451b35edb669f4199ea2af14cfdbb63","modified":1744317704987}],"Category":[{"name":"System","_id":"clanvcfhh00047bsb47q9auiv"},{"name":"Gallery","_id":"clanvcfhm000p7bsbashy10a6"},{"name":"Cat","parent":"clanvcfhm000p7bsbashy10a6","_id":"clanvcfhn000q7bsb6dnha1fg"},{"name":"Code","_id":"clbo3zivi0001jhsba6ho9lbw"},{"name":"Fun","_id":"cm9bs0vxm00032cpbh2oc4019"}],"Data":[],"Page":[{"title":"404","date":"2022-11-19T10:59:16.000Z","_content":"\n# You've reached the end of the cat's tail","source":"404/index.md","raw":"---\ntitle: 404\ndate: 2022-11-19 02:59:16\n---\n\n# You've reached the end of the cat's tail","updated":"2022-11-19T11:01:05.985Z","path":"404/index.html","comments":1,"layout":"page","_id":"clanvcfhd00007bsbbltue6e9","content":"<h1 id=\"You’ve-reached-the-end-of-the-cat’s-tail\"><a href=\"#You’ve-reached-the-end-of-the-cat’s-tail\" class=\"headerlink\" title=\"You’ve reached the end of the cat’s tail\"></a>You’ve reached the end of the cat’s tail</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"You’ve-reached-the-end-of-the-cat’s-tail\"><a href=\"#You’ve-reached-the-end-of-the-cat’s-tail\" class=\"headerlink\" title=\"You’ve reached the end of the cat’s tail\"></a>You’ve reached the end of the cat’s tail</h1>"},{"title":"Categories","date":"2022-11-19T09:41:09.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2022-11-19 01:41:09\ntype: \"categories\"\ncomments: false\n---\n","updated":"2022-11-19T09:43:16.215Z","path":"categories/index.html","layout":"page","_id":"clanvcfhg00027bsb0lwpcgwm","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","date":"2022-11-19T09:40:10.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2022-11-19 01:40:10\ntype: \"tags\"\ncomments: false\n---\n","updated":"2022-11-19T09:42:59.668Z","path":"tags/index.html","layout":"page","_id":"clanvcfhi00067bsb61he72xw","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"archlinux-easy-setup-for-deep-learning","date":"2022-11-19T10:13:52.000Z","_content":"\n# My setup\n- CPU: AMD Ryzen 9 5900X\n- GPU: NVIDIA GeForce RTX 3090 FE\n- Archlinux 6.0.8-arch1-1\n- WM i3\n\n# Update pacman source\n```bash\nsudo pacman -Syu\n```\n\n# Nvidia driver\n\n**Skip** if you've installed properitary nvidia driver\n- for those need to switch nouveu to nvidia, please check archlinux wiki for more information\n\n```bash\nsudo pacman -S nvidia nvidia-utils\n# or nvidia-dkms and linux-header for custom kernel\n```\n\n# CUDA & CUDNN\n```bash\nsudo pacman -S cuda cudnn\n```\n\nIf you havn't set the path for dynamic/shared library, it's time to set it. Otherwise errors like `# Unimplemented: DNN library is not found` might present when running CNN\n\n\nAdd the following line to either `~/.bashrc` or `~/.bash_profile` or `.profile` (last two are preferred)\n```bash\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/lib\"\n```\n\nVerify the installation\n```bash\ncd /opt/cuda/extras/demo_suite\n./deviceQuery\n```\n<!-- more -->\n```\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"NVIDIA GeForce RTX 3090\"\n  CUDA Driver Version / Runtime Version          11.8 / 11.8\n  CUDA Capability Major/Minor version number:    8.6\n  Total amount of global memory:                 24265 MBytes (25443893248 bytes)\n  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores\n  GPU Max Clock rate:                            1695 MHz (1.70 GHz)\n  Memory Clock rate:                             9751 Mhz\n  Memory Bus Width:                              384-bit\n  L2 Cache Size:                                 6291456 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  1536\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n  Run time limit on kernels:                     Yes\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090\nResult = PASS\n```\n\n# Tensorflow & Keras\n\n`python-tensorflow-opt-cuda` vs `python-tensorflow-cuda`\n- opt might be slightly faster as the package is optimized for certain intel cpu\n\n```bash\nsudo pacman -S python-tensorflow-cuda keras\n```\n\n# Try it out!\n```python\nfrom keras import layers  \nfrom keras import models  \nmodel = models.Sequential()  \nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))  \nmodel.add(layers.MaxPooling2D((2, 2)))  \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))  \nmodel.add(layers.MaxPooling2D((2, 2)))  \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))  \nmodel.add(layers.Flatten())  \nmodel.add(layers.Dense(64, activation='relu'))  \nmodel.add(layers.Dense(10, activation='softmax'))  \nprint(model.summary())  \n  \nfrom keras.datasets import mnist  \nfrom keras.utils import to_categorical  \n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  \n  \ntrain_images = train_images.reshape((60000, 28, 28, 1))  \ntrain_images = train_images.astype('float32') / 255  \ntest_images = test_images.reshape((10000, 28, 28, 1))  \ntest_images = test_images.astype('float32') / 255  \ntrain_labels = to_categorical(train_labels)  \ntest_labels = to_categorical(test_labels)  \nmodel.compile(optimizer='rmsprop',  \n\tloss='categorical_crossentropy',  \n\tmetrics=['accuracy'])  \nmodel.fit(train_images, train_labels, epochs=5, batch_size=64)  \n  \ntest_loss, test_acc = model.evaluate(test_images, test_labels)  \nprint('test_acc:', test_acc)\n```\n\n```\n2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n                                                                 \n flatten (Flatten)           (None, 576)               0         \n                                                                 \n dense (Dense)               (None, 64)                36928     \n                                                                 \n dense_1 (Dense)             (None, 10)                650       \n                                                                 \n=================================================================\nTotal params: 93,322\nTrainable params: 93,322\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/5\n2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469\nEpoch 2/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853\nEpoch 3/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902\nEpoch 4/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919\nEpoch 5/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940\n313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880\ntest_acc: 0.9879999756813049\n```","source":"_posts/System/archlinux-easy-setup-for-deep-learning.md","raw":"---\ntitle: archlinux-easy-setup-for-deep-learning\ncategories:\n  - System\ndate: 2022-11-19 02:13:52\ntags: \n- archlinux\n- installation\n- deep-learning\n---\n\n# My setup\n- CPU: AMD Ryzen 9 5900X\n- GPU: NVIDIA GeForce RTX 3090 FE\n- Archlinux 6.0.8-arch1-1\n- WM i3\n\n# Update pacman source\n```bash\nsudo pacman -Syu\n```\n\n# Nvidia driver\n\n**Skip** if you've installed properitary nvidia driver\n- for those need to switch nouveu to nvidia, please check archlinux wiki for more information\n\n```bash\nsudo pacman -S nvidia nvidia-utils\n# or nvidia-dkms and linux-header for custom kernel\n```\n\n# CUDA & CUDNN\n```bash\nsudo pacman -S cuda cudnn\n```\n\nIf you havn't set the path for dynamic/shared library, it's time to set it. Otherwise errors like `# Unimplemented: DNN library is not found` might present when running CNN\n\n\nAdd the following line to either `~/.bashrc` or `~/.bash_profile` or `.profile` (last two are preferred)\n```bash\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/lib\"\n```\n\nVerify the installation\n```bash\ncd /opt/cuda/extras/demo_suite\n./deviceQuery\n```\n<!-- more -->\n```\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"NVIDIA GeForce RTX 3090\"\n  CUDA Driver Version / Runtime Version          11.8 / 11.8\n  CUDA Capability Major/Minor version number:    8.6\n  Total amount of global memory:                 24265 MBytes (25443893248 bytes)\n  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores\n  GPU Max Clock rate:                            1695 MHz (1.70 GHz)\n  Memory Clock rate:                             9751 Mhz\n  Memory Bus Width:                              384-bit\n  L2 Cache Size:                                 6291456 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  1536\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n  Run time limit on kernels:                     Yes\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090\nResult = PASS\n```\n\n# Tensorflow & Keras\n\n`python-tensorflow-opt-cuda` vs `python-tensorflow-cuda`\n- opt might be slightly faster as the package is optimized for certain intel cpu\n\n```bash\nsudo pacman -S python-tensorflow-cuda keras\n```\n\n# Try it out!\n```python\nfrom keras import layers  \nfrom keras import models  \nmodel = models.Sequential()  \nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))  \nmodel.add(layers.MaxPooling2D((2, 2)))  \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))  \nmodel.add(layers.MaxPooling2D((2, 2)))  \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))  \nmodel.add(layers.Flatten())  \nmodel.add(layers.Dense(64, activation='relu'))  \nmodel.add(layers.Dense(10, activation='softmax'))  \nprint(model.summary())  \n  \nfrom keras.datasets import mnist  \nfrom keras.utils import to_categorical  \n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  \n  \ntrain_images = train_images.reshape((60000, 28, 28, 1))  \ntrain_images = train_images.astype('float32') / 255  \ntest_images = test_images.reshape((10000, 28, 28, 1))  \ntest_images = test_images.astype('float32') / 255  \ntrain_labels = to_categorical(train_labels)  \ntest_labels = to_categorical(test_labels)  \nmodel.compile(optimizer='rmsprop',  \n\tloss='categorical_crossentropy',  \n\tmetrics=['accuracy'])  \nmodel.fit(train_images, train_labels, epochs=5, batch_size=64)  \n  \ntest_loss, test_acc = model.evaluate(test_images, test_labels)  \nprint('test_acc:', test_acc)\n```\n\n```\n2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n                                                                 \n flatten (Flatten)           (None, 576)               0         \n                                                                 \n dense (Dense)               (None, 64)                36928     \n                                                                 \n dense_1 (Dense)             (None, 10)                650       \n                                                                 \n=================================================================\nTotal params: 93,322\nTrainable params: 93,322\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/5\n2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469\nEpoch 2/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853\nEpoch 3/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902\nEpoch 4/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919\nEpoch 5/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940\n313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880\ntest_acc: 0.9879999756813049\n```","slug":"System/archlinux-easy-setup-for-deep-learning","published":1,"updated":"2022-11-19T10:49:23.495Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clanvcfhe00017bsbfe457cxc","content":"<h1 id=\"My-setup\"><a href=\"#My-setup\" class=\"headerlink\" title=\"My setup\"></a>My setup</h1><ul>\n<li>CPU: AMD Ryzen 9 5900X</li>\n<li>GPU: NVIDIA GeForce RTX 3090 FE</li>\n<li>Archlinux 6.0.8-arch1-1</li>\n<li>WM i3</li>\n</ul>\n<h1 id=\"Update-pacman-source\"><a href=\"#Update-pacman-source\" class=\"headerlink\" title=\"Update pacman source\"></a>Update pacman source</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -Syu</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Nvidia-driver\"><a href=\"#Nvidia-driver\" class=\"headerlink\" title=\"Nvidia driver\"></a>Nvidia driver</h1><p><strong>Skip</strong> if you’ve installed properitary nvidia driver</p>\n<ul>\n<li>for those need to switch nouveu to nvidia, please check archlinux wiki for more information</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S nvidia nvidia-utils</span><br><span class=\"line\"><span class=\"comment\"># or nvidia-dkms and linux-header for custom kernel</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"CUDA-amp-CUDNN\"><a href=\"#CUDA-amp-CUDNN\" class=\"headerlink\" title=\"CUDA &amp; CUDNN\"></a>CUDA &amp; CUDNN</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S cuda cudnn</span><br></pre></td></tr></table></figure>\n\n<p>If you havn’t set the path for dynamic&#x2F;shared library, it’s time to set it. Otherwise errors like <code># Unimplemented: DNN library is not found</code> might present when running CNN</p>\n<p>Add the following line to either <code>~/.bashrc</code> or <code>~/.bash_profile</code> or <code>.profile</code> (last two are preferred)</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> LD_LIBRARY_PATH=<span class=\"string\">&quot;<span class=\"variable\">$&#123;LD_LIBRARY_PATH&#125;</span>:/usr/lib&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>Verify the installation</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> /opt/cuda/extras/demo_suite</span><br><span class=\"line\">./deviceQuery</span><br></pre></td></tr></table></figure>\n<span id=\"more\"></span>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class=\"line\"></span><br><span class=\"line\">Detected 1 CUDA Capable device(s)</span><br><span class=\"line\"></span><br><span class=\"line\">Device 0: &quot;NVIDIA GeForce RTX 3090&quot;</span><br><span class=\"line\">  CUDA Driver Version / Runtime Version          11.8 / 11.8</span><br><span class=\"line\">  CUDA Capability Major/Minor version number:    8.6</span><br><span class=\"line\">  Total amount of global memory:                 24265 MBytes (25443893248 bytes)</span><br><span class=\"line\">  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores</span><br><span class=\"line\">  GPU Max Clock rate:                            1695 MHz (1.70 GHz)</span><br><span class=\"line\">  Memory Clock rate:                             9751 Mhz</span><br><span class=\"line\">  Memory Bus Width:                              384-bit</span><br><span class=\"line\">  L2 Cache Size:                                 6291456 bytes</span><br><span class=\"line\">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class=\"line\">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class=\"line\">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class=\"line\">  Total amount of constant memory:               65536 bytes</span><br><span class=\"line\">  Total amount of shared memory per block:       49152 bytes</span><br><span class=\"line\">  Total number of registers available per block: 65536</span><br><span class=\"line\">  Warp size:                                     32</span><br><span class=\"line\">  Maximum number of threads per multiprocessor:  1536</span><br><span class=\"line\">  Maximum number of threads per block:           1024</span><br><span class=\"line\">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class=\"line\">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class=\"line\">  Maximum memory pitch:                          2147483647 bytes</span><br><span class=\"line\">  Texture alignment:                             512 bytes</span><br><span class=\"line\">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class=\"line\">  Run time limit on kernels:                     Yes</span><br><span class=\"line\">  Integrated GPU sharing Host Memory:            No</span><br><span class=\"line\">  Support host page-locked memory mapping:       Yes</span><br><span class=\"line\">  Alignment requirement for Surfaces:            Yes</span><br><span class=\"line\">  Device has ECC support:                        Disabled</span><br><span class=\"line\">  Device supports Unified Addressing (UVA):      Yes</span><br><span class=\"line\">  Device supports Compute Preemption:            Yes</span><br><span class=\"line\">  Supports Cooperative Kernel Launch:            Yes</span><br><span class=\"line\">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class=\"line\">  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0</span><br><span class=\"line\">  Compute Mode:</span><br><span class=\"line\">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class=\"line\"></span><br><span class=\"line\">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090</span><br><span class=\"line\">Result = PASS</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Tensorflow-amp-Keras\"><a href=\"#Tensorflow-amp-Keras\" class=\"headerlink\" title=\"Tensorflow &amp; Keras\"></a>Tensorflow &amp; Keras</h1><p><code>python-tensorflow-opt-cuda</code> vs <code>python-tensorflow-cuda</code></p>\n<ul>\n<li>opt might be slightly faster as the package is optimized for certain intel cpu</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S python-tensorflow-cuda keras</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Try-it-out\"><a href=\"#Try-it-out\" class=\"headerlink\" title=\"Try it out!\"></a>Try it out!</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> layers  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> models  </span><br><span class=\"line\">model = models.Sequential()  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">32</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, input_shape=(<span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>)))  </span><br><span class=\"line\">model.add(layers.MaxPooling2D((<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.MaxPooling2D((<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.Flatten())  </span><br><span class=\"line\">model.add(layers.Dense(<span class=\"number\">64</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;softmax&#x27;</span>))  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(model.summary())  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.datasets <span class=\"keyword\">import</span> mnist  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.utils <span class=\"keyword\">import</span> to_categorical  </span><br><span class=\"line\">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  </span><br><span class=\"line\">  </span><br><span class=\"line\">train_images = train_images.reshape((<span class=\"number\">60000</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))  </span><br><span class=\"line\">train_images = train_images.astype(<span class=\"string\">&#x27;float32&#x27;</span>) / <span class=\"number\">255</span>  </span><br><span class=\"line\">test_images = test_images.reshape((<span class=\"number\">10000</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))  </span><br><span class=\"line\">test_images = test_images.astype(<span class=\"string\">&#x27;float32&#x27;</span>) / <span class=\"number\">255</span>  </span><br><span class=\"line\">train_labels = to_categorical(train_labels)  </span><br><span class=\"line\">test_labels = to_categorical(test_labels)  </span><br><span class=\"line\">model.<span class=\"built_in\">compile</span>(optimizer=<span class=\"string\">&#x27;rmsprop&#x27;</span>,  </span><br><span class=\"line\">\tloss=<span class=\"string\">&#x27;categorical_crossentropy&#x27;</span>,  </span><br><span class=\"line\">\tmetrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])  </span><br><span class=\"line\">model.fit(train_images, train_labels, epochs=<span class=\"number\">5</span>, batch_size=<span class=\"number\">64</span>)  </span><br><span class=\"line\">  </span><br><span class=\"line\">test_loss, test_acc = model.evaluate(test_images, test_labels)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;test_acc:&#x27;</span>, test_acc)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class=\"line\">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class=\"line\">2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</span><br><span class=\"line\">2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class=\"line\">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class=\"line\">2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6</span><br><span class=\"line\">2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.</span><br><span class=\"line\">Model: &quot;sequential&quot;</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\"> Layer (type)                Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"> conv2d (Conv2D)             (None, 26, 26, 32)        320       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         </span><br><span class=\"line\"> )                                                               </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         </span><br><span class=\"line\"> 2D)                                                             </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> flatten (Flatten)           (None, 576)               0         </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense (Dense)               (None, 64)                36928     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_1 (Dense)             (None, 10)                650       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 93,322</span><br><span class=\"line\">Trainable params: 93,322</span><br><span class=\"line\">Non-trainable params: 0</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">None</span><br><span class=\"line\">Epoch 1/5</span><br><span class=\"line\">2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600</span><br><span class=\"line\">2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.</span><br><span class=\"line\">938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469</span><br><span class=\"line\">Epoch 2/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853</span><br><span class=\"line\">Epoch 3/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902</span><br><span class=\"line\">Epoch 4/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919</span><br><span class=\"line\">Epoch 5/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940</span><br><span class=\"line\">313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880</span><br><span class=\"line\">test_acc: 0.9879999756813049</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<h1 id=\"My-setup\"><a href=\"#My-setup\" class=\"headerlink\" title=\"My setup\"></a>My setup</h1><ul>\n<li>CPU: AMD Ryzen 9 5900X</li>\n<li>GPU: NVIDIA GeForce RTX 3090 FE</li>\n<li>Archlinux 6.0.8-arch1-1</li>\n<li>WM i3</li>\n</ul>\n<h1 id=\"Update-pacman-source\"><a href=\"#Update-pacman-source\" class=\"headerlink\" title=\"Update pacman source\"></a>Update pacman source</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -Syu</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Nvidia-driver\"><a href=\"#Nvidia-driver\" class=\"headerlink\" title=\"Nvidia driver\"></a>Nvidia driver</h1><p><strong>Skip</strong> if you’ve installed properitary nvidia driver</p>\n<ul>\n<li>for those need to switch nouveu to nvidia, please check archlinux wiki for more information</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S nvidia nvidia-utils</span><br><span class=\"line\"><span class=\"comment\"># or nvidia-dkms and linux-header for custom kernel</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"CUDA-amp-CUDNN\"><a href=\"#CUDA-amp-CUDNN\" class=\"headerlink\" title=\"CUDA &amp; CUDNN\"></a>CUDA &amp; CUDNN</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S cuda cudnn</span><br></pre></td></tr></table></figure>\n\n<p>If you havn’t set the path for dynamic&#x2F;shared library, it’s time to set it. Otherwise errors like <code># Unimplemented: DNN library is not found</code> might present when running CNN</p>\n<p>Add the following line to either <code>~/.bashrc</code> or <code>~/.bash_profile</code> or <code>.profile</code> (last two are preferred)</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> LD_LIBRARY_PATH=<span class=\"string\">&quot;<span class=\"variable\">$&#123;LD_LIBRARY_PATH&#125;</span>:/usr/lib&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>Verify the installation</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> /opt/cuda/extras/demo_suite</span><br><span class=\"line\">./deviceQuery</span><br></pre></td></tr></table></figure>","more":"<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class=\"line\"></span><br><span class=\"line\">Detected 1 CUDA Capable device(s)</span><br><span class=\"line\"></span><br><span class=\"line\">Device 0: &quot;NVIDIA GeForce RTX 3090&quot;</span><br><span class=\"line\">  CUDA Driver Version / Runtime Version          11.8 / 11.8</span><br><span class=\"line\">  CUDA Capability Major/Minor version number:    8.6</span><br><span class=\"line\">  Total amount of global memory:                 24265 MBytes (25443893248 bytes)</span><br><span class=\"line\">  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores</span><br><span class=\"line\">  GPU Max Clock rate:                            1695 MHz (1.70 GHz)</span><br><span class=\"line\">  Memory Clock rate:                             9751 Mhz</span><br><span class=\"line\">  Memory Bus Width:                              384-bit</span><br><span class=\"line\">  L2 Cache Size:                                 6291456 bytes</span><br><span class=\"line\">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class=\"line\">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class=\"line\">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class=\"line\">  Total amount of constant memory:               65536 bytes</span><br><span class=\"line\">  Total amount of shared memory per block:       49152 bytes</span><br><span class=\"line\">  Total number of registers available per block: 65536</span><br><span class=\"line\">  Warp size:                                     32</span><br><span class=\"line\">  Maximum number of threads per multiprocessor:  1536</span><br><span class=\"line\">  Maximum number of threads per block:           1024</span><br><span class=\"line\">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class=\"line\">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class=\"line\">  Maximum memory pitch:                          2147483647 bytes</span><br><span class=\"line\">  Texture alignment:                             512 bytes</span><br><span class=\"line\">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class=\"line\">  Run time limit on kernels:                     Yes</span><br><span class=\"line\">  Integrated GPU sharing Host Memory:            No</span><br><span class=\"line\">  Support host page-locked memory mapping:       Yes</span><br><span class=\"line\">  Alignment requirement for Surfaces:            Yes</span><br><span class=\"line\">  Device has ECC support:                        Disabled</span><br><span class=\"line\">  Device supports Unified Addressing (UVA):      Yes</span><br><span class=\"line\">  Device supports Compute Preemption:            Yes</span><br><span class=\"line\">  Supports Cooperative Kernel Launch:            Yes</span><br><span class=\"line\">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class=\"line\">  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0</span><br><span class=\"line\">  Compute Mode:</span><br><span class=\"line\">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class=\"line\"></span><br><span class=\"line\">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090</span><br><span class=\"line\">Result = PASS</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Tensorflow-amp-Keras\"><a href=\"#Tensorflow-amp-Keras\" class=\"headerlink\" title=\"Tensorflow &amp; Keras\"></a>Tensorflow &amp; Keras</h1><p><code>python-tensorflow-opt-cuda</code> vs <code>python-tensorflow-cuda</code></p>\n<ul>\n<li>opt might be slightly faster as the package is optimized for certain intel cpu</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S python-tensorflow-cuda keras</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Try-it-out\"><a href=\"#Try-it-out\" class=\"headerlink\" title=\"Try it out!\"></a>Try it out!</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> layers  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> models  </span><br><span class=\"line\">model = models.Sequential()  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">32</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, input_shape=(<span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>)))  </span><br><span class=\"line\">model.add(layers.MaxPooling2D((<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.MaxPooling2D((<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.Flatten())  </span><br><span class=\"line\">model.add(layers.Dense(<span class=\"number\">64</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;softmax&#x27;</span>))  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(model.summary())  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.datasets <span class=\"keyword\">import</span> mnist  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.utils <span class=\"keyword\">import</span> to_categorical  </span><br><span class=\"line\">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  </span><br><span class=\"line\">  </span><br><span class=\"line\">train_images = train_images.reshape((<span class=\"number\">60000</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))  </span><br><span class=\"line\">train_images = train_images.astype(<span class=\"string\">&#x27;float32&#x27;</span>) / <span class=\"number\">255</span>  </span><br><span class=\"line\">test_images = test_images.reshape((<span class=\"number\">10000</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))  </span><br><span class=\"line\">test_images = test_images.astype(<span class=\"string\">&#x27;float32&#x27;</span>) / <span class=\"number\">255</span>  </span><br><span class=\"line\">train_labels = to_categorical(train_labels)  </span><br><span class=\"line\">test_labels = to_categorical(test_labels)  </span><br><span class=\"line\">model.<span class=\"built_in\">compile</span>(optimizer=<span class=\"string\">&#x27;rmsprop&#x27;</span>,  </span><br><span class=\"line\">\tloss=<span class=\"string\">&#x27;categorical_crossentropy&#x27;</span>,  </span><br><span class=\"line\">\tmetrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])  </span><br><span class=\"line\">model.fit(train_images, train_labels, epochs=<span class=\"number\">5</span>, batch_size=<span class=\"number\">64</span>)  </span><br><span class=\"line\">  </span><br><span class=\"line\">test_loss, test_acc = model.evaluate(test_images, test_labels)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;test_acc:&#x27;</span>, test_acc)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class=\"line\">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class=\"line\">2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</span><br><span class=\"line\">2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class=\"line\">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class=\"line\">2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6</span><br><span class=\"line\">2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.</span><br><span class=\"line\">Model: &quot;sequential&quot;</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\"> Layer (type)                Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"> conv2d (Conv2D)             (None, 26, 26, 32)        320       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         </span><br><span class=\"line\"> )                                                               </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         </span><br><span class=\"line\"> 2D)                                                             </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> flatten (Flatten)           (None, 576)               0         </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense (Dense)               (None, 64)                36928     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_1 (Dense)             (None, 10)                650       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 93,322</span><br><span class=\"line\">Trainable params: 93,322</span><br><span class=\"line\">Non-trainable params: 0</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">None</span><br><span class=\"line\">Epoch 1/5</span><br><span class=\"line\">2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600</span><br><span class=\"line\">2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.</span><br><span class=\"line\">938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469</span><br><span class=\"line\">Epoch 2/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853</span><br><span class=\"line\">Epoch 3/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902</span><br><span class=\"line\">Epoch 4/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919</span><br><span class=\"line\">Epoch 5/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940</span><br><span class=\"line\">313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880</span><br><span class=\"line\">test_acc: 0.9879999756813049</span><br></pre></td></tr></table></figure>"},{"title":"Emoji Not Showing Properly on Archlinux.md","date":"2022-06-03T09:26:39.000Z","_content":"\nThe issue persisted for a long time. \n\nEmojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today \n\n[Original Solution](https://flammie.github.io/dotfiles/fontconfig.html)\n\nInsert the following lines to `/etc/font/fonts.conf` inside the `<font-config>` tag\n\n```xml\n<match target=\"font\">\n\t\t<test name=\"family\" compare=\"contains\">\n\t\t\t<string>Emoji</string>\n\t\t</test>\n\t\t<edit name=\"hinting\" mode=\"assign\">\n\t\t\t<bool>true</bool>\n\t\t</edit>\n\t\t<edit name=\"hintstyle\" mode=\"assign\">\n\t\t\t<const>hintslight</const>\n\t\t</edit>\n\t\t<edit name=\"embeddedbitmap\" mode=\"assign\">\n\t\t\t<bool>true</bool>\n\t\t</edit>\n\t</match>\n```\n","source":"_posts/System/archlinux-emoji-display-problem.md","raw":"---\ntitle: Emoji Not Showing Properly on Archlinux.md\ncategories:\n  - System\ndate: 2022-06-03 02:26:39\n---\n\nThe issue persisted for a long time. \n\nEmojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today \n\n[Original Solution](https://flammie.github.io/dotfiles/fontconfig.html)\n\nInsert the following lines to `/etc/font/fonts.conf` inside the `<font-config>` tag\n\n```xml\n<match target=\"font\">\n\t\t<test name=\"family\" compare=\"contains\">\n\t\t\t<string>Emoji</string>\n\t\t</test>\n\t\t<edit name=\"hinting\" mode=\"assign\">\n\t\t\t<bool>true</bool>\n\t\t</edit>\n\t\t<edit name=\"hintstyle\" mode=\"assign\">\n\t\t\t<const>hintslight</const>\n\t\t</edit>\n\t\t<edit name=\"embeddedbitmap\" mode=\"assign\">\n\t\t\t<bool>true</bool>\n\t\t</edit>\n\t</match>\n```\n","slug":"System/archlinux-emoji-display-problem","published":1,"updated":"2022-11-19T10:04:46.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clanvcfhg00037bsb15ui6tgh","content":"<p>The issue persisted for a long time. </p>\n<p>Emojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today </p>\n<p><a href=\"https://flammie.github.io/dotfiles/fontconfig.html\">Original Solution</a></p>\n<p>Insert the following lines to <code>/etc/font/fonts.conf</code> inside the <code>&lt;font-config&gt;</code> tag</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">match</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;font&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">test</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;family&quot;</span> <span class=\"attr\">compare</span>=<span class=\"string\">&quot;contains&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">string</span>&gt;</span>Emoji<span class=\"tag\">&lt;/<span class=\"name\">string</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">test</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;hinting&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">bool</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">bool</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;hintstyle&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">const</span>&gt;</span>hintslight<span class=\"tag\">&lt;/<span class=\"name\">const</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;embeddedbitmap&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">bool</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">bool</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">match</span>&gt;</span></span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>The issue persisted for a long time. </p>\n<p>Emojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today </p>\n<p><a href=\"https://flammie.github.io/dotfiles/fontconfig.html\">Original Solution</a></p>\n<p>Insert the following lines to <code>/etc/font/fonts.conf</code> inside the <code>&lt;font-config&gt;</code> tag</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">match</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;font&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">test</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;family&quot;</span> <span class=\"attr\">compare</span>=<span class=\"string\">&quot;contains&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">string</span>&gt;</span>Emoji<span class=\"tag\">&lt;/<span class=\"name\">string</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">test</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;hinting&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">bool</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">bool</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;hintstyle&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">const</span>&gt;</span>hintslight<span class=\"tag\">&lt;/<span class=\"name\">const</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;embeddedbitmap&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">bool</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">bool</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">match</span>&gt;</span></span><br></pre></td></tr></table></figure>\n"},{"title":"Missing or Oversized Zoom Window on Arch","draft":false,"date":"2022-06-02T06:32:52.000Z","_content":"\nThe zoom client installed from AUR (and also the one from flatpak) had two weird issues:\n\n1. the window didn't show up\n2. the window is oversize and cannot be rescaled.\n\n\nchecked the terminal output (there's no output) and the log file under `$HOME/.zoom/log` (nothing suspicious).\n\n# Solution\nAfter few google searches, I found a workaround\n\n```sh\nvim $HOME/.config/zoomus.conf\n```\n\n```toml\nautoScale=false\n```\n\nI reinstalled the zoom (restart also works) and it worked\n\n","source":"_posts/System/archlinux-zoom-oversized-window.md","raw":"---\ntitle: Missing or Oversized Zoom Window on Arch\ntags:\n  - archlinux\n  - zoom\n  - solved\ndraft: false\ncategories:\n  - System\ndate: 2022-06-01 23:32:52\n---\n\nThe zoom client installed from AUR (and also the one from flatpak) had two weird issues:\n\n1. the window didn't show up\n2. the window is oversize and cannot be rescaled.\n\n\nchecked the terminal output (there's no output) and the log file under `$HOME/.zoom/log` (nothing suspicious).\n\n# Solution\nAfter few google searches, I found a workaround\n\n```sh\nvim $HOME/.config/zoomus.conf\n```\n\n```toml\nautoScale=false\n```\n\nI reinstalled the zoom (restart also works) and it worked\n\n","slug":"System/archlinux-zoom-oversized-window","published":1,"updated":"2022-11-19T10:05:15.799Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clanvcfhi00077bsb4vg47tup","content":"<p>The zoom client installed from AUR (and also the one from flatpak) had two weird issues:</p>\n<ol>\n<li>the window didn’t show up</li>\n<li>the window is oversize and cannot be rescaled.</li>\n</ol>\n<p>checked the terminal output (there’s no output) and the log file under <code>$HOME/.zoom/log</code> (nothing suspicious).</p>\n<h1 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h1><p>After few google searches, I found a workaround</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim <span class=\"variable\">$HOME</span>/.config/zoomus.conf</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight toml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">autoScale</span>=<span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n\n<p>I reinstalled the zoom (restart also works) and it worked</p>\n","site":{"data":{}},"excerpt":"","more":"<p>The zoom client installed from AUR (and also the one from flatpak) had two weird issues:</p>\n<ol>\n<li>the window didn’t show up</li>\n<li>the window is oversize and cannot be rescaled.</li>\n</ol>\n<p>checked the terminal output (there’s no output) and the log file under <code>$HOME/.zoom/log</code> (nothing suspicious).</p>\n<h1 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h1><p>After few google searches, I found a workaround</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim <span class=\"variable\">$HOME</span>/.config/zoomus.conf</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight toml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">autoScale</span>=<span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n\n<p>I reinstalled the zoom (restart also works) and it worked</p>\n"},{"title":"Lazy Cat","date":"2022-11-19T07:47:39.000Z","excerpt":"Some funny cat pics 🐈🐈🐈","_content":"\n![](cateye.jpg)\n![](greasycat.jpg)\n\n","source":"_posts/Gallery/Cat/Funny.md","raw":"---\ntitle: Lazy Cat\ncategories:\n  - Gallery\n  - Cat\ndate: 2022-11-18 23:47:39\nexcerpt: Some funny cat pics 🐈🐈🐈\ntags:\n---\n\n![](cateye.jpg)\n![](greasycat.jpg)\n\n","slug":"Gallery/Cat/Funny","published":1,"updated":"2022-11-19T10:07:49.888Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clanvcfhm000o7bsb8zmz38mg","content":"<p><img data-src=\"/2022/11/18/Gallery/Cat/Funny/cateye.jpg\"><br><img data-src=\"/2022/11/18/Gallery/Cat/Funny/greasycat.jpg\"></p>\n","site":{"data":{}},"more":"<p><img data-src=\"/2022/11/18/Gallery/Cat/Funny/cateye.jpg\"><br><img data-src=\"/2022/11/18/Gallery/Cat/Funny/greasycat.jpg\"></p>\n"},{"title":"Set up Sunshine steaming in Windows Hyper-V with GPU partitioning","date":"2025-04-10T19:48:18.000Z","_content":"\n# Set up Sunshine steaming in Windows Hyper-V with GPU Parititoning enabled\n\nWhy gaming on a virtual machine?\n\nGaming on a virtual machine (VM) can be an option for several reasons:\n- **Isolation**: Running games in a VM can provide a layer of isolation from the host system, which can be useful for security and stability.\n- **Flexibility**: VMs can be easily created, modified, and deleted, allowing for quick experimentation with different configurations or setups.\n\n<!-- more -->\n## Pre-requisites\n- A Windows machine with Hyper-V enabled\n\nI will briefly go over the steps to set up a Windows 10 VM.\n1. Make sure you have Hyper-V(Not available on Windows 10/11 Home) enabled through Control Panel \n2. Getting an ISO image of Windows 10/11, either through the Microsoft website or through other means.\n3. Open Hyper-V Manager and create a new VM, assigning reasonable resources to it.\n4. Attach the Windows 10/11 ISO image to the VM and start it (Tips: you might need to disable Secure Boot in the VM settings and try tapping keys when you first boot the image).\n5. Proceed with the installation of Windows 10/11.\n\n## GPU Partitioning adopted from [This guide](https://www.youtube.com/watch?v=KDc8lbE2I6I)\nThe GPU can be shared with the host and the VM, alternatively, you can try to enable GPU passthrough, but it is not covered in this guide.\n\nMy Machine has 2 GPUs, one is the integrated AMD GPU and the other is a dedicated NVIDIA GPU. I will need to disable the integrated GPU in the Device Manager and assign the NVIDIA GPU to the VM.\n\n1. Open Device Manager and disable the integrated GPU.\n2. Mount the VM disk file by simply opening the file in Windows Explorer and assume it is the new `F:` drive.\n3. Create a folder called `HostDriverStore` in `F:\\Windows\\System32\\` (This is the mounted drive).\n3. Copy the whole folder `C:/Windows/System32/DriverStore/FileRepository` to `F:\\Windows\\System32\\HostDriverStore`.\n4. (Nvidia only) Copy everyfile starts with `nv` in `C:\\Windows\\System32\\` to `F:\\Windows\\System32\\`.\n5. (AMD) Amd probably has a similar folder, but I don't have an AMD GPU to test it.\n\nNext we will need to paritition and assign the GPU to the VM. \n1. Open PowerShell ISE as administrator.\n2. Change the name of the `vm` variable to the name of your VM in the script below.\n3. Change the RAM size to your liking.\n3. Run the script below \n\n```powershell\n$vm = \"Game\"\nif (Get-VMGpuPartitionAdapter -VMName $vm -ErrorAction SilentlyContinue) {\n   Remove-VMGpuPartitionAdapter -VMName $vm\n}\n\nSet-VM -GuestControlledCacheTypes $true -VMName $vm\nSet-VM -LowMemoryMappedIoSpace 1Gb -VMName $vm\nSet-VM -HighMemoryMappedIoSpace 32Gb -VMName $vm\nAdd-VMGpuPartitionAdapter -VMName $vm\n```\n\nLast step before we start the machine is to disable the `Checkpoint` feature in Hyper-V.\n1. Open Hyper-V Manager and select the VM.\n2. Click on `Settings` and disable `Checkpoints`.\n3. Now start the VM and you should see the graphic card in in Device Manager.\n\n\n# Install Virtual Audio Device\nWe need a virtual audio device for sunshine to steam audio. You can use the Virtual Audio Device from the `VB-Cable` project or the `Virtual Audio Cable` project.\n1. Download either `VB-Cable` or `Virtual Audio Cable` and Install it.\n2. You should see the virtual audio device in the device manager (it will not appear in `Sound` settings)\n\n# LAN access \nIf you want to access the VM from another device in local network, you can do port forwarding (not covered here) or simply add an external network adapter to the VM.\n1. Open Hyper-V Manager and select the Network Switch on the right side\n2. Click on `Virtual Switch Manager` and create a new `External` network switch.\n3. Toggle the share network adapter option\n4. Open the VM settings and add a new network adapter and select the external network switch.\n\n# Install Sunshine\nLast step is to install sunshine on the VM and you should be able to connect to it.\n\n\n\n\n\n","source":"_posts/Fun/setup_sunshine_hyperv.md","raw":"---\ntitle: Set up Sunshine steaming in Windows Hyper-V with GPU partitioning\ntags: 'hyperv, sunshine, gaming'\ncategories:\n  - Fun\ndate: 2025-04-10 15:48:18\n---\n\n# Set up Sunshine steaming in Windows Hyper-V with GPU Parititoning enabled\n\nWhy gaming on a virtual machine?\n\nGaming on a virtual machine (VM) can be an option for several reasons:\n- **Isolation**: Running games in a VM can provide a layer of isolation from the host system, which can be useful for security and stability.\n- **Flexibility**: VMs can be easily created, modified, and deleted, allowing for quick experimentation with different configurations or setups.\n\n<!-- more -->\n## Pre-requisites\n- A Windows machine with Hyper-V enabled\n\nI will briefly go over the steps to set up a Windows 10 VM.\n1. Make sure you have Hyper-V(Not available on Windows 10/11 Home) enabled through Control Panel \n2. Getting an ISO image of Windows 10/11, either through the Microsoft website or through other means.\n3. Open Hyper-V Manager and create a new VM, assigning reasonable resources to it.\n4. Attach the Windows 10/11 ISO image to the VM and start it (Tips: you might need to disable Secure Boot in the VM settings and try tapping keys when you first boot the image).\n5. Proceed with the installation of Windows 10/11.\n\n## GPU Partitioning adopted from [This guide](https://www.youtube.com/watch?v=KDc8lbE2I6I)\nThe GPU can be shared with the host and the VM, alternatively, you can try to enable GPU passthrough, but it is not covered in this guide.\n\nMy Machine has 2 GPUs, one is the integrated AMD GPU and the other is a dedicated NVIDIA GPU. I will need to disable the integrated GPU in the Device Manager and assign the NVIDIA GPU to the VM.\n\n1. Open Device Manager and disable the integrated GPU.\n2. Mount the VM disk file by simply opening the file in Windows Explorer and assume it is the new `F:` drive.\n3. Create a folder called `HostDriverStore` in `F:\\Windows\\System32\\` (This is the mounted drive).\n3. Copy the whole folder `C:/Windows/System32/DriverStore/FileRepository` to `F:\\Windows\\System32\\HostDriverStore`.\n4. (Nvidia only) Copy everyfile starts with `nv` in `C:\\Windows\\System32\\` to `F:\\Windows\\System32\\`.\n5. (AMD) Amd probably has a similar folder, but I don't have an AMD GPU to test it.\n\nNext we will need to paritition and assign the GPU to the VM. \n1. Open PowerShell ISE as administrator.\n2. Change the name of the `vm` variable to the name of your VM in the script below.\n3. Change the RAM size to your liking.\n3. Run the script below \n\n```powershell\n$vm = \"Game\"\nif (Get-VMGpuPartitionAdapter -VMName $vm -ErrorAction SilentlyContinue) {\n   Remove-VMGpuPartitionAdapter -VMName $vm\n}\n\nSet-VM -GuestControlledCacheTypes $true -VMName $vm\nSet-VM -LowMemoryMappedIoSpace 1Gb -VMName $vm\nSet-VM -HighMemoryMappedIoSpace 32Gb -VMName $vm\nAdd-VMGpuPartitionAdapter -VMName $vm\n```\n\nLast step before we start the machine is to disable the `Checkpoint` feature in Hyper-V.\n1. Open Hyper-V Manager and select the VM.\n2. Click on `Settings` and disable `Checkpoints`.\n3. Now start the VM and you should see the graphic card in in Device Manager.\n\n\n# Install Virtual Audio Device\nWe need a virtual audio device for sunshine to steam audio. You can use the Virtual Audio Device from the `VB-Cable` project or the `Virtual Audio Cable` project.\n1. Download either `VB-Cable` or `Virtual Audio Cable` and Install it.\n2. You should see the virtual audio device in the device manager (it will not appear in `Sound` settings)\n\n# LAN access \nIf you want to access the VM from another device in local network, you can do port forwarding (not covered here) or simply add an external network adapter to the VM.\n1. Open Hyper-V Manager and select the Network Switch on the right side\n2. Click on `Virtual Switch Manager` and create a new `External` network switch.\n3. Toggle the share network adapter option\n4. Open the VM settings and add a new network adapter and select the external network switch.\n\n# Install Sunshine\nLast step is to install sunshine on the VM and you should be able to connect to it.\n\n\n\n\n\n","slug":"Fun/setup_sunshine_hyperv","published":1,"updated":"2025-04-10T20:40:35.752Z","_id":"cm9bs0vds00002cpb2sr5405n","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Set-up-Sunshine-steaming-in-Windows-Hyper-V-with-GPU-Parititoning-enabled\"><a href=\"#Set-up-Sunshine-steaming-in-Windows-Hyper-V-with-GPU-Parititoning-enabled\" class=\"headerlink\" title=\"Set up Sunshine steaming in Windows Hyper-V with GPU Parititoning enabled\"></a>Set up Sunshine steaming in Windows Hyper-V with GPU Parititoning enabled</h1><p>Why gaming on a virtual machine?</p>\n<p>Gaming on a virtual machine (VM) can be an option for several reasons:</p>\n<ul>\n<li><strong>Isolation</strong>: Running games in a VM can provide a layer of isolation from the host system, which can be useful for security and stability.</li>\n<li><strong>Flexibility</strong>: VMs can be easily created, modified, and deleted, allowing for quick experimentation with different configurations or setups.</li>\n</ul>\n<span id=\"more\"></span>\n<h2 id=\"Pre-requisites\"><a href=\"#Pre-requisites\" class=\"headerlink\" title=\"Pre-requisites\"></a>Pre-requisites</h2><ul>\n<li>A Windows machine with Hyper-V enabled</li>\n</ul>\n<p>I will briefly go over the steps to set up a Windows 10 VM.</p>\n<ol>\n<li>Make sure you have Hyper-V(Not available on Windows 10&#x2F;11 Home) enabled through Control Panel </li>\n<li>Getting an ISO image of Windows 10&#x2F;11, either through the Microsoft website or through other means.</li>\n<li>Open Hyper-V Manager and create a new VM, assigning reasonable resources to it.</li>\n<li>Attach the Windows 10&#x2F;11 ISO image to the VM and start it (Tips: you might need to disable Secure Boot in the VM settings and try tapping keys when you first boot the image).</li>\n<li>Proceed with the installation of Windows 10&#x2F;11.</li>\n</ol>\n<h2 id=\"GPU-Partitioning-adopted-from-This-guide\"><a href=\"#GPU-Partitioning-adopted-from-This-guide\" class=\"headerlink\" title=\"GPU Partitioning adopted from This guide\"></a>GPU Partitioning adopted from <a href=\"https://www.youtube.com/watch?v=KDc8lbE2I6I\">This guide</a></h2><p>The GPU can be shared with the host and the VM, alternatively, you can try to enable GPU passthrough, but it is not covered in this guide.</p>\n<p>My Machine has 2 GPUs, one is the integrated AMD GPU and the other is a dedicated NVIDIA GPU. I will need to disable the integrated GPU in the Device Manager and assign the NVIDIA GPU to the VM.</p>\n<ol>\n<li>Open Device Manager and disable the integrated GPU.</li>\n<li>Mount the VM disk file by simply opening the file in Windows Explorer and assume it is the new <code>F:</code> drive.</li>\n<li>Create a folder called <code>HostDriverStore</code> in <code>F:\\Windows\\System32\\</code> (This is the mounted drive).</li>\n<li>Copy the whole folder <code>C:/Windows/System32/DriverStore/FileRepository</code> to <code>F:\\Windows\\System32\\HostDriverStore</code>.</li>\n<li>(Nvidia only) Copy everyfile starts with <code>nv</code> in <code>C:\\Windows\\System32\\</code> to <code>F:\\Windows\\System32\\</code>.</li>\n<li>(AMD) Amd probably has a similar folder, but I don’t have an AMD GPU to test it.</li>\n</ol>\n<p>Next we will need to paritition and assign the GPU to the VM. </p>\n<ol>\n<li>Open PowerShell ISE as administrator.</li>\n<li>Change the name of the <code>vm</code> variable to the name of your VM in the script below.</li>\n<li>Change the RAM size to your liking.</li>\n<li>Run the script below</li>\n</ol>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable\">$vm</span> = <span class=\"string\">&quot;Game&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (<span class=\"built_in\">Get-VMGpuPartitionAdapter</span> <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span> <span class=\"literal\">-ErrorAction</span> SilentlyContinue) &#123;</span><br><span class=\"line\">   <span class=\"built_in\">Remove-VMGpuPartitionAdapter</span> <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">Set-VM</span> <span class=\"literal\">-GuestControlledCacheTypes</span> <span class=\"variable\">$true</span> <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br><span class=\"line\"><span class=\"built_in\">Set-VM</span> <span class=\"literal\">-LowMemoryMappedIoSpace</span> <span class=\"number\">1</span>Gb <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br><span class=\"line\"><span class=\"built_in\">Set-VM</span> <span class=\"literal\">-HighMemoryMappedIoSpace</span> <span class=\"number\">32</span>Gb <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br><span class=\"line\"><span class=\"built_in\">Add-VMGpuPartitionAdapter</span> <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br></pre></td></tr></table></figure>\n\n<p>Last step before we start the machine is to disable the <code>Checkpoint</code> feature in Hyper-V.</p>\n<ol>\n<li>Open Hyper-V Manager and select the VM.</li>\n<li>Click on <code>Settings</code> and disable <code>Checkpoints</code>.</li>\n<li>Now start the VM and you should see the graphic card in in Device Manager.</li>\n</ol>\n<h1 id=\"Install-Virtual-Audio-Device\"><a href=\"#Install-Virtual-Audio-Device\" class=\"headerlink\" title=\"Install Virtual Audio Device\"></a>Install Virtual Audio Device</h1><p>We need a virtual audio device for sunshine to steam audio. You can use the Virtual Audio Device from the <code>VB-Cable</code> project or the <code>Virtual Audio Cable</code> project.</p>\n<ol>\n<li>Download either <code>VB-Cable</code> or <code>Virtual Audio Cable</code> and Install it.</li>\n<li>You should see the virtual audio device in the device manager (it will not appear in <code>Sound</code> settings)</li>\n</ol>\n<h1 id=\"LAN-access\"><a href=\"#LAN-access\" class=\"headerlink\" title=\"LAN access\"></a>LAN access</h1><p>If you want to access the VM from another device in local network, you can do port forwarding (not covered here) or simply add an external network adapter to the VM.</p>\n<ol>\n<li>Open Hyper-V Manager and select the Network Switch on the right side</li>\n<li>Click on <code>Virtual Switch Manager</code> and create a new <code>External</code> network switch.</li>\n<li>Toggle the share network adapter option</li>\n<li>Open the VM settings and add a new network adapter and select the external network switch.</li>\n</ol>\n<h1 id=\"Install-Sunshine\"><a href=\"#Install-Sunshine\" class=\"headerlink\" title=\"Install Sunshine\"></a>Install Sunshine</h1><p>Last step is to install sunshine on the VM and you should be able to connect to it.</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Set-up-Sunshine-steaming-in-Windows-Hyper-V-with-GPU-Parititoning-enabled\"><a href=\"#Set-up-Sunshine-steaming-in-Windows-Hyper-V-with-GPU-Parititoning-enabled\" class=\"headerlink\" title=\"Set up Sunshine steaming in Windows Hyper-V with GPU Parititoning enabled\"></a>Set up Sunshine steaming in Windows Hyper-V with GPU Parititoning enabled</h1><p>Why gaming on a virtual machine?</p>\n<p>Gaming on a virtual machine (VM) can be an option for several reasons:</p>\n<ul>\n<li><strong>Isolation</strong>: Running games in a VM can provide a layer of isolation from the host system, which can be useful for security and stability.</li>\n<li><strong>Flexibility</strong>: VMs can be easily created, modified, and deleted, allowing for quick experimentation with different configurations or setups.</li>\n</ul>","more":"<h2 id=\"Pre-requisites\"><a href=\"#Pre-requisites\" class=\"headerlink\" title=\"Pre-requisites\"></a>Pre-requisites</h2><ul>\n<li>A Windows machine with Hyper-V enabled</li>\n</ul>\n<p>I will briefly go over the steps to set up a Windows 10 VM.</p>\n<ol>\n<li>Make sure you have Hyper-V(Not available on Windows 10&#x2F;11 Home) enabled through Control Panel </li>\n<li>Getting an ISO image of Windows 10&#x2F;11, either through the Microsoft website or through other means.</li>\n<li>Open Hyper-V Manager and create a new VM, assigning reasonable resources to it.</li>\n<li>Attach the Windows 10&#x2F;11 ISO image to the VM and start it (Tips: you might need to disable Secure Boot in the VM settings and try tapping keys when you first boot the image).</li>\n<li>Proceed with the installation of Windows 10&#x2F;11.</li>\n</ol>\n<h2 id=\"GPU-Partitioning-adopted-from-This-guide\"><a href=\"#GPU-Partitioning-adopted-from-This-guide\" class=\"headerlink\" title=\"GPU Partitioning adopted from This guide\"></a>GPU Partitioning adopted from <a href=\"https://www.youtube.com/watch?v=KDc8lbE2I6I\">This guide</a></h2><p>The GPU can be shared with the host and the VM, alternatively, you can try to enable GPU passthrough, but it is not covered in this guide.</p>\n<p>My Machine has 2 GPUs, one is the integrated AMD GPU and the other is a dedicated NVIDIA GPU. I will need to disable the integrated GPU in the Device Manager and assign the NVIDIA GPU to the VM.</p>\n<ol>\n<li>Open Device Manager and disable the integrated GPU.</li>\n<li>Mount the VM disk file by simply opening the file in Windows Explorer and assume it is the new <code>F:</code> drive.</li>\n<li>Create a folder called <code>HostDriverStore</code> in <code>F:\\Windows\\System32\\</code> (This is the mounted drive).</li>\n<li>Copy the whole folder <code>C:/Windows/System32/DriverStore/FileRepository</code> to <code>F:\\Windows\\System32\\HostDriverStore</code>.</li>\n<li>(Nvidia only) Copy everyfile starts with <code>nv</code> in <code>C:\\Windows\\System32\\</code> to <code>F:\\Windows\\System32\\</code>.</li>\n<li>(AMD) Amd probably has a similar folder, but I don’t have an AMD GPU to test it.</li>\n</ol>\n<p>Next we will need to paritition and assign the GPU to the VM. </p>\n<ol>\n<li>Open PowerShell ISE as administrator.</li>\n<li>Change the name of the <code>vm</code> variable to the name of your VM in the script below.</li>\n<li>Change the RAM size to your liking.</li>\n<li>Run the script below</li>\n</ol>\n<figure class=\"highlight powershell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"variable\">$vm</span> = <span class=\"string\">&quot;Game&quot;</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (<span class=\"built_in\">Get-VMGpuPartitionAdapter</span> <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span> <span class=\"literal\">-ErrorAction</span> SilentlyContinue) &#123;</span><br><span class=\"line\">   <span class=\"built_in\">Remove-VMGpuPartitionAdapter</span> <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">Set-VM</span> <span class=\"literal\">-GuestControlledCacheTypes</span> <span class=\"variable\">$true</span> <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br><span class=\"line\"><span class=\"built_in\">Set-VM</span> <span class=\"literal\">-LowMemoryMappedIoSpace</span> <span class=\"number\">1</span>Gb <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br><span class=\"line\"><span class=\"built_in\">Set-VM</span> <span class=\"literal\">-HighMemoryMappedIoSpace</span> <span class=\"number\">32</span>Gb <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br><span class=\"line\"><span class=\"built_in\">Add-VMGpuPartitionAdapter</span> <span class=\"literal\">-VMName</span> <span class=\"variable\">$vm</span></span><br></pre></td></tr></table></figure>\n\n<p>Last step before we start the machine is to disable the <code>Checkpoint</code> feature in Hyper-V.</p>\n<ol>\n<li>Open Hyper-V Manager and select the VM.</li>\n<li>Click on <code>Settings</code> and disable <code>Checkpoints</code>.</li>\n<li>Now start the VM and you should see the graphic card in in Device Manager.</li>\n</ol>\n<h1 id=\"Install-Virtual-Audio-Device\"><a href=\"#Install-Virtual-Audio-Device\" class=\"headerlink\" title=\"Install Virtual Audio Device\"></a>Install Virtual Audio Device</h1><p>We need a virtual audio device for sunshine to steam audio. You can use the Virtual Audio Device from the <code>VB-Cable</code> project or the <code>Virtual Audio Cable</code> project.</p>\n<ol>\n<li>Download either <code>VB-Cable</code> or <code>Virtual Audio Cable</code> and Install it.</li>\n<li>You should see the virtual audio device in the device manager (it will not appear in <code>Sound</code> settings)</li>\n</ol>\n<h1 id=\"LAN-access\"><a href=\"#LAN-access\" class=\"headerlink\" title=\"LAN access\"></a>LAN access</h1><p>If you want to access the VM from another device in local network, you can do port forwarding (not covered here) or simply add an external network adapter to the VM.</p>\n<ol>\n<li>Open Hyper-V Manager and select the Network Switch on the right side</li>\n<li>Click on <code>Virtual Switch Manager</code> and create a new <code>External</code> network switch.</li>\n<li>Toggle the share network adapter option</li>\n<li>Open the VM settings and add a new network adapter and select the external network switch.</li>\n</ol>\n<h1 id=\"Install-Sunshine\"><a href=\"#Install-Sunshine\" class=\"headerlink\" title=\"Install Sunshine\"></a>Install Sunshine</h1><p>Last step is to install sunshine on the VM and you should be able to connect to it.</p>"}],"PostAsset":[{"_id":"source/_posts/Gallery/Cat/Funny/cateye.jpg","slug":"cateye.jpg","post":"clanvcfhm000o7bsb8zmz38mg","modified":0,"renderable":0},{"_id":"source/_posts/Gallery/Cat/Funny/greasycat.jpg","slug":"greasycat.jpg","post":"clanvcfhm000o7bsb8zmz38mg","modified":0,"renderable":0}],"PostCategory":[{"post_id":"clanvcfhe00017bsbfe457cxc","category_id":"clanvcfhh00047bsb47q9auiv","_id":"clanvcfhk000b7bsbemjo43jm"},{"post_id":"clanvcfhg00037bsb15ui6tgh","category_id":"clanvcfhh00047bsb47q9auiv","_id":"clanvcfhk000d7bsb3h5xci5t"},{"post_id":"clanvcfhi00077bsb4vg47tup","category_id":"clanvcfhh00047bsb47q9auiv","_id":"clanvcfhl000g7bsb4g75dbg2"},{"post_id":"clanvcfhm000o7bsb8zmz38mg","category_id":"clanvcfhm000p7bsbashy10a6","_id":"clanvcfhn000r7bsb55t95tl8"},{"post_id":"clanvcfhm000o7bsb8zmz38mg","category_id":"clanvcfhn000q7bsb6dnha1fg","_id":"clanvcfhn000s7bsbcsba79ac"},{"post_id":"cm9bs0vds00002cpb2sr5405n","category_id":"cm9bs0vxm00032cpbh2oc4019","_id":"cm9bs0vxn00042cpb1sbchfjb"}],"PostTag":[{"post_id":"clanvcfhe00017bsbfe457cxc","tag_id":"clanvcfhi00057bsb70fb5r9v","_id":"clanvcfhl000f7bsbezo30hq5"},{"post_id":"clanvcfhe00017bsbfe457cxc","tag_id":"clanvcfhj00097bsb7j3j8i0l","_id":"clanvcfhl000h7bsb5qzf25ls"},{"post_id":"clanvcfhe00017bsbfe457cxc","tag_id":"clanvcfhk000c7bsbejyd07rw","_id":"clanvcfhl000j7bsb80io18aw"},{"post_id":"clanvcfhi00077bsb4vg47tup","tag_id":"clanvcfhi00057bsb70fb5r9v","_id":"clanvcfhl000l7bsbcvk20ypi"},{"post_id":"clanvcfhi00077bsb4vg47tup","tag_id":"clanvcfhl000i7bsb8atobv1c","_id":"clanvcfhl000m7bsbbuundm2m"},{"post_id":"clanvcfhi00077bsb4vg47tup","tag_id":"clanvcfhl000k7bsb4jul7fx4","_id":"clanvcfhl000n7bsb4uq76dvv"},{"post_id":"cm9bs0vds00002cpb2sr5405n","tag_id":"cm9bs0ve000012cpb5jb49n3g","_id":"cm9bs0ve700022cpbe5sy70rk"}],"Tag":[{"name":"archlinux","_id":"clanvcfhi00057bsb70fb5r9v"},{"name":"installation","_id":"clanvcfhj00097bsb7j3j8i0l"},{"name":"deep-learning","_id":"clanvcfhk000c7bsbejyd07rw"},{"name":"zoom","_id":"clanvcfhl000i7bsb8atobv1c"},{"name":"solved","_id":"clanvcfhl000k7bsb4jul7fx4"},{"name":"c++","_id":"clbo3zivj0002jhsbb2nb2qvm"},{"name":"machine-learning","_id":"clbo3zivj0003jhsb4tuu93os"},{"name":"neural-network","_id":"clbo3zivk0005jhsbe2ymhnxw"},{"name":"hyperv, sunshine, gaming","_id":"cm9bs0ve000012cpb5jb49n3g"}]}}