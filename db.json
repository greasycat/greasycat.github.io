{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","path":"js/third-party/analytics/matomo.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/gitter.js","path":"js/third-party/chat/gitter.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1},{"_id":"source/images/cateye.jpg","path":"images/cateye.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"source/.obsidian/app.json","hash":"bfadd47324d70b4cd0b738e55227027a85f1bd77","modified":1671050501048},{"_id":"source/.obsidian/appearance.json","hash":"9439122150a0431e432676115220bc925af96094","modified":1668853278040},{"_id":"source/.obsidian/core-plugins.json","hash":"bef954208c2c5c5e253ee8738889b6d7d02793eb","modified":1668849171032},{"_id":"source/.obsidian/hotkeys.json","hash":"bf21a9e8fbc5a3846fb05b4fa0859e0917b2202f","modified":1668849170529},{"_id":"source/.obsidian/workspace.json","hash":"8a553df0206ded67234d8435eeff1b327668d2b1","modified":1671235236090},{"_id":"source/404/index.md","hash":"99a9cfc6e59ccfc68caf5a1d5c3f41d268f26d23","modified":1668855665985},{"_id":"source/categories/index.md","hash":"ea34e09c1a592f3892acde873f30539d20cbe233","modified":1668850996215},{"_id":"source/tags/index.md","hash":"3b4712581282205c08005b3bee3a4a6388c46d43","modified":1668850979668},{"_id":"source/_posts/System/archlinux-easy-setup-for-deep-learning.md","hash":"94f95db43e1e65aa696ee1b7e7021776ddc527ac","modified":1668854963495},{"_id":"source/_posts/System/archlinux-emoji-display-problem.md","hash":"0ee5db8d7e36aa9f289f2d3ec6fa8fa347464e75","modified":1668852286659},{"_id":"source/_posts/System/archlinux-zoom-oversized-window.md","hash":"65b608166c82dfa41aae6fd82cf3cc57fb832ce6","modified":1668852315799},{"_id":"source/_posts/Gallery/Cat/Funny.md","hash":"4c7c307effbcad35935d479847533b4ce1089e1d","modified":1668852469888},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1668844772641},{"_id":"source/images/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1668846721611},{"_id":"node_modules/hexo-theme-next/README.md","hash":"27ad28dd2228066631650704665478af15b49396","modified":1671319837004},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"c27e934f63440d62a8258194001ae2ba6118b87c","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"8aef425c1923e50022953323d717e3a0df69654e","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/package.json","hash":"b924187b67841c57de4998ae64fd737dcc5d4ef1","modified":1671319836994},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1668844772638},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"46026e98fe279a9db5d68bf91afa5d8e41f9ccfb","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1668844772638},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"74133a827c104d1d84509177f283947fe9327a80","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"c4fc18c338309271d1afe737e6c404a6fa313bbc","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"4bbdb4284afe495437cf7817b97b45f60e12d1e4","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"8b6d4a8a056df4362d20ae1f6ac7a590dd7246e5","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"c4bad93b23c7eefa730fd7fec5b05ace3ce9ca4d","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"4d6efcdaea89dfe751b7707d91858bf33d3538b7","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"2436942610e38b73b175cb0f8ed1f8f23d42a7ce","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"1e256c8df039ddeba66526c5eb2d6c79177a7fc6","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"8a4da307b4a19e3c96b90a07a2da8dc5d50e5d9e","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"a0906702d3c87d0e1661e300ad0bdf7e679a3d91","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"25ee0d5b9a0464a91af7d2efb33293c09ad5abdf","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"10d80915c41328f31a4f2d8ac736bba020f373d8","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"fa7c43ec872aee2739b25dd7260e0764ea9e26fd","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"034c5a0df49144e8f16ae2300dba9fde58b9329e","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"37d9af426b040004841273d163059cd49cd67d65","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"16b96783ba363255b4c8156c3e1efebdb37676f8","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"fbaed3039ed8605b81422003a4ecb2a6514e339d","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"a6ba04c743a5b494ac56612ce6b858d9ae3ae1bd","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"5cebea74f3198a4a20cacf23069c3b91e4f03d85","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"890f87f6a33ad8452b771607d4c3ff14810b35fa","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"fc0a45112f2dcfc2642404e8934ea32a793c3bd7","modified":1671319837004},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"8ab7219563dab13885840207a86055000d173465","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"9fdce9d316e205fc132b2181254ef3b04007a97d","modified":1671319837040},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"b0660b2af0ac7d3fda14ca4d9f2c9e79ef06c6f9","modified":1671319837017},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"0bfce9f133f501a9a4837257e3b862b3bbca15be","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7a06d443f374bd1e84294067a0ac796afd9fbe60","modified":1668844772641},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"1eb74890534377a5dfa456e9e39bc805726c6dc2","modified":1671319837004},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"1a30d751871dabfa80940042ddb1f77d07d830b9","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"09938ba9239c20a764939de08b93123371c1e5fb","modified":1671319837004},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"547c62ab14d9e05d2d9116db9048a677fbe1fb6d","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1668844772641},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"c40760b559c516677c8b11a00ba50c011f2079fd","modified":1671319837014},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"bc719473ed5948ab6859449d60b8d36cfc1542b4","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"7e8268fd5cbd552322b276f52459fd187c2453d2","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"e7f988ecddb2159313699a00827a45eca5622bd4","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"aa37f8e98208177b63e3328d6e53b022c6edf3b2","modified":1671319837014},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/rating.njk","hash":"1bcdbc7fde26d6d9ef4e7fa43ffcff5a9506b20e","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"bd9ea82376cd87df611ea3ae077875c7c595a3df","modified":1671319836980},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"fdc8a0af90035e89c3fcb754a0eb189b8951a2bc","modified":1671319836990},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"ce8477e7fb226525bae5872cd68a1c2c23ad50c8","modified":1671319836984},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"a11b71ba0c5012e2cdcab31c15439156b215563e","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"afdd6a188a74c188f0dd154fac70efd4080ca262","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"226fccbe9c93265e65a300e3cb4bf6f9065cfdd7","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"17f9451ce1f10f78437f52218757d38d4e1591b0","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1668844772624},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"921a58577f411cf4eb5cfd66db0a241f8f88578c","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"dadc81256afb127b77eac6763d5ee0ec9c77f0a3","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"d8261df9bd877aa7675ba28577caeeb9b3fc44ea","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"099e11ab995a2c8981427a85476d082609848c77","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"c2fe7e13c0dd0255d8db9ab9c0df4130824ba0b3","modified":1671319836984},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"90fb7f346f777434ea68ab4e4be1d7b999ad63ac","modified":1671319836987},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"da0f07f9eaaa83de70128b0feaea3fdadb90457a","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"5388b157bba4a40b9312f4a45c6678974ccf0837","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"dd9c4c03e99dfde0dfb8edefcb2c933f2f560efc","modified":1671319837010},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"a6aaa06207b8818258355e514596bd738d2a49bf","modified":1671319836990},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-footer.njk","hash":"bde2c7356d9362972bde41cc206d5816f8ed714d","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"133942922e34abae9e4de7ea5591d77c0caa4b37","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"c1e33b4889f75acc490af3c8bde0ec56c518ff41","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"e8b8a7c41e9ec612d0c0c73419529d55d1c16256","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"e0986db00a0201dd3c60570f964829c84ba5bc68","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"1981ec6e82df3094812d40dc93c81748aef7c1c8","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"f900306497b133e8b098bd9f4b96b93d1d96c185","modified":1671319837014},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/matomo.njk","hash":"4e89648a8ec8194c5823064cbca39c938a799006","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/gitter.njk","hash":"f8cc14b7aa949999a1faaeb7855e2f20b59a386d","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"1ebf658690468ea197bdd0416eb7cfa4bd0b083a","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1668844772644},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1668844772648},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1668844772651},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1668844772654},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"9ec51eb61f7fee612ffc5252f489003a0fa301fc","modified":1671319836974},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"ec996d0673f766167c86df0966e9da1ae036e103","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"64e4024376b51fe81be7ad80235abdf0a83853bd","modified":1668844772624},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"5798cfc8f63665031dd3e01debed051628cec319","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"62faf6b0b0020066a0dec1f0123cf1fee3198e7e","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1668844772624},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"48f4f277946a168d0db1ea02804e85c22ca2c7db","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"a1418c9dc8c0f1a0ad4ded0f4627c45bf0db1a10","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"0c490e9ba82efbb8bdf8465e6b537fafd51e1ed7","modified":1671319836987},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"7664491542046df9a3887cf40a06e00c0b4086a9","modified":1671319837027},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"2298e521253b3bf376a2412271bc2a7d305051f3","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"9845209c54174a42cbff5b5efd5e2e2fb7e60589","modified":1671319837027},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"64775c729512b30b144ab5ae9dc4a4dfd4e13f35","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1668844772661},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"f4228c759db4a650c8d38745c2edd1dc83c45687","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"782ee1fc5e669d3ddbfeb82b73ad7fe561f1a4fb","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"fd49b521d67eaccc629f77b4e095cb7310327565","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"77b85d4de5ab747e04008ab31200311b29748740","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"fa4fd8f76464e214fb7318f325b13c2b62f4b478","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"7fecfb76420f786d6bf60218a81705bb48fb1d18","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"d856127cd7e0a28a88edf0b2eb860ede9c3fdb60","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"485d23ccb42c0d0c8ead7ea8930dd3e06d79a285","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"b7f48be3c43bfa393d62142544a5487a67871713","modified":1668844772658},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1668844772661},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"cef9c5f9524fd01b59b0a89e51904b42cbdedc8c","modified":1671319837027},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"ac2dc0ce9c775a83ef7132ae957b54539366ac9c","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"9f77d8c07dff6944b577110fb22f8f7c407a107b","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1668844772661},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1668844772661},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"91dbf3ca5c3a613d4e30618c120da535bf2d0336","modified":1671319837027},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"33a82207a15aad9d1c8fb2251f9e3eba50452932","modified":1668844772611},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1668844772624},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1668844772614},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1668844772608},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"f27d817b0c2138dd3215b1f46af0753f60a008f3","modified":1668844772618},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1668844772621},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"098d4bd034e986fcf7e443eac4fc2193935461b7","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"1ecfd64507954810b07a9d21fb5305b5378feda0","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"11497388f124bfbb4001495a67d3629a9f618405","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"d757768a58743601d0d84158ba955eb15d4c3c01","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"ec37a36e94ba791663607a5022f763915778578f","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"40c9839d3288c3b7de0bf38ac2e18f6c8eba6227","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"04cf4a69537fc14d3b8904f965d283356853847f","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"010c901e4ef49a606f8a350efbf09044e76d2ff3","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"8f094c4ac17e2ab45569b12d157747f9c7333c12","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"877a537d5b95beb048142e4fdee6f17e6ef9c7bb","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitter.styl","hash":"35104dc6883a61c31e0e368dac8ac2f697be62fe","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"77550e0d3e029b7458e35d8c5ae1fbd612c9673b","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"0c79462439b1361034a03590cd69a8abb3a678a6","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/related-posts.styl","hash":"41ed817e1eb64078074e245e771446ee041e5790","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1668844772668},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"6e0d0796ef7fbbb62ffdfb448753a850de82c74f","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"b750af2fb833c10c4313b5a4258237161a7833d7","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"a851e9d5aefcd027c95eeb323860b6da70f202d1","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"da5e88f8debd5ac8d7af5c6ba6240df66104955f","modified":1671319837030},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"52fc98b1435129eb3edb9293ced9e507741f1350","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"c6a27beb3f741211a14576026f3b4cfc44cc6407","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"fbdb63c6a8887d19b7137325ba7d6806f728139c","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"b926e368f702f8686aaa2eb98d3d2e533418958c","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"f3506fd0c0303ea365de1c7774d98a1a3f3027cf","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"021a37cf178440cc341940a299d3bca359996c6b","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"9a7c71560fbdc936ad4e736fe15063ea3e8a644b","modified":1671319837037},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1668844772678},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"670fc109b56a010b166b86b616823a1aae97a738","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"9056be572ec1cfa429abb22be4b45a662d5b0fb1","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1668844772664},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"3f76c73a891bbc10679753e702feba9e8a5ffdd2","modified":1668844772671},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"98d4c20aff0f0fcfe1824017fb06ab21ef0d218e","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1668844772674},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"7a39bcce7274284e87388743db62afc847fe6897","modified":1668844772678},{"_id":"source/_posts/Gallery/Cat/Funny/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1668848912792},{"_id":"source/_posts/Gallery/Cat/Funny/greasycat.jpg","hash":"980b1b329451b35edb669f4199ea2af14cfdbb63","modified":1668848912792},{"_id":"public/search.xml","hash":"3a45a61865a85c0395e13d7a264ff55775d35956","modified":1671319493489},{"_id":"public/sitemap.xml","hash":"598824f0138840009cbad17155358a16e87a4ab0","modified":1671319493489},{"_id":"public/sitemap.txt","hash":"88c5af33fe277373abc6b9219768e21fc117129a","modified":1671319493489},{"_id":"public/categories/index.html","hash":"5e60d12f4f533d4ff4adc576373009cec75826ff","modified":1671320270199},{"_id":"public/404/index.html","hash":"7bd76f37251bb0f6a2fdae25b908bf00d6efe2de","modified":1671320270199},{"_id":"public/tags/index.html","hash":"b9209ca8c2372145d54b19d9f4d9efdedca28c9f","modified":1671320270199},{"_id":"public/2022/11/18/Gallery/Cat/Funny/index.html","hash":"a7c1340bb97c554bc976d220fe409a2c646589d8","modified":1671320270199},{"_id":"public/2022/06/03/System/archlinux-emoji-display-problem/index.html","hash":"aa1004dc39e93126fb78834cad3e9d4a7731dc3b","modified":1671320270199},{"_id":"public/2022/06/01/System/archlinux-zoom-oversized-window/index.html","hash":"dffc785cc4c822c60ee1725ffb48c5a52fbe6469","modified":1671320270199},{"_id":"public/categories/System/index.html","hash":"cc9c36bd6ec4915d9fcca7afe77e40dd6050b192","modified":1671320270199},{"_id":"public/categories/Gallery/index.html","hash":"35f6297d961dd5ae409c5fd168c075857aacd408","modified":1671320270199},{"_id":"public/categories/Gallery/Cat/index.html","hash":"4e7cd09024575fb82e8e88765e5d6d29d00eddaf","modified":1671320270199},{"_id":"public/archives/index.html","hash":"facc70a02b03f55cbad4f9ec86e8792ca97a5eeb","modified":1671320270199},{"_id":"public/archives/2022/index.html","hash":"5d04de2921126e8ea985458905b3c0ad445d8fb8","modified":1671320270199},{"_id":"public/archives/2022/06/index.html","hash":"e93c4fb58f3ad17efacf91c1c4969f91f878b26d","modified":1671320270199},{"_id":"public/archives/2022/11/index.html","hash":"3272f228f66cda2a5d88dc2a4209b7d8b0bda54a","modified":1671320270199},{"_id":"public/tags/archlinux/index.html","hash":"a269477e4f262ac50b6972744ef6851ae8529a5e","modified":1671320270199},{"_id":"public/tags/installation/index.html","hash":"098166f75ba56198083c1c0c5b86a13f7f480747","modified":1671320270199},{"_id":"public/tags/deep-learning/index.html","hash":"7774199b20b03d654f58d7e411c7cbfe86ef24ab","modified":1671320270199},{"_id":"public/tags/zoom/index.html","hash":"0191c8e105ee01ff02223ebeb0a9948d1f90691d","modified":1671320270199},{"_id":"public/tags/solved/index.html","hash":"749eaa17fbba4f5dfb946d661a0e5ce3aacc7021","modified":1671320270199},{"_id":"public/2022/11/19/System/archlinux-easy-setup-for-deep-learning/index.html","hash":"7aafb5972026a49c812fd5af4811fe4586d277fd","modified":1671320270199},{"_id":"public/index.html","hash":"965edf8c1232f853c9ac47251719614cdabad0bc","modified":1671320270199},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1668858639243},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1668858639243},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1668858639243},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1668858639243},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1668858639243},{"_id":"public/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1668858639243},{"_id":"public/css/noscript.css","hash":"ec89b3425fbce20863d554c6fd495ea29c3c303d","modified":1668858639243},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1668858639243},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1668858639243},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1668858639243},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1668858639243},{"_id":"public/js/motion.js","hash":"f7c825cbff11885fa0dffa64824fd00e505d6a8d","modified":1668858639243},{"_id":"public/js/next-boot.js","hash":"da0f07f9eaaa83de70128b0feaea3fdadb90457a","modified":1668858639243},{"_id":"public/js/pjax.js","hash":"919f5281c4a04d11cfd94573ecf57b3dbabd3cc8","modified":1668858639243},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1668858639243},{"_id":"public/js/utils.js","hash":"200088bfd042f5304b2a04befab0829148845e0e","modified":1668858639243},{"_id":"public/js/schemes/muse.js","hash":"9794bd4fc6a458322949d6a0ade89cd1026bc69f","modified":1668858639243},{"_id":"public/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1668858639243},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1668858639243},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1668858639243},{"_id":"public/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1668858639243},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1668858639243},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1668858639243},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1668858639243},{"_id":"public/js/third-party/analytics/matomo.js","hash":"c6a25b26a1443caa70b47fd3dfa282271574deb5","modified":1668858639243},{"_id":"public/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1668858639243},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1668858639243},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1668858639243},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1668858639243},{"_id":"public/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1668858639243},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"33a82207a15aad9d1c8fb2251f9e3eba50452932","modified":1668858639243},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1668858639243},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1668858639243},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1668858639243},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1668858639243},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1668858639243},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1668858639243},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1668858639243},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1668858639243},{"_id":"public/js/third-party/statistics/firestore.js","hash":"6e0682bb42170d61b13b786295f45f9c785f8b73","modified":1668858639243},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"835cbf54c49ef1327f47df70ff2636ad36b6f57d","modified":1668858639243},{"_id":"public/js/third-party/tags/mermaid.js","hash":"f27d817b0c2138dd3215b1f46af0753f60a008f3","modified":1668858639243},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1668858639243},{"_id":"public/css/main.css","hash":"41e3ba772ff37ade0c2068df348d8b5cea6168d8","modified":1668858639243},{"_id":"public/images/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1668858639243},{"_id":"public/2022/11/18/Gallery/Cat/Funny/cateye.jpg","hash":"c55fc80ed87ece06dd370641dd77ad10981f4d8a","modified":1668858639243},{"_id":"public/2022/11/18/Gallery/Cat/Funny/greasycat.jpg","hash":"980b1b329451b35edb669f4199ea2af14cfdbb63","modified":1668858639243},{"_id":"source/_posts/.Rhistory","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1671049517931},{"_id":"source/_posts/Naive-Neural-Network-in-Cpp.md","hash":"3ca27581eefe2c196cdb9797addf74d3870f3e59","modified":1671050038702},{"_id":"public/2021/01/05/Naive-Neural-Network-in-Cpp/index.html","hash":"ecddfcd4ad830dc88c658efc2b4288143d8f522a","modified":1671050041739},{"_id":"public/archives/2021/index.html","hash":"ff5218ade4392c31f2d150c6dce929f9687eedaa","modified":1671320270199},{"_id":"public/archives/2021/01/index.html","hash":"aeb14fd200258a05e4a77f3e92a3f4eb52a96f88","modified":1671320270199},{"_id":"public/categories/Code/index.html","hash":"2c259d642e62a584ae1cb9189f7eeffbee618836","modified":1671320270199},{"_id":"public/tags/c/index.html","hash":"8cf55daa23ab089f488041874c3e19af2f1ae3af","modified":1671320270199},{"_id":"public/tags/machine-learning/index.html","hash":"f7267f3e4478c86a09a473f709cdb1c1d9e21b40","modified":1671320270199},{"_id":"public/tags/neural-network/index.html","hash":"24be4def8056c581da5d33518e0baa417f4512d7","modified":1671320270199},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp.md","hash":"59cfaa1ffd474f79cdf56df39b8fa791ccf51632","modified":1671050041996},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp/index.html","hash":"4bcdc5b5568e70e932e6ab09b52635e739514879","modified":1671050061831},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp-1.md","hash":"c175a569424a260fbfc7efd56fac5852c89335f8","modified":1671233235785},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp-1/index.html","hash":"7fd40c0f49c6f9bf5ed9f0cc094056f858b98d17","modified":1671320270199},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp-2.md","hash":"af03eb55ca4710e071db98c45271d629bcd1325d","modified":1671235060452},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp-2/index.html","hash":"7a8076493a35f688aa25370ce0e68f0befa96e2a","modified":1671320270199},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp-3.md","hash":"3de59dd8045952564530489d84966648ebaa6bb6","modified":1671235128323},{"_id":"source/_posts/Code/Naive-Neural-Network-in-Cpp-4 Network.md","hash":"0e169ee279e0e11aa3cba7503f82c30d7abad1d7","modified":1671235097462},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp-3/index.html","hash":"c3b32ba403e23d2823c8751246f9548e0b08d275","modified":1671320270199},{"_id":"public/2021/01/05/Code/Naive-Neural-Network-in-Cpp-4 Network/index.html","hash":"cdcb4b7555c56ec8e240bbd644e58bf6e8adb385","modified":1671320270199},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-paginator.js","hash":"e86c764b546e4fbb87970cabc4135a56f9ef9fe1","modified":1671319836987},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-share.njk","hash":"3f28cc4411c0ffc0e41b7970d5ab329c7e46f497","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/plausible.njk","hash":"bb664a2327b5cf4745895f63f25f9296e9dea6e2","modified":1671319837020},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/related-posts.styl","hash":"b05908f04ef95f2d91e6eba89b12411c378d050f","modified":1671319837034},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/umami.njk","hash":"4ca17a5241c6079d03f6297ba1ae3821c00136ea","modified":1671319837024},{"_id":"node_modules/hexo-theme-next/languages/th.yml","hash":"4fac74a39d3906c4a727476be4750530a505933a","modified":1671319837040}],"Category":[{"name":"System","_id":"clanvcfhh00047bsb47q9auiv"},{"name":"Gallery","_id":"clanvcfhm000p7bsbashy10a6"},{"name":"Cat","parent":"clanvcfhm000p7bsbashy10a6","_id":"clanvcfhn000q7bsb6dnha1fg"},{"name":"Code","_id":"clbo3zivi0001jhsba6ho9lbw"}],"Data":[],"Page":[{"title":"404","date":"2022-11-19T10:59:16.000Z","_content":"\n# You've reached the end of the cat's tail","source":"404/index.md","raw":"---\ntitle: 404\ndate: 2022-11-19 02:59:16\n---\n\n# You've reached the end of the cat's tail","updated":"2022-11-19T11:01:05.985Z","path":"404/index.html","comments":1,"layout":"page","_id":"clanvcfhd00007bsbbltue6e9","content":"<h1 id=\"You’ve-reached-the-end-of-the-cat’s-tail\"><a href=\"#You’ve-reached-the-end-of-the-cat’s-tail\" class=\"headerlink\" title=\"You’ve reached the end of the cat’s tail\"></a>You’ve reached the end of the cat’s tail</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"You’ve-reached-the-end-of-the-cat’s-tail\"><a href=\"#You’ve-reached-the-end-of-the-cat’s-tail\" class=\"headerlink\" title=\"You’ve reached the end of the cat’s tail\"></a>You’ve reached the end of the cat’s tail</h1>"},{"title":"Categories","date":"2022-11-19T09:41:09.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2022-11-19 01:41:09\ntype: \"categories\"\ncomments: false\n---\n","updated":"2022-11-19T09:43:16.215Z","path":"categories/index.html","layout":"page","_id":"clanvcfhg00027bsb0lwpcgwm","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","date":"2022-11-19T09:40:10.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2022-11-19 01:40:10\ntype: \"tags\"\ncomments: false\n---\n","updated":"2022-11-19T09:42:59.668Z","path":"tags/index.html","layout":"page","_id":"clanvcfhi00067bsb61he72xw","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"archlinux-easy-setup-for-deep-learning","date":"2022-11-19T10:13:52.000Z","_content":"\n# My setup\n- CPU: AMD Ryzen 9 5900X\n- GPU: NVIDIA GeForce RTX 3090 FE\n- Archlinux 6.0.8-arch1-1\n- WM i3\n\n# Update pacman source\n```bash\nsudo pacman -Syu\n```\n\n# Nvidia driver\n\n**Skip** if you've installed properitary nvidia driver\n- for those need to switch nouveu to nvidia, please check archlinux wiki for more information\n\n```bash\nsudo pacman -S nvidia nvidia-utils\n# or nvidia-dkms and linux-header for custom kernel\n```\n\n# CUDA & CUDNN\n```bash\nsudo pacman -S cuda cudnn\n```\n\nIf you havn't set the path for dynamic/shared library, it's time to set it. Otherwise errors like `# Unimplemented: DNN library is not found` might present when running CNN\n\n\nAdd the following line to either `~/.bashrc` or `~/.bash_profile` or `.profile` (last two are preferred)\n```bash\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/lib\"\n```\n\nVerify the installation\n```bash\ncd /opt/cuda/extras/demo_suite\n./deviceQuery\n```\n<!-- more -->\n```\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"NVIDIA GeForce RTX 3090\"\n  CUDA Driver Version / Runtime Version          11.8 / 11.8\n  CUDA Capability Major/Minor version number:    8.6\n  Total amount of global memory:                 24265 MBytes (25443893248 bytes)\n  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores\n  GPU Max Clock rate:                            1695 MHz (1.70 GHz)\n  Memory Clock rate:                             9751 Mhz\n  Memory Bus Width:                              384-bit\n  L2 Cache Size:                                 6291456 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  1536\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n  Run time limit on kernels:                     Yes\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090\nResult = PASS\n```\n\n# Tensorflow & Keras\n\n`python-tensorflow-opt-cuda` vs `python-tensorflow-cuda`\n- opt might be slightly faster as the package is optimized for certain intel cpu\n\n```bash\nsudo pacman -S python-tensorflow-cuda keras\n```\n\n# Try it out!\n```python\nfrom keras import layers  \nfrom keras import models  \nmodel = models.Sequential()  \nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))  \nmodel.add(layers.MaxPooling2D((2, 2)))  \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))  \nmodel.add(layers.MaxPooling2D((2, 2)))  \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))  \nmodel.add(layers.Flatten())  \nmodel.add(layers.Dense(64, activation='relu'))  \nmodel.add(layers.Dense(10, activation='softmax'))  \nprint(model.summary())  \n  \nfrom keras.datasets import mnist  \nfrom keras.utils import to_categorical  \n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  \n  \ntrain_images = train_images.reshape((60000, 28, 28, 1))  \ntrain_images = train_images.astype('float32') / 255  \ntest_images = test_images.reshape((10000, 28, 28, 1))  \ntest_images = test_images.astype('float32') / 255  \ntrain_labels = to_categorical(train_labels)  \ntest_labels = to_categorical(test_labels)  \nmodel.compile(optimizer='rmsprop',  \n\tloss='categorical_crossentropy',  \n\tmetrics=['accuracy'])  \nmodel.fit(train_images, train_labels, epochs=5, batch_size=64)  \n  \ntest_loss, test_acc = model.evaluate(test_images, test_labels)  \nprint('test_acc:', test_acc)\n```\n\n```\n2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n                                                                 \n flatten (Flatten)           (None, 576)               0         \n                                                                 \n dense (Dense)               (None, 64)                36928     \n                                                                 \n dense_1 (Dense)             (None, 10)                650       \n                                                                 \n=================================================================\nTotal params: 93,322\nTrainable params: 93,322\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/5\n2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469\nEpoch 2/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853\nEpoch 3/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902\nEpoch 4/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919\nEpoch 5/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940\n313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880\ntest_acc: 0.9879999756813049\n```","source":"_posts/System/archlinux-easy-setup-for-deep-learning.md","raw":"---\ntitle: archlinux-easy-setup-for-deep-learning\ncategories:\n  - System\ndate: 2022-11-19 02:13:52\ntags: \n- archlinux\n- installation\n- deep-learning\n---\n\n# My setup\n- CPU: AMD Ryzen 9 5900X\n- GPU: NVIDIA GeForce RTX 3090 FE\n- Archlinux 6.0.8-arch1-1\n- WM i3\n\n# Update pacman source\n```bash\nsudo pacman -Syu\n```\n\n# Nvidia driver\n\n**Skip** if you've installed properitary nvidia driver\n- for those need to switch nouveu to nvidia, please check archlinux wiki for more information\n\n```bash\nsudo pacman -S nvidia nvidia-utils\n# or nvidia-dkms and linux-header for custom kernel\n```\n\n# CUDA & CUDNN\n```bash\nsudo pacman -S cuda cudnn\n```\n\nIf you havn't set the path for dynamic/shared library, it's time to set it. Otherwise errors like `# Unimplemented: DNN library is not found` might present when running CNN\n\n\nAdd the following line to either `~/.bashrc` or `~/.bash_profile` or `.profile` (last two are preferred)\n```bash\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/usr/lib\"\n```\n\nVerify the installation\n```bash\ncd /opt/cuda/extras/demo_suite\n./deviceQuery\n```\n<!-- more -->\n```\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"NVIDIA GeForce RTX 3090\"\n  CUDA Driver Version / Runtime Version          11.8 / 11.8\n  CUDA Capability Major/Minor version number:    8.6\n  Total amount of global memory:                 24265 MBytes (25443893248 bytes)\n  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores\n  GPU Max Clock rate:                            1695 MHz (1.70 GHz)\n  Memory Clock rate:                             9751 Mhz\n  Memory Bus Width:                              384-bit\n  L2 Cache Size:                                 6291456 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  1536\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n  Run time limit on kernels:                     Yes\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device supports Compute Preemption:            Yes\n  Supports Cooperative Kernel Launch:            Yes\n  Supports MultiDevice Co-op Kernel Launch:      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090\nResult = PASS\n```\n\n# Tensorflow & Keras\n\n`python-tensorflow-opt-cuda` vs `python-tensorflow-cuda`\n- opt might be slightly faster as the package is optimized for certain intel cpu\n\n```bash\nsudo pacman -S python-tensorflow-cuda keras\n```\n\n# Try it out!\n```python\nfrom keras import layers  \nfrom keras import models  \nmodel = models.Sequential()  \nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))  \nmodel.add(layers.MaxPooling2D((2, 2)))  \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))  \nmodel.add(layers.MaxPooling2D((2, 2)))  \nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))  \nmodel.add(layers.Flatten())  \nmodel.add(layers.Dense(64, activation='relu'))  \nmodel.add(layers.Dense(10, activation='softmax'))  \nprint(model.summary())  \n  \nfrom keras.datasets import mnist  \nfrom keras.utils import to_categorical  \n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  \n  \ntrain_images = train_images.reshape((60000, 28, 28, 1))  \ntrain_images = train_images.astype('float32') / 255  \ntest_images = test_images.reshape((10000, 28, 28, 1))  \ntest_images = test_images.astype('float32') / 255  \ntrain_labels = to_categorical(train_labels)  \ntest_labels = to_categorical(test_labels)  \nmodel.compile(optimizer='rmsprop',  \n\tloss='categorical_crossentropy',  \n\tmetrics=['accuracy'])  \nmodel.fit(train_images, train_labels, epochs=5, batch_size=64)  \n  \ntest_loss, test_acc = model.evaluate(test_images, test_labels)  \nprint('test_acc:', test_acc)\n```\n\n```\n2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n 2D)                                                             \n                                                                 \n conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n                                                                 \n flatten (Flatten)           (None, 576)               0         \n                                                                 \n dense (Dense)               (None, 64)                36928     \n                                                                 \n dense_1 (Dense)             (None, 10)                650       \n                                                                 \n=================================================================\nTotal params: 93,322\nTrainable params: 93,322\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/5\n2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600\n2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469\nEpoch 2/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853\nEpoch 3/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902\nEpoch 4/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919\nEpoch 5/5\n938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940\n313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880\ntest_acc: 0.9879999756813049\n```","slug":"System/archlinux-easy-setup-for-deep-learning","published":1,"updated":"2022-11-19T10:49:23.495Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clanvcfhe00017bsbfe457cxc","content":"<h1 id=\"My-setup\"><a href=\"#My-setup\" class=\"headerlink\" title=\"My setup\"></a>My setup</h1><ul>\n<li>CPU: AMD Ryzen 9 5900X</li>\n<li>GPU: NVIDIA GeForce RTX 3090 FE</li>\n<li>Archlinux 6.0.8-arch1-1</li>\n<li>WM i3</li>\n</ul>\n<h1 id=\"Update-pacman-source\"><a href=\"#Update-pacman-source\" class=\"headerlink\" title=\"Update pacman source\"></a>Update pacman source</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -Syu</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Nvidia-driver\"><a href=\"#Nvidia-driver\" class=\"headerlink\" title=\"Nvidia driver\"></a>Nvidia driver</h1><p><strong>Skip</strong> if you’ve installed properitary nvidia driver</p>\n<ul>\n<li>for those need to switch nouveu to nvidia, please check archlinux wiki for more information</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S nvidia nvidia-utils</span><br><span class=\"line\"><span class=\"comment\"># or nvidia-dkms and linux-header for custom kernel</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"CUDA-amp-CUDNN\"><a href=\"#CUDA-amp-CUDNN\" class=\"headerlink\" title=\"CUDA &amp; CUDNN\"></a>CUDA &amp; CUDNN</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S cuda cudnn</span><br></pre></td></tr></table></figure>\n\n<p>If you havn’t set the path for dynamic&#x2F;shared library, it’s time to set it. Otherwise errors like <code># Unimplemented: DNN library is not found</code> might present when running CNN</p>\n<p>Add the following line to either <code>~/.bashrc</code> or <code>~/.bash_profile</code> or <code>.profile</code> (last two are preferred)</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> LD_LIBRARY_PATH=<span class=\"string\">&quot;<span class=\"variable\">$&#123;LD_LIBRARY_PATH&#125;</span>:/usr/lib&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>Verify the installation</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> /opt/cuda/extras/demo_suite</span><br><span class=\"line\">./deviceQuery</span><br></pre></td></tr></table></figure>\n<span id=\"more\"></span>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class=\"line\"></span><br><span class=\"line\">Detected 1 CUDA Capable device(s)</span><br><span class=\"line\"></span><br><span class=\"line\">Device 0: &quot;NVIDIA GeForce RTX 3090&quot;</span><br><span class=\"line\">  CUDA Driver Version / Runtime Version          11.8 / 11.8</span><br><span class=\"line\">  CUDA Capability Major/Minor version number:    8.6</span><br><span class=\"line\">  Total amount of global memory:                 24265 MBytes (25443893248 bytes)</span><br><span class=\"line\">  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores</span><br><span class=\"line\">  GPU Max Clock rate:                            1695 MHz (1.70 GHz)</span><br><span class=\"line\">  Memory Clock rate:                             9751 Mhz</span><br><span class=\"line\">  Memory Bus Width:                              384-bit</span><br><span class=\"line\">  L2 Cache Size:                                 6291456 bytes</span><br><span class=\"line\">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class=\"line\">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class=\"line\">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class=\"line\">  Total amount of constant memory:               65536 bytes</span><br><span class=\"line\">  Total amount of shared memory per block:       49152 bytes</span><br><span class=\"line\">  Total number of registers available per block: 65536</span><br><span class=\"line\">  Warp size:                                     32</span><br><span class=\"line\">  Maximum number of threads per multiprocessor:  1536</span><br><span class=\"line\">  Maximum number of threads per block:           1024</span><br><span class=\"line\">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class=\"line\">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class=\"line\">  Maximum memory pitch:                          2147483647 bytes</span><br><span class=\"line\">  Texture alignment:                             512 bytes</span><br><span class=\"line\">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class=\"line\">  Run time limit on kernels:                     Yes</span><br><span class=\"line\">  Integrated GPU sharing Host Memory:            No</span><br><span class=\"line\">  Support host page-locked memory mapping:       Yes</span><br><span class=\"line\">  Alignment requirement for Surfaces:            Yes</span><br><span class=\"line\">  Device has ECC support:                        Disabled</span><br><span class=\"line\">  Device supports Unified Addressing (UVA):      Yes</span><br><span class=\"line\">  Device supports Compute Preemption:            Yes</span><br><span class=\"line\">  Supports Cooperative Kernel Launch:            Yes</span><br><span class=\"line\">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class=\"line\">  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0</span><br><span class=\"line\">  Compute Mode:</span><br><span class=\"line\">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class=\"line\"></span><br><span class=\"line\">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090</span><br><span class=\"line\">Result = PASS</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Tensorflow-amp-Keras\"><a href=\"#Tensorflow-amp-Keras\" class=\"headerlink\" title=\"Tensorflow &amp; Keras\"></a>Tensorflow &amp; Keras</h1><p><code>python-tensorflow-opt-cuda</code> vs <code>python-tensorflow-cuda</code></p>\n<ul>\n<li>opt might be slightly faster as the package is optimized for certain intel cpu</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S python-tensorflow-cuda keras</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Try-it-out\"><a href=\"#Try-it-out\" class=\"headerlink\" title=\"Try it out!\"></a>Try it out!</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> layers  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> models  </span><br><span class=\"line\">model = models.Sequential()  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">32</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, input_shape=(<span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>)))  </span><br><span class=\"line\">model.add(layers.MaxPooling2D((<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.MaxPooling2D((<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.Flatten())  </span><br><span class=\"line\">model.add(layers.Dense(<span class=\"number\">64</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;softmax&#x27;</span>))  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(model.summary())  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.datasets <span class=\"keyword\">import</span> mnist  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.utils <span class=\"keyword\">import</span> to_categorical  </span><br><span class=\"line\">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  </span><br><span class=\"line\">  </span><br><span class=\"line\">train_images = train_images.reshape((<span class=\"number\">60000</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))  </span><br><span class=\"line\">train_images = train_images.astype(<span class=\"string\">&#x27;float32&#x27;</span>) / <span class=\"number\">255</span>  </span><br><span class=\"line\">test_images = test_images.reshape((<span class=\"number\">10000</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))  </span><br><span class=\"line\">test_images = test_images.astype(<span class=\"string\">&#x27;float32&#x27;</span>) / <span class=\"number\">255</span>  </span><br><span class=\"line\">train_labels = to_categorical(train_labels)  </span><br><span class=\"line\">test_labels = to_categorical(test_labels)  </span><br><span class=\"line\">model.<span class=\"built_in\">compile</span>(optimizer=<span class=\"string\">&#x27;rmsprop&#x27;</span>,  </span><br><span class=\"line\">\tloss=<span class=\"string\">&#x27;categorical_crossentropy&#x27;</span>,  </span><br><span class=\"line\">\tmetrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])  </span><br><span class=\"line\">model.fit(train_images, train_labels, epochs=<span class=\"number\">5</span>, batch_size=<span class=\"number\">64</span>)  </span><br><span class=\"line\">  </span><br><span class=\"line\">test_loss, test_acc = model.evaluate(test_images, test_labels)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;test_acc:&#x27;</span>, test_acc)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class=\"line\">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class=\"line\">2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</span><br><span class=\"line\">2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class=\"line\">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class=\"line\">2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6</span><br><span class=\"line\">2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.</span><br><span class=\"line\">Model: &quot;sequential&quot;</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\"> Layer (type)                Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"> conv2d (Conv2D)             (None, 26, 26, 32)        320       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         </span><br><span class=\"line\"> )                                                               </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         </span><br><span class=\"line\"> 2D)                                                             </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> flatten (Flatten)           (None, 576)               0         </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense (Dense)               (None, 64)                36928     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_1 (Dense)             (None, 10)                650       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 93,322</span><br><span class=\"line\">Trainable params: 93,322</span><br><span class=\"line\">Non-trainable params: 0</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">None</span><br><span class=\"line\">Epoch 1/5</span><br><span class=\"line\">2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600</span><br><span class=\"line\">2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.</span><br><span class=\"line\">938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469</span><br><span class=\"line\">Epoch 2/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853</span><br><span class=\"line\">Epoch 3/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902</span><br><span class=\"line\">Epoch 4/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919</span><br><span class=\"line\">Epoch 5/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940</span><br><span class=\"line\">313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880</span><br><span class=\"line\">test_acc: 0.9879999756813049</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<h1 id=\"My-setup\"><a href=\"#My-setup\" class=\"headerlink\" title=\"My setup\"></a>My setup</h1><ul>\n<li>CPU: AMD Ryzen 9 5900X</li>\n<li>GPU: NVIDIA GeForce RTX 3090 FE</li>\n<li>Archlinux 6.0.8-arch1-1</li>\n<li>WM i3</li>\n</ul>\n<h1 id=\"Update-pacman-source\"><a href=\"#Update-pacman-source\" class=\"headerlink\" title=\"Update pacman source\"></a>Update pacman source</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -Syu</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Nvidia-driver\"><a href=\"#Nvidia-driver\" class=\"headerlink\" title=\"Nvidia driver\"></a>Nvidia driver</h1><p><strong>Skip</strong> if you’ve installed properitary nvidia driver</p>\n<ul>\n<li>for those need to switch nouveu to nvidia, please check archlinux wiki for more information</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S nvidia nvidia-utils</span><br><span class=\"line\"><span class=\"comment\"># or nvidia-dkms and linux-header for custom kernel</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"CUDA-amp-CUDNN\"><a href=\"#CUDA-amp-CUDNN\" class=\"headerlink\" title=\"CUDA &amp; CUDNN\"></a>CUDA &amp; CUDNN</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S cuda cudnn</span><br></pre></td></tr></table></figure>\n\n<p>If you havn’t set the path for dynamic&#x2F;shared library, it’s time to set it. Otherwise errors like <code># Unimplemented: DNN library is not found</code> might present when running CNN</p>\n<p>Add the following line to either <code>~/.bashrc</code> or <code>~/.bash_profile</code> or <code>.profile</code> (last two are preferred)</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> LD_LIBRARY_PATH=<span class=\"string\">&quot;<span class=\"variable\">$&#123;LD_LIBRARY_PATH&#125;</span>:/usr/lib&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>Verify the installation</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> /opt/cuda/extras/demo_suite</span><br><span class=\"line\">./deviceQuery</span><br></pre></td></tr></table></figure>","more":"<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class=\"line\"></span><br><span class=\"line\">Detected 1 CUDA Capable device(s)</span><br><span class=\"line\"></span><br><span class=\"line\">Device 0: &quot;NVIDIA GeForce RTX 3090&quot;</span><br><span class=\"line\">  CUDA Driver Version / Runtime Version          11.8 / 11.8</span><br><span class=\"line\">  CUDA Capability Major/Minor version number:    8.6</span><br><span class=\"line\">  Total amount of global memory:                 24265 MBytes (25443893248 bytes)</span><br><span class=\"line\">  (82) Multiprocessors, (128) CUDA Cores/MP:     10496 CUDA Cores</span><br><span class=\"line\">  GPU Max Clock rate:                            1695 MHz (1.70 GHz)</span><br><span class=\"line\">  Memory Clock rate:                             9751 Mhz</span><br><span class=\"line\">  Memory Bus Width:                              384-bit</span><br><span class=\"line\">  L2 Cache Size:                                 6291456 bytes</span><br><span class=\"line\">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class=\"line\">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class=\"line\">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class=\"line\">  Total amount of constant memory:               65536 bytes</span><br><span class=\"line\">  Total amount of shared memory per block:       49152 bytes</span><br><span class=\"line\">  Total number of registers available per block: 65536</span><br><span class=\"line\">  Warp size:                                     32</span><br><span class=\"line\">  Maximum number of threads per multiprocessor:  1536</span><br><span class=\"line\">  Maximum number of threads per block:           1024</span><br><span class=\"line\">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class=\"line\">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class=\"line\">  Maximum memory pitch:                          2147483647 bytes</span><br><span class=\"line\">  Texture alignment:                             512 bytes</span><br><span class=\"line\">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class=\"line\">  Run time limit on kernels:                     Yes</span><br><span class=\"line\">  Integrated GPU sharing Host Memory:            No</span><br><span class=\"line\">  Support host page-locked memory mapping:       Yes</span><br><span class=\"line\">  Alignment requirement for Surfaces:            Yes</span><br><span class=\"line\">  Device has ECC support:                        Disabled</span><br><span class=\"line\">  Device supports Unified Addressing (UVA):      Yes</span><br><span class=\"line\">  Device supports Compute Preemption:            Yes</span><br><span class=\"line\">  Supports Cooperative Kernel Launch:            Yes</span><br><span class=\"line\">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class=\"line\">  Device PCI Domain ID / Bus ID / location ID:   0 / 9 / 0</span><br><span class=\"line\">  Compute Mode:</span><br><span class=\"line\">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class=\"line\"></span><br><span class=\"line\">deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1, Device0 = NVIDIA GeForce RTX 3090</span><br><span class=\"line\">Result = PASS</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Tensorflow-amp-Keras\"><a href=\"#Tensorflow-amp-Keras\" class=\"headerlink\" title=\"Tensorflow &amp; Keras\"></a>Tensorflow &amp; Keras</h1><p><code>python-tensorflow-opt-cuda</code> vs <code>python-tensorflow-cuda</code></p>\n<ul>\n<li>opt might be slightly faster as the package is optimized for certain intel cpu</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pacman -S python-tensorflow-cuda keras</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Try-it-out\"><a href=\"#Try-it-out\" class=\"headerlink\" title=\"Try it out!\"></a>Try it out!</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> layers  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras <span class=\"keyword\">import</span> models  </span><br><span class=\"line\">model = models.Sequential()  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">32</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>, input_shape=(<span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>)))  </span><br><span class=\"line\">model.add(layers.MaxPooling2D((<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.MaxPooling2D((<span class=\"number\">2</span>, <span class=\"number\">2</span>)))  </span><br><span class=\"line\">model.add(layers.Conv2D(<span class=\"number\">64</span>, (<span class=\"number\">3</span>, <span class=\"number\">3</span>), activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.Flatten())  </span><br><span class=\"line\">model.add(layers.Dense(<span class=\"number\">64</span>, activation=<span class=\"string\">&#x27;relu&#x27;</span>))  </span><br><span class=\"line\">model.add(layers.Dense(<span class=\"number\">10</span>, activation=<span class=\"string\">&#x27;softmax&#x27;</span>))  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(model.summary())  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.datasets <span class=\"keyword\">import</span> mnist  </span><br><span class=\"line\"><span class=\"keyword\">from</span> keras.utils <span class=\"keyword\">import</span> to_categorical  </span><br><span class=\"line\">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  </span><br><span class=\"line\">  </span><br><span class=\"line\">train_images = train_images.reshape((<span class=\"number\">60000</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))  </span><br><span class=\"line\">train_images = train_images.astype(<span class=\"string\">&#x27;float32&#x27;</span>) / <span class=\"number\">255</span>  </span><br><span class=\"line\">test_images = test_images.reshape((<span class=\"number\">10000</span>, <span class=\"number\">28</span>, <span class=\"number\">28</span>, <span class=\"number\">1</span>))  </span><br><span class=\"line\">test_images = test_images.astype(<span class=\"string\">&#x27;float32&#x27;</span>) / <span class=\"number\">255</span>  </span><br><span class=\"line\">train_labels = to_categorical(train_labels)  </span><br><span class=\"line\">test_labels = to_categorical(test_labels)  </span><br><span class=\"line\">model.<span class=\"built_in\">compile</span>(optimizer=<span class=\"string\">&#x27;rmsprop&#x27;</span>,  </span><br><span class=\"line\">\tloss=<span class=\"string\">&#x27;categorical_crossentropy&#x27;</span>,  </span><br><span class=\"line\">\tmetrics=[<span class=\"string\">&#x27;accuracy&#x27;</span>])  </span><br><span class=\"line\">model.fit(train_images, train_labels, epochs=<span class=\"number\">5</span>, batch_size=<span class=\"number\">64</span>)  </span><br><span class=\"line\">  </span><br><span class=\"line\">test_loss, test_acc = model.evaluate(test_images, test_labels)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;test_acc:&#x27;</span>, test_acc)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2022-11-19 02:48:54.199427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class=\"line\">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class=\"line\">2022-11-19 02:48:54.280223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</span><br><span class=\"line\">2022-11-19 02:48:55.103112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.112918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA</span><br><span class=\"line\">To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</span><br><span class=\"line\">2022-11-19 02:48:55.114464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.114573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.114662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class=\"line\">2022-11-19 02:48:55.310662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21342 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6</span><br><span class=\"line\">2022-11-19 02:48:55.310982: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.</span><br><span class=\"line\">Model: &quot;sequential&quot;</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\"> Layer (type)                Output Shape              Param #   </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\"> conv2d (Conv2D)             (None, 26, 26, 32)        320       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         </span><br><span class=\"line\"> )                                                               </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         </span><br><span class=\"line\"> 2D)                                                             </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> flatten (Flatten)           (None, 576)               0         </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense (Dense)               (None, 64)                36928     </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\"> dense_1 (Dense)             (None, 10)                650       </span><br><span class=\"line\">                                                                 </span><br><span class=\"line\">=================================================================</span><br><span class=\"line\">Total params: 93,322</span><br><span class=\"line\">Trainable params: 93,322</span><br><span class=\"line\">Non-trainable params: 0</span><br><span class=\"line\">_________________________________________________________________</span><br><span class=\"line\">None</span><br><span class=\"line\">Epoch 1/5</span><br><span class=\"line\">2022-11-19 02:48:56.553880: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8600</span><br><span class=\"line\">2022-11-19 02:48:56.923119: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.</span><br><span class=\"line\">938/938 [==============================] - 3s 2ms/step - loss: 0.1702 - accuracy: 0.9469</span><br><span class=\"line\">Epoch 2/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0460 - accuracy: 0.9853</span><br><span class=\"line\">Epoch 3/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0321 - accuracy: 0.9902</span><br><span class=\"line\">Epoch 4/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9919</span><br><span class=\"line\">Epoch 5/5</span><br><span class=\"line\">938/938 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9940</span><br><span class=\"line\">313/313 [==============================] - 0s 970us/step - loss: 0.0403 - accuracy: 0.9880</span><br><span class=\"line\">test_acc: 0.9879999756813049</span><br></pre></td></tr></table></figure>"},{"title":"Emoji Not Showing Properly on Archlinux.md","date":"2022-06-03T09:26:39.000Z","_content":"\nThe issue persisted for a long time. \n\nEmojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today \n\n[Original Solution](https://flammie.github.io/dotfiles/fontconfig.html)\n\nInsert the following lines to `/etc/font/fonts.conf` inside the `<font-config>` tag\n\n```xml\n<match target=\"font\">\n\t\t<test name=\"family\" compare=\"contains\">\n\t\t\t<string>Emoji</string>\n\t\t</test>\n\t\t<edit name=\"hinting\" mode=\"assign\">\n\t\t\t<bool>true</bool>\n\t\t</edit>\n\t\t<edit name=\"hintstyle\" mode=\"assign\">\n\t\t\t<const>hintslight</const>\n\t\t</edit>\n\t\t<edit name=\"embeddedbitmap\" mode=\"assign\">\n\t\t\t<bool>true</bool>\n\t\t</edit>\n\t</match>\n```\n","source":"_posts/System/archlinux-emoji-display-problem.md","raw":"---\ntitle: Emoji Not Showing Properly on Archlinux.md\ncategories:\n  - System\ndate: 2022-06-03 02:26:39\n---\n\nThe issue persisted for a long time. \n\nEmojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today \n\n[Original Solution](https://flammie.github.io/dotfiles/fontconfig.html)\n\nInsert the following lines to `/etc/font/fonts.conf` inside the `<font-config>` tag\n\n```xml\n<match target=\"font\">\n\t\t<test name=\"family\" compare=\"contains\">\n\t\t\t<string>Emoji</string>\n\t\t</test>\n\t\t<edit name=\"hinting\" mode=\"assign\">\n\t\t\t<bool>true</bool>\n\t\t</edit>\n\t\t<edit name=\"hintstyle\" mode=\"assign\">\n\t\t\t<const>hintslight</const>\n\t\t</edit>\n\t\t<edit name=\"embeddedbitmap\" mode=\"assign\">\n\t\t\t<bool>true</bool>\n\t\t</edit>\n\t</match>\n```\n","slug":"System/archlinux-emoji-display-problem","published":1,"updated":"2022-11-19T10:04:46.659Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clanvcfhg00037bsb15ui6tgh","content":"<p>The issue persisted for a long time. </p>\n<p>Emojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today </p>\n<p><a href=\"https://flammie.github.io/dotfiles/fontconfig.html\">Original Solution</a></p>\n<p>Insert the following lines to <code>/etc/font/fonts.conf</code> inside the <code>&lt;font-config&gt;</code> tag</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">match</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;font&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">test</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;family&quot;</span> <span class=\"attr\">compare</span>=<span class=\"string\">&quot;contains&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">string</span>&gt;</span>Emoji<span class=\"tag\">&lt;/<span class=\"name\">string</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">test</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;hinting&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">bool</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">bool</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;hintstyle&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">const</span>&gt;</span>hintslight<span class=\"tag\">&lt;/<span class=\"name\">const</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;embeddedbitmap&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">bool</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">bool</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">match</span>&gt;</span></span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<p>The issue persisted for a long time. </p>\n<p>Emojis in kconsole are displayed as blocks(cubes) even though the noto-font-emoji had been installed. I found a solution today </p>\n<p><a href=\"https://flammie.github.io/dotfiles/fontconfig.html\">Original Solution</a></p>\n<p>Insert the following lines to <code>/etc/font/fonts.conf</code> inside the <code>&lt;font-config&gt;</code> tag</p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">match</span> <span class=\"attr\">target</span>=<span class=\"string\">&quot;font&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">test</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;family&quot;</span> <span class=\"attr\">compare</span>=<span class=\"string\">&quot;contains&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">string</span>&gt;</span>Emoji<span class=\"tag\">&lt;/<span class=\"name\">string</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">test</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;hinting&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">bool</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">bool</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;hintstyle&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">const</span>&gt;</span>hintslight<span class=\"tag\">&lt;/<span class=\"name\">const</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">edit</span> <span class=\"attr\">name</span>=<span class=\"string\">&quot;embeddedbitmap&quot;</span> <span class=\"attr\">mode</span>=<span class=\"string\">&quot;assign&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">bool</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">bool</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">edit</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">match</span>&gt;</span></span><br></pre></td></tr></table></figure>\n"},{"title":"Missing or Oversized Zoom Window on Arch","draft":false,"date":"2022-06-02T06:32:52.000Z","_content":"\nThe zoom client installed from AUR (and also the one from flatpak) had two weird issues:\n\n1. the window didn't show up\n2. the window is oversize and cannot be rescaled.\n\n\nchecked the terminal output (there's no output) and the log file under `$HOME/.zoom/log` (nothing suspicious).\n\n# Solution\nAfter few google searches, I found a workaround\n\n```sh\nvim $HOME/.config/zoomus.conf\n```\n\n```toml\nautoScale=false\n```\n\nI reinstalled the zoom (restart also works) and it worked\n\n","source":"_posts/System/archlinux-zoom-oversized-window.md","raw":"---\ntitle: Missing or Oversized Zoom Window on Arch\ntags:\n  - archlinux\n  - zoom\n  - solved\ndraft: false\ncategories:\n  - System\ndate: 2022-06-01 23:32:52\n---\n\nThe zoom client installed from AUR (and also the one from flatpak) had two weird issues:\n\n1. the window didn't show up\n2. the window is oversize and cannot be rescaled.\n\n\nchecked the terminal output (there's no output) and the log file under `$HOME/.zoom/log` (nothing suspicious).\n\n# Solution\nAfter few google searches, I found a workaround\n\n```sh\nvim $HOME/.config/zoomus.conf\n```\n\n```toml\nautoScale=false\n```\n\nI reinstalled the zoom (restart also works) and it worked\n\n","slug":"System/archlinux-zoom-oversized-window","published":1,"updated":"2022-11-19T10:05:15.799Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clanvcfhi00077bsb4vg47tup","content":"<p>The zoom client installed from AUR (and also the one from flatpak) had two weird issues:</p>\n<ol>\n<li>the window didn’t show up</li>\n<li>the window is oversize and cannot be rescaled.</li>\n</ol>\n<p>checked the terminal output (there’s no output) and the log file under <code>$HOME/.zoom/log</code> (nothing suspicious).</p>\n<h1 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h1><p>After few google searches, I found a workaround</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim <span class=\"variable\">$HOME</span>/.config/zoomus.conf</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight toml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">autoScale</span>=<span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n\n<p>I reinstalled the zoom (restart also works) and it worked</p>\n","site":{"data":{}},"excerpt":"","more":"<p>The zoom client installed from AUR (and also the one from flatpak) had two weird issues:</p>\n<ol>\n<li>the window didn’t show up</li>\n<li>the window is oversize and cannot be rescaled.</li>\n</ol>\n<p>checked the terminal output (there’s no output) and the log file under <code>$HOME/.zoom/log</code> (nothing suspicious).</p>\n<h1 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h1><p>After few google searches, I found a workaround</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vim <span class=\"variable\">$HOME</span>/.config/zoomus.conf</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight toml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">autoScale</span>=<span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n\n<p>I reinstalled the zoom (restart also works) and it worked</p>\n"},{"title":"Lazy Cat","date":"2022-11-19T07:47:39.000Z","excerpt":"Some funny cat pics 🐈🐈🐈","_content":"\n![](cateye.jpg)\n![](greasycat.jpg)\n\n","source":"_posts/Gallery/Cat/Funny.md","raw":"---\ntitle: Lazy Cat\ncategories:\n  - Gallery\n  - Cat\ndate: 2022-11-18 23:47:39\nexcerpt: Some funny cat pics 🐈🐈🐈\ntags:\n---\n\n![](cateye.jpg)\n![](greasycat.jpg)\n\n","slug":"Gallery/Cat/Funny","published":1,"updated":"2022-11-19T10:07:49.888Z","comments":1,"layout":"post","photos":[],"link":"","_id":"clanvcfhm000o7bsb8zmz38mg","content":"<p><img data-src=\"/2022/11/18/Gallery/Cat/Funny/cateye.jpg\"><br><img data-src=\"/2022/11/18/Gallery/Cat/Funny/greasycat.jpg\"></p>\n","site":{"data":{}},"more":"<p><img data-src=\"/2022/11/18/Gallery/Cat/Funny/cateye.jpg\"><br><img data-src=\"/2022/11/18/Gallery/Cat/Funny/greasycat.jpg\"></p>\n"},{"title":"Very Naive Neural Network in C++ Part 1 Matrix","date":"2021-01-06T02:10:44.000Z","_content":"# Why even bother writing any neural networks in C++?\nFor production purposes, C++ implementations are assumed to be faster since it can utilize the  hardware to their limit if coded properly. But in most cases, python is powerful enough, especially when the python community is much larger.\nBut sometime I wonder if I rely on those beautiful toolkits a bit too much. Imagine someday the python community collapses (purely fictional). Will it be awesome if you know how to code things in those \"ancient\" languages?\n\n# A 2D Matrix Class\nThe fundamental of any higher dimensional computation is matrix. There're actually a lot of implementation, but I still want to reinvent the wheel just for fun.\n\nLet's create one \n1. to reduce the typing workload, I will overload most of the algebraic operators.\n2. I need the dot operation and transpose\n3. I also need a few getters and setters\n4. I will put everything in a 2D `std::vector` \n<!-- more -->\n> It might be to avoid std container if I know how to handle the power of dynamic memory.\n\n```cpp\n#include <iostream>\nclass Matrix {  \n  \npublic:  \n    explicit Matrix(std::vector<std::vector<double>>);  \n    Matrix(int rows, int cols);  \n    Matrix(Matrix const &matrix) noexcept;  \n    ~Matrix();  \n  \n    // Operators overloading  \n    double &operator()(int row, int col);  \n    double operator()(int row, int col) const;  \n  \n    Matrix operator+(const Matrix &other) const;  \n    Matrix operator+(double scalar) const;  \n    Matrix operator-(const Matrix &other) const;  \n    Matrix operator*(const Matrix &other) const;  \n    Matrix operator*(double scalar) const;  \n    Matrix operator/(double scalar) const;  \n    Matrix &operator+=(const Matrix &other);  \n    Matrix &operator-=(const Matrix &other);  \n    Matrix &operator*=(double scalar);  \n    Matrix &operator/=(double scalar);  \n    Matrix &operator=(const Matrix &other);  \n    Matrix &operator=(Matrix &&other) noexcept; \n  \n    // Other computations  \n    Matrix dot(const Matrix &other) const;  \n    Matrix transpose() const;  \n    //Fetching Operations  \n    Matrix getRow(int row) const;  \n    Matrix getCol(int col) const; \n    // Getter  \n    int size() const;  \n    int rows() const;  \n    int cols() const;  \n    double sum() const;  \n    double mean() const; \n    std::pair<int, int> shape() const;  \n    bool isScalar() const;  \n  \n    // Setter  \n    Matrix changeAt(int row, int col, double value);  \n    // Manipulation  \n    void addRow(const Matrix &row);  \n    void setRow(int row, const Matrix &newRow);  \n  \n    // Static functions  \n    static Matrix EmptyMatrix() { return {0, 0}; };  \n  \n    // Print  \n    friend std::ostream &operator<<(std::ostream &os, const Matrix &matrix);  \n    void print() const;  \n    std::string printableShape() const;  \n  \nprivate:  \n    std::vector<std::vector<double>> data;  \n    void checkIfSameDimension(const Matrix &other) const;  \n    void checkForDotMultiplication(const Matrix &other) const;  \n  \n};  \n#endif //NAIVE_NN_MATRIX_H\n```\n\n\n# Constructors\n```cpp\n#include \"Matrix.h\"  \nMatrix::Matrix(int rows, int cols){  \n    // initialize the vector  \n    this->data = std::vector<std::vector<double>>(rows, std::vector<double>(cols, 0));  \n  \n}  \n  \nMatrix::Matrix(std::vector<std::vector<double>> data) {  \n    // Copy data from the array to the matrix  \n    this->data = std::move(data);  \n}  \n\n\nMatrix::Matrix(Matrix const &matrix) noexcept {  \n    this->data = matrix.data;  \n}  \nMatrix::~Matrix() = default;  \n```\n\n# Operators overloading\n- For the `*` multiplication, it is the similar to the broadcasting logic in numpy\n\n```cpp  \ndouble &Matrix::operator()(int row, int col) {  \n    return data[row][col];  \n}  \n  \ndouble Matrix::operator()(int row, int col) const {  \n    return data[row][col];  \n}  \n  \nMatrix Matrix::operator+(const Matrix &other) const {  \n    checkIfSameDimension(other);  \n    // Add two matrices  \n    Matrix result = Matrix(other.rows(), other.cols());  \n    for (int i = 0; i < other.rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            result(i, j) = this->data[i][j] + other(i, j);  \n        }  \n    }  \n    return result;  \n}  \n\nMatrix Matrix::operator+(double scalar) const {  \n    Matrix result = Matrix(this->rows(), this->cols());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            result(i, j) = this->data[i][j] + scalar;  \n        }  \n    }  \n    return result;  \n} \n  \nMatrix Matrix::operator-(const Matrix &other) const {  \n    checkIfSameDimension(other);  \n  \n    Matrix result = Matrix(other.rows(), other.cols());  \n    for (int i = 0; i < other.rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            result(i, j) = this->data[i][j] - other(i, j);  \n        }  \n    }  \n    return result;  \n  \n}  \n  \nMatrix Matrix::operator*(const Matrix &other) const {  \n    //Broadcasting  \n    // If one of the matrix is a scalar, then we can just multiply the scalar with the other matrix    if (this->isScalar()) {  \n        return other * this->data[0][0];  \n    } else if (other.isScalar()) {  \n        return *this * other.data[0][0];  \n    }  \n  \n    // If the two matrices has the same dimension, then we can just multiply them  \n    if (this->rows() == other.rows() && this->cols() == other.cols()) {  \n        Matrix result = Matrix(this->rows(), this->cols());  \n        for (int i = 0; i < this->rows(); i++) {  \n            for (int j = 0; j < this->cols(); j++) {  \n                result(i, j) = this->data[i][j] * other(i, j);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    // If the two matrices has the same number of rows and one of them has only one column, then we can just multiply them element-wise  \n    if (this->rows() == other.rows() && other.cols() == 1) {  \n        Matrix result = Matrix(this->rows(), this->cols());  \n        for (int i = 0; i < this->rows(); i++) {  \n            for (int j = 0; j < this->cols(); j++) {  \n                result(i, j) = this->data[i][j] * other(i, 0);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    if (this->rows() == other.rows() && this->cols() == 1) {  \n        Matrix result = Matrix(this->rows(), other.cols());  \n        for (int i = 0; i < this->rows(); i++) {  \n            for (int j = 0; j < other.cols(); j++) {  \n                result(i, j) = this->data[i][0] * other(i, j);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    // If the two matrices has the same number of columns and one of them has only one row, then we can just multiply them element-wise  \n    if (this->cols() == other.cols() && other.rows() == 1) {  \n        Matrix result = Matrix(this->rows(), this->cols());  \n        for (int i = 0; i < this->rows(); i++) {  \n            for (int j = 0; j < this->cols(); j++) {  \n                result(i, j) = this->data[i][j] * other(0, j);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    if (this->cols() == other.cols() && this->rows() == 1) {  \n        Matrix result = Matrix(other.rows(), this->cols());  \n        for (int i = 0; i < other.rows(); i++) {  \n            for (int j = 0; j < this->cols(); j++) {  \n                result(i, j) = this->data[0][j] * other(i, j);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    std::cerr << \"Shape mismatch: \" << this->printableShape() << \" and \" << other.printableShape() << std::endl;  \n    throw std::invalid_argument(\"The two matrices cannot be multiplied\");  \n}  \n  \nMatrix Matrix::operator*(double scalar) const {  \n    Matrix result = Matrix(this->rows(), this->cols());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            result(i, j) = this->data[i][j] * scalar;  \n        }  \n    }  \n    return result;  \n}  \n  \nMatrix Matrix::operator/(double scalar) const {  \n    Matrix result = Matrix(this->rows(), this->cols());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            result(i, j) = this->data[i][j] / scalar;  \n        }  \n    }  \n    return result;  \n}  \n  \nMatrix &Matrix::operator+=(const Matrix &other) {  \n    checkIfSameDimension(other);  \n    for (int i = 0; i < other.rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            this->data[i][j] += other(i, j);  \n        }  \n    }  \n    return *this;  \n}  \n  \nMatrix &Matrix::operator-=(const Matrix &other) {  \n    checkIfSameDimension(other);  \n  \n    for (int i = 0; i < other.rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            this->data[i][j] -= other(i, j);  \n        }  \n    }  \n    return *this;  \n}  \n  \nMatrix &Matrix::operator*=(double scalar) {  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            this->data[i][j] *= scalar;  \n        }  \n    }  \n    return *this;  \n}  \n  \nMatrix &Matrix::operator/=(double scalar) {  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            this->data[i][j] /= scalar;  \n        }  \n    }  \n    return *this;  \n}  \n  \nMatrix &Matrix::operator=(const Matrix &other) = default;  \n  \nMatrix &Matrix::operator=(Matrix &&other) noexcept {  \n    this->data = std::move(other.data);  \n    return *this;  \n}  \n  \n\n```\n\n# Some helper methods\n```cpp\nvoid Matrix::checkIfSameDimension(const Matrix &other) const {  \n    if (this->rows() != other.rows() || this->cols() != other.cols()) {  \n        throw std::invalid_argument(\"Matrix dimensions do not match\");  \n    }  \n}  \n  \nvoid Matrix::checkForDotMultiplication(const Matrix &other) const {  \n    if (this->cols() != other.rows()) {  \n        throw std::invalid_argument(\"Matrix dimensions do not match\");  \n    }  \n}  \n```\n\n# Dot and transpose\n```cpp  \nMatrix Matrix::dot(const Matrix &other) const {  \n    checkForDotMultiplication(other);  \n    Matrix result = Matrix(this->rows(), other.cols());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            double sum = 0;  \n            for (int k = 0; k < this->cols(); k++) {  \n                sum += this->data[i][k] * other(k, j);  \n            }  \n            result(i, j) = sum;  \n        }  \n    }  \n    return result;  \n} \nMatrix Matrix::transpose() const {  \n    Matrix result = Matrix(this->cols(), this->rows());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            result(j, i) = this->data[i][j];  \n        }  \n    }  \n    return result;  \n}  \n\n```\n\n# Basic stats\n- sum\n- mean\n- size\n- shape\n\n```cpp\n  \ndouble Matrix::sum() const {  \n    double sum = 0;  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            sum += this->data[i][j];  \n        }  \n    }  \n    return sum;  \n}  \n  \ndouble Matrix::mean() const {  \n    return this->sum() / this->size();  \n}  \n  \nint Matrix::size() const {  \n    return this->rows() * this->cols();  \n}  \n  \n \nint Matrix::rows() const {  \n    return (int)data.size();  \n}  \n  \nint Matrix::cols() const {  \n    if (data.empty()) {  \n        return 0;  \n    }  \n    return (int)data[0].size();  \n  \nstd::pair<int, int> Matrix::shape() const {  \n    return {rows(), cols()};  \n}  \n}  \n```\n# Row and column operations\n```cpp\nMatrix Matrix::getRow(int row) const {  \n    Matrix result = Matrix(1, this->cols());  \n    for (int i = 0; i < this->cols(); i++) {  \n        result(0, i) = this->data[row][i];  \n    }  \n    return result;  \n}  \n  \nMatrix Matrix::getCol(int col) const {  \n    Matrix result = Matrix(this->rows(), 1);  \n    for (int i = 0; i < this->rows(); i++) {  \n        result(i, 0) = this->data[i][col];  \n    }  \n    return result;  \n}  \n  \nvoid Matrix::addRow(const Matrix &row) {  \n    if ((row.cols() != this->cols() && this->rows() > 0)|| row.rows() < 1) {  \n        std::cerr << \"Shape mismatch: \" << this->printableShape() << \" and \" << row.printableShape() << std::endl;  \n        throw std::invalid_argument(\"Row dimensions do not match\");  \n    }  \n    this->data.push_back(row.data[0]);  \n}  \n  \nvoid Matrix::setRow(int row, const Matrix &newRow) {  \n    if (newRow.cols() != this->cols()) {  \n        throw std::invalid_argument(\"Row dimensions do not match\");  \n    }  \n    for (int j = 0; j < this->cols(); j++) {  \n        this->data[row][j] = newRow(0, j);  \n    }  \n}  \n  \nMatrix Matrix::changeAt(int row, int col, double value) {  \n    // check if row and col are in range  \n    if (row >= this->rows() || col >= this->cols()) {  \n        // print range  \n        std::cout << \"Row: \" << row << \" Col: \" << col << std::endl;  \n        std::cout << \"Max Rows: \" << this->rows() << \" Max Cols: \" << this->cols() << std::endl;  \n        throw std::invalid_argument(\"Row or column out of range\");  \n  \n    }  \n  \n    this->data[row][col] = value;  \n    return *this;  \n}  \n```\n\n# Formatted Output\n```cpp\n// To string  \nstd::ostream &operator<<(std::ostream &os, const Matrix &matrix) {  \n    for (int i = 0; i < matrix.rows(); i++) {  \n        for (int j = 0; j < matrix.cols(); j++) {  \n            os << matrix.data[i][j] << \", \";  \n        }  \n        os << std::endl;  \n    }  \n    return os;  \n}  \n\nvoid Matrix::print() const {  \n    for (auto &row : data) {  \n        for (auto &col : row) {  \n            std::cout << col << \" \";  \n        }  \n        std::cout << std::endl;  \n    }  \n}  \n  \nstd::string Matrix::printableShape() const {  \n    return \"(\" + std::to_string(this->rows()) + \", \" + std::to_string(this->cols()) + \")\";  \n}  \n  \n\n```\n\n# Simple check\n```cpp\nbool Matrix::isScalar() const {  \n    return this->rows() == 1 && this->cols() == 1;  \n}  \n```\n\nAnd we're done here, the build of the neural network will be in part 2","source":"_posts/Code/Naive-Neural-Network-in-Cpp-1.md","raw":"---\ntitle: Very Naive Neural Network in C++ Part 1 Matrix\ntags:\n  - c++\n  - deep-learning\n  - machine-learning\n  - neural-network\ncategories:\n  - Code\ndate: 2021-01-05 18:10:44\n---\n# Why even bother writing any neural networks in C++?\nFor production purposes, C++ implementations are assumed to be faster since it can utilize the  hardware to their limit if coded properly. But in most cases, python is powerful enough, especially when the python community is much larger.\nBut sometime I wonder if I rely on those beautiful toolkits a bit too much. Imagine someday the python community collapses (purely fictional). Will it be awesome if you know how to code things in those \"ancient\" languages?\n\n# A 2D Matrix Class\nThe fundamental of any higher dimensional computation is matrix. There're actually a lot of implementation, but I still want to reinvent the wheel just for fun.\n\nLet's create one \n1. to reduce the typing workload, I will overload most of the algebraic operators.\n2. I need the dot operation and transpose\n3. I also need a few getters and setters\n4. I will put everything in a 2D `std::vector` \n<!-- more -->\n> It might be to avoid std container if I know how to handle the power of dynamic memory.\n\n```cpp\n#include <iostream>\nclass Matrix {  \n  \npublic:  \n    explicit Matrix(std::vector<std::vector<double>>);  \n    Matrix(int rows, int cols);  \n    Matrix(Matrix const &matrix) noexcept;  \n    ~Matrix();  \n  \n    // Operators overloading  \n    double &operator()(int row, int col);  \n    double operator()(int row, int col) const;  \n  \n    Matrix operator+(const Matrix &other) const;  \n    Matrix operator+(double scalar) const;  \n    Matrix operator-(const Matrix &other) const;  \n    Matrix operator*(const Matrix &other) const;  \n    Matrix operator*(double scalar) const;  \n    Matrix operator/(double scalar) const;  \n    Matrix &operator+=(const Matrix &other);  \n    Matrix &operator-=(const Matrix &other);  \n    Matrix &operator*=(double scalar);  \n    Matrix &operator/=(double scalar);  \n    Matrix &operator=(const Matrix &other);  \n    Matrix &operator=(Matrix &&other) noexcept; \n  \n    // Other computations  \n    Matrix dot(const Matrix &other) const;  \n    Matrix transpose() const;  \n    //Fetching Operations  \n    Matrix getRow(int row) const;  \n    Matrix getCol(int col) const; \n    // Getter  \n    int size() const;  \n    int rows() const;  \n    int cols() const;  \n    double sum() const;  \n    double mean() const; \n    std::pair<int, int> shape() const;  \n    bool isScalar() const;  \n  \n    // Setter  \n    Matrix changeAt(int row, int col, double value);  \n    // Manipulation  \n    void addRow(const Matrix &row);  \n    void setRow(int row, const Matrix &newRow);  \n  \n    // Static functions  \n    static Matrix EmptyMatrix() { return {0, 0}; };  \n  \n    // Print  \n    friend std::ostream &operator<<(std::ostream &os, const Matrix &matrix);  \n    void print() const;  \n    std::string printableShape() const;  \n  \nprivate:  \n    std::vector<std::vector<double>> data;  \n    void checkIfSameDimension(const Matrix &other) const;  \n    void checkForDotMultiplication(const Matrix &other) const;  \n  \n};  \n#endif //NAIVE_NN_MATRIX_H\n```\n\n\n# Constructors\n```cpp\n#include \"Matrix.h\"  \nMatrix::Matrix(int rows, int cols){  \n    // initialize the vector  \n    this->data = std::vector<std::vector<double>>(rows, std::vector<double>(cols, 0));  \n  \n}  \n  \nMatrix::Matrix(std::vector<std::vector<double>> data) {  \n    // Copy data from the array to the matrix  \n    this->data = std::move(data);  \n}  \n\n\nMatrix::Matrix(Matrix const &matrix) noexcept {  \n    this->data = matrix.data;  \n}  \nMatrix::~Matrix() = default;  \n```\n\n# Operators overloading\n- For the `*` multiplication, it is the similar to the broadcasting logic in numpy\n\n```cpp  \ndouble &Matrix::operator()(int row, int col) {  \n    return data[row][col];  \n}  \n  \ndouble Matrix::operator()(int row, int col) const {  \n    return data[row][col];  \n}  \n  \nMatrix Matrix::operator+(const Matrix &other) const {  \n    checkIfSameDimension(other);  \n    // Add two matrices  \n    Matrix result = Matrix(other.rows(), other.cols());  \n    for (int i = 0; i < other.rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            result(i, j) = this->data[i][j] + other(i, j);  \n        }  \n    }  \n    return result;  \n}  \n\nMatrix Matrix::operator+(double scalar) const {  \n    Matrix result = Matrix(this->rows(), this->cols());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            result(i, j) = this->data[i][j] + scalar;  \n        }  \n    }  \n    return result;  \n} \n  \nMatrix Matrix::operator-(const Matrix &other) const {  \n    checkIfSameDimension(other);  \n  \n    Matrix result = Matrix(other.rows(), other.cols());  \n    for (int i = 0; i < other.rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            result(i, j) = this->data[i][j] - other(i, j);  \n        }  \n    }  \n    return result;  \n  \n}  \n  \nMatrix Matrix::operator*(const Matrix &other) const {  \n    //Broadcasting  \n    // If one of the matrix is a scalar, then we can just multiply the scalar with the other matrix    if (this->isScalar()) {  \n        return other * this->data[0][0];  \n    } else if (other.isScalar()) {  \n        return *this * other.data[0][0];  \n    }  \n  \n    // If the two matrices has the same dimension, then we can just multiply them  \n    if (this->rows() == other.rows() && this->cols() == other.cols()) {  \n        Matrix result = Matrix(this->rows(), this->cols());  \n        for (int i = 0; i < this->rows(); i++) {  \n            for (int j = 0; j < this->cols(); j++) {  \n                result(i, j) = this->data[i][j] * other(i, j);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    // If the two matrices has the same number of rows and one of them has only one column, then we can just multiply them element-wise  \n    if (this->rows() == other.rows() && other.cols() == 1) {  \n        Matrix result = Matrix(this->rows(), this->cols());  \n        for (int i = 0; i < this->rows(); i++) {  \n            for (int j = 0; j < this->cols(); j++) {  \n                result(i, j) = this->data[i][j] * other(i, 0);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    if (this->rows() == other.rows() && this->cols() == 1) {  \n        Matrix result = Matrix(this->rows(), other.cols());  \n        for (int i = 0; i < this->rows(); i++) {  \n            for (int j = 0; j < other.cols(); j++) {  \n                result(i, j) = this->data[i][0] * other(i, j);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    // If the two matrices has the same number of columns and one of them has only one row, then we can just multiply them element-wise  \n    if (this->cols() == other.cols() && other.rows() == 1) {  \n        Matrix result = Matrix(this->rows(), this->cols());  \n        for (int i = 0; i < this->rows(); i++) {  \n            for (int j = 0; j < this->cols(); j++) {  \n                result(i, j) = this->data[i][j] * other(0, j);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    if (this->cols() == other.cols() && this->rows() == 1) {  \n        Matrix result = Matrix(other.rows(), this->cols());  \n        for (int i = 0; i < other.rows(); i++) {  \n            for (int j = 0; j < this->cols(); j++) {  \n                result(i, j) = this->data[0][j] * other(i, j);  \n            }  \n        }  \n        return result;  \n    }  \n  \n    std::cerr << \"Shape mismatch: \" << this->printableShape() << \" and \" << other.printableShape() << std::endl;  \n    throw std::invalid_argument(\"The two matrices cannot be multiplied\");  \n}  \n  \nMatrix Matrix::operator*(double scalar) const {  \n    Matrix result = Matrix(this->rows(), this->cols());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            result(i, j) = this->data[i][j] * scalar;  \n        }  \n    }  \n    return result;  \n}  \n  \nMatrix Matrix::operator/(double scalar) const {  \n    Matrix result = Matrix(this->rows(), this->cols());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            result(i, j) = this->data[i][j] / scalar;  \n        }  \n    }  \n    return result;  \n}  \n  \nMatrix &Matrix::operator+=(const Matrix &other) {  \n    checkIfSameDimension(other);  \n    for (int i = 0; i < other.rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            this->data[i][j] += other(i, j);  \n        }  \n    }  \n    return *this;  \n}  \n  \nMatrix &Matrix::operator-=(const Matrix &other) {  \n    checkIfSameDimension(other);  \n  \n    for (int i = 0; i < other.rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            this->data[i][j] -= other(i, j);  \n        }  \n    }  \n    return *this;  \n}  \n  \nMatrix &Matrix::operator*=(double scalar) {  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            this->data[i][j] *= scalar;  \n        }  \n    }  \n    return *this;  \n}  \n  \nMatrix &Matrix::operator/=(double scalar) {  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            this->data[i][j] /= scalar;  \n        }  \n    }  \n    return *this;  \n}  \n  \nMatrix &Matrix::operator=(const Matrix &other) = default;  \n  \nMatrix &Matrix::operator=(Matrix &&other) noexcept {  \n    this->data = std::move(other.data);  \n    return *this;  \n}  \n  \n\n```\n\n# Some helper methods\n```cpp\nvoid Matrix::checkIfSameDimension(const Matrix &other) const {  \n    if (this->rows() != other.rows() || this->cols() != other.cols()) {  \n        throw std::invalid_argument(\"Matrix dimensions do not match\");  \n    }  \n}  \n  \nvoid Matrix::checkForDotMultiplication(const Matrix &other) const {  \n    if (this->cols() != other.rows()) {  \n        throw std::invalid_argument(\"Matrix dimensions do not match\");  \n    }  \n}  \n```\n\n# Dot and transpose\n```cpp  \nMatrix Matrix::dot(const Matrix &other) const {  \n    checkForDotMultiplication(other);  \n    Matrix result = Matrix(this->rows(), other.cols());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < other.cols(); j++) {  \n            double sum = 0;  \n            for (int k = 0; k < this->cols(); k++) {  \n                sum += this->data[i][k] * other(k, j);  \n            }  \n            result(i, j) = sum;  \n        }  \n    }  \n    return result;  \n} \nMatrix Matrix::transpose() const {  \n    Matrix result = Matrix(this->cols(), this->rows());  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            result(j, i) = this->data[i][j];  \n        }  \n    }  \n    return result;  \n}  \n\n```\n\n# Basic stats\n- sum\n- mean\n- size\n- shape\n\n```cpp\n  \ndouble Matrix::sum() const {  \n    double sum = 0;  \n    for (int i = 0; i < this->rows(); i++) {  \n        for (int j = 0; j < this->cols(); j++) {  \n            sum += this->data[i][j];  \n        }  \n    }  \n    return sum;  \n}  \n  \ndouble Matrix::mean() const {  \n    return this->sum() / this->size();  \n}  \n  \nint Matrix::size() const {  \n    return this->rows() * this->cols();  \n}  \n  \n \nint Matrix::rows() const {  \n    return (int)data.size();  \n}  \n  \nint Matrix::cols() const {  \n    if (data.empty()) {  \n        return 0;  \n    }  \n    return (int)data[0].size();  \n  \nstd::pair<int, int> Matrix::shape() const {  \n    return {rows(), cols()};  \n}  \n}  \n```\n# Row and column operations\n```cpp\nMatrix Matrix::getRow(int row) const {  \n    Matrix result = Matrix(1, this->cols());  \n    for (int i = 0; i < this->cols(); i++) {  \n        result(0, i) = this->data[row][i];  \n    }  \n    return result;  \n}  \n  \nMatrix Matrix::getCol(int col) const {  \n    Matrix result = Matrix(this->rows(), 1);  \n    for (int i = 0; i < this->rows(); i++) {  \n        result(i, 0) = this->data[i][col];  \n    }  \n    return result;  \n}  \n  \nvoid Matrix::addRow(const Matrix &row) {  \n    if ((row.cols() != this->cols() && this->rows() > 0)|| row.rows() < 1) {  \n        std::cerr << \"Shape mismatch: \" << this->printableShape() << \" and \" << row.printableShape() << std::endl;  \n        throw std::invalid_argument(\"Row dimensions do not match\");  \n    }  \n    this->data.push_back(row.data[0]);  \n}  \n  \nvoid Matrix::setRow(int row, const Matrix &newRow) {  \n    if (newRow.cols() != this->cols()) {  \n        throw std::invalid_argument(\"Row dimensions do not match\");  \n    }  \n    for (int j = 0; j < this->cols(); j++) {  \n        this->data[row][j] = newRow(0, j);  \n    }  \n}  \n  \nMatrix Matrix::changeAt(int row, int col, double value) {  \n    // check if row and col are in range  \n    if (row >= this->rows() || col >= this->cols()) {  \n        // print range  \n        std::cout << \"Row: \" << row << \" Col: \" << col << std::endl;  \n        std::cout << \"Max Rows: \" << this->rows() << \" Max Cols: \" << this->cols() << std::endl;  \n        throw std::invalid_argument(\"Row or column out of range\");  \n  \n    }  \n  \n    this->data[row][col] = value;  \n    return *this;  \n}  \n```\n\n# Formatted Output\n```cpp\n// To string  \nstd::ostream &operator<<(std::ostream &os, const Matrix &matrix) {  \n    for (int i = 0; i < matrix.rows(); i++) {  \n        for (int j = 0; j < matrix.cols(); j++) {  \n            os << matrix.data[i][j] << \", \";  \n        }  \n        os << std::endl;  \n    }  \n    return os;  \n}  \n\nvoid Matrix::print() const {  \n    for (auto &row : data) {  \n        for (auto &col : row) {  \n            std::cout << col << \" \";  \n        }  \n        std::cout << std::endl;  \n    }  \n}  \n  \nstd::string Matrix::printableShape() const {  \n    return \"(\" + std::to_string(this->rows()) + \", \" + std::to_string(this->cols()) + \")\";  \n}  \n  \n\n```\n\n# Simple check\n```cpp\nbool Matrix::isScalar() const {  \n    return this->rows() == 1 && this->cols() == 1;  \n}  \n```\n\nAnd we're done here, the build of the neural network will be in part 2","slug":"Code/Naive-Neural-Network-in-Cpp-1","published":1,"updated":"2022-12-16T23:27:15.785Z","_id":"clbo42yi10000o2sbhexn3dd3","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Why-even-bother-writing-any-neural-networks-in-C\"><a href=\"#Why-even-bother-writing-any-neural-networks-in-C\" class=\"headerlink\" title=\"Why even bother writing any neural networks in C++?\"></a>Why even bother writing any neural networks in C++?</h1><p>For production purposes, C++ implementations are assumed to be faster since it can utilize the  hardware to their limit if coded properly. But in most cases, python is powerful enough, especially when the python community is much larger.<br>But sometime I wonder if I rely on those beautiful toolkits a bit too much. Imagine someday the python community collapses (purely fictional). Will it be awesome if you know how to code things in those “ancient” languages?</p>\n<h1 id=\"A-2D-Matrix-Class\"><a href=\"#A-2D-Matrix-Class\" class=\"headerlink\" title=\"A 2D Matrix Class\"></a>A 2D Matrix Class</h1><p>The fundamental of any higher dimensional computation is matrix. There’re actually a lot of implementation, but I still want to reinvent the wheel just for fun.</p>\n<p>Let’s create one </p>\n<ol>\n<li>to reduce the typing workload, I will overload most of the algebraic operators.</li>\n<li>I need the dot operation and transpose</li>\n<li>I also need a few getters and setters</li>\n<li>I will put everything in a 2D <code>std::vector</code> <span id=\"more\"></span>\n<blockquote>\n<p>It might be to avoid std container if I know how to handle the power of dynamic memory.</p>\n</blockquote>\n</li>\n</ol>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Matrix</span> &#123;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">Matrix</span><span class=\"params\">(std::vector&lt;std::vector&lt;<span class=\"type\">double</span>&gt;&gt;)</span></span>;  </span><br><span class=\"line\">    <span class=\"built_in\">Matrix</span>(<span class=\"type\">int</span> rows, <span class=\"type\">int</span> cols);  </span><br><span class=\"line\">    <span class=\"built_in\">Matrix</span>(Matrix <span class=\"type\">const</span> &amp;matrix) <span class=\"keyword\">noexcept</span>;  </span><br><span class=\"line\">    ~<span class=\"built_in\">Matrix</span>();  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Operators overloading  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">double</span> &amp;<span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>+(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>+(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>-(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>*(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>*(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>/(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>+=(<span class=\"type\">const</span> Matrix &amp;other);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>-=(<span class=\"type\">const</span> Matrix &amp;other);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>*=(<span class=\"type\">double</span> scalar);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>/=(<span class=\"type\">double</span> scalar);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>=(<span class=\"type\">const</span> Matrix &amp;other);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>=(Matrix &amp;&amp;other) <span class=\"keyword\">noexcept</span>; </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Other computations  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">dot</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">transpose</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"comment\">//Fetching Operations  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getRow</span><span class=\"params\">(<span class=\"type\">int</span> row)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getCol</span><span class=\"params\">(<span class=\"type\">int</span> col)</span> <span class=\"type\">const</span></span>; </span><br><span class=\"line\">    <span class=\"comment\">// Getter  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">size</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">rows</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">cols</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">sum</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">mean</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>; </span><br><span class=\"line\">    <span class=\"function\">std::pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; <span class=\"title\">shape</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">isScalar</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Setter  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">changeAt</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col, <span class=\"type\">double</span> value)</span></span>;  </span><br><span class=\"line\">    <span class=\"comment\">// Manipulation  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">addRow</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;row)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">setRow</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">const</span> Matrix &amp;newRow)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Static functions  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">EmptyMatrix</span><span class=\"params\">()</span> </span>&#123; <span class=\"keyword\">return</span> &#123;<span class=\"number\">0</span>, <span class=\"number\">0</span>&#125;; &#125;;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Print  </span></span><br><span class=\"line\">    <span class=\"keyword\">friend</span> std::ostream &amp;<span class=\"keyword\">operator</span>&lt;&lt;(std::ostream &amp;os, <span class=\"type\">const</span> Matrix &amp;matrix);  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">print</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">std::string <span class=\"title\">printableShape</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">private</span>:  </span><br><span class=\"line\">    std::vector&lt;std::vector&lt;<span class=\"type\">double</span>&gt;&gt; data;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">checkIfSameDimension</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">checkForDotMultiplication</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;;  </span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">endif</span> <span class=\"comment\">//NAIVE_NN_MATRIX_H</span></span></span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"Constructors\"><a href=\"#Constructors\" class=\"headerlink\" title=\"Constructors\"></a>Constructors</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Matrix.h&quot;</span>  </span></span><br><span class=\"line\">Matrix::<span class=\"built_in\">Matrix</span>(<span class=\"type\">int</span> rows, <span class=\"type\">int</span> cols)&#123;  </span><br><span class=\"line\">    <span class=\"comment\">// initialize the vector  </span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data = std::vector&lt;std::vector&lt;<span class=\"type\">double</span>&gt;&gt;(rows, std::<span class=\"built_in\">vector</span>&lt;<span class=\"type\">double</span>&gt;(cols, <span class=\"number\">0</span>));  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix::<span class=\"built_in\">Matrix</span>(std::vector&lt;std::vector&lt;<span class=\"type\">double</span>&gt;&gt; data) &#123;  </span><br><span class=\"line\">    <span class=\"comment\">// Copy data from the array to the matrix  </span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data = std::<span class=\"built_in\">move</span>(data);  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Matrix::<span class=\"built_in\">Matrix</span>(Matrix <span class=\"type\">const</span> &amp;matrix) <span class=\"keyword\">noexcept</span> &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data = matrix.data;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">Matrix::~<span class=\"built_in\">Matrix</span>() = <span class=\"keyword\">default</span>;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Operators-overloading\"><a href=\"#Operators-overloading\" class=\"headerlink\" title=\"Operators overloading\"></a>Operators overloading</h1><ul>\n<li>For the <code>*</code> multiplication, it is the similar to the broadcasting logic in numpy</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> &amp;<span class=\"title\">Matrix::operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> data[row][col];  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">Matrix::operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> data[row][col];  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>+(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkIfSameDimension</span>(other);  </span><br><span class=\"line\">    <span class=\"comment\">// Add two matrices  </span></span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(other.<span class=\"built_in\">rows</span>(), other.<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] + <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>+(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] + scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125; </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>-(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkIfSameDimension</span>(other);  </span><br><span class=\"line\">  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(other.<span class=\"built_in\">rows</span>(), other.<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] - <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>*(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    <span class=\"comment\">//Broadcasting  </span></span><br><span class=\"line\">    <span class=\"comment\">// If one of the matrix is a scalar, then we can just multiply the scalar with the other matrix    if (this-&gt;isScalar()) &#123;  </span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> other * <span class=\"keyword\">this</span>-&gt;data[<span class=\"number\">0</span>][<span class=\"number\">0</span>];  </span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (other.<span class=\"built_in\">isScalar</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span> * other.data[<span class=\"number\">0</span>][<span class=\"number\">0</span>];  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// If the two matrices has the same dimension, then we can just multiply them  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == other.<span class=\"built_in\">rows</span>() &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == other.<span class=\"built_in\">cols</span>()) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] * <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// If the two matrices has the same number of rows and one of them has only one column, then we can just multiply them element-wise  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == other.<span class=\"built_in\">rows</span>() &amp;&amp; other.<span class=\"built_in\">cols</span>() == <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] * <span class=\"built_in\">other</span>(i, <span class=\"number\">0</span>);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == other.<span class=\"built_in\">rows</span>() &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), other.<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][<span class=\"number\">0</span>] * <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// If the two matrices has the same number of columns and one of them has only one row, then we can just multiply them element-wise  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == other.<span class=\"built_in\">cols</span>() &amp;&amp; other.<span class=\"built_in\">rows</span>() == <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] * <span class=\"built_in\">other</span>(<span class=\"number\">0</span>, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == other.<span class=\"built_in\">cols</span>() &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(other.<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[<span class=\"number\">0</span>][j] * <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    std::cerr &lt;&lt; <span class=\"string\">&quot;Shape mismatch: &quot;</span> &lt;&lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">printableShape</span>() &lt;&lt; <span class=\"string\">&quot; and &quot;</span> &lt;&lt; other.<span class=\"built_in\">printableShape</span>() &lt;&lt; std::endl;  </span><br><span class=\"line\">    <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;The two matrices cannot be multiplied&quot;</span>);  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>*(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] * scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>/(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] / scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>+=(<span class=\"type\">const</span> Matrix &amp;other) &#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkIfSameDimension</span>(other);  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;data[i][j] += <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>-=(<span class=\"type\">const</span> Matrix &amp;other) &#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkIfSameDimension</span>(other);  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;data[i][j] -= <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>*=(<span class=\"type\">double</span> scalar) &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;data[i][j] *= scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>/=(<span class=\"type\">double</span> scalar) &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;data[i][j] /= scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>=(<span class=\"type\">const</span> Matrix &amp;other) = <span class=\"keyword\">default</span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>=(Matrix &amp;&amp;other) <span class=\"keyword\">noexcept</span> &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data = std::<span class=\"built_in\">move</span>(other.data);  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Some-helper-methods\"><a href=\"#Some-helper-methods\" class=\"headerlink\" title=\"Some helper methods\"></a>Some helper methods</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::checkIfSameDimension</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() != other.<span class=\"built_in\">rows</span>() || <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() != other.<span class=\"built_in\">cols</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Matrix dimensions do not match&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::checkForDotMultiplication</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() != other.<span class=\"built_in\">rows</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Matrix dimensions do not match&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Dot-and-transpose\"><a href=\"#Dot-and-transpose\" class=\"headerlink\" title=\"Dot and transpose\"></a>Dot and transpose</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::dot</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkForDotMultiplication</span>(other);  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), other.<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"type\">double</span> sum = <span class=\"number\">0</span>;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> k = <span class=\"number\">0</span>; k &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); k++) &#123;  </span><br><span class=\"line\">                sum += <span class=\"keyword\">this</span>-&gt;data[i][k] * <span class=\"built_in\">other</span>(k, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = sum;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125; </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::transpose</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(j, i) = <span class=\"keyword\">this</span>-&gt;data[i][j];  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Basic-stats\"><a href=\"#Basic-stats\" class=\"headerlink\" title=\"Basic stats\"></a>Basic stats</h1><ul>\n<li>sum</li>\n<li>mean</li>\n<li>size</li>\n<li>shape</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">Matrix::sum</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"type\">double</span> sum = <span class=\"number\">0</span>;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            sum += <span class=\"keyword\">this</span>-&gt;data[i][j];  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> sum;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">Matrix::mean</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">sum</span>() / <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">size</span>();  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Matrix::size</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() * <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>();  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Matrix::rows</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"type\">int</span>)data.<span class=\"built_in\">size</span>();  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Matrix::cols</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (data.<span class=\"built_in\">empty</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"type\">int</span>)data[<span class=\"number\">0</span>].<span class=\"built_in\">size</span>();  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">std::pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; <span class=\"title\">Matrix::shape</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> &#123;<span class=\"built_in\">rows</span>(), <span class=\"built_in\">cols</span>()&#125;;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Row-and-column-operations\"><a href=\"#Row-and-column-operations\" class=\"headerlink\" title=\"Row and column operations\"></a>Row and column operations</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::getRow</span><span class=\"params\">(<span class=\"type\">int</span> row)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"number\">1</span>, <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"built_in\">result</span>(<span class=\"number\">0</span>, i) = <span class=\"keyword\">this</span>-&gt;data[row][i];  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::getCol</span><span class=\"params\">(<span class=\"type\">int</span> col)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"number\">1</span>);  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"built_in\">result</span>(i, <span class=\"number\">0</span>) = <span class=\"keyword\">this</span>-&gt;data[i][col];  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::addRow</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;row)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> ((row.<span class=\"built_in\">cols</span>() != <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() &gt; <span class=\"number\">0</span>)|| row.<span class=\"built_in\">rows</span>() &lt; <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        std::cerr &lt;&lt; <span class=\"string\">&quot;Shape mismatch: &quot;</span> &lt;&lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">printableShape</span>() &lt;&lt; <span class=\"string\">&quot; and &quot;</span> &lt;&lt; row.<span class=\"built_in\">printableShape</span>() &lt;&lt; std::endl;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Row dimensions do not match&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data.<span class=\"built_in\">push_back</span>(row.data[<span class=\"number\">0</span>]);  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::setRow</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">const</span> Matrix &amp;newRow)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (newRow.<span class=\"built_in\">cols</span>() != <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Row dimensions do not match&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;data[row][j] = <span class=\"built_in\">newRow</span>(<span class=\"number\">0</span>, j);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::changeAt</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col, <span class=\"type\">double</span> value)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"comment\">// check if row and col are in range  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (row &gt;= <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() || col &gt;= <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"comment\">// print range  </span></span><br><span class=\"line\">        std::cout &lt;&lt; <span class=\"string\">&quot;Row: &quot;</span> &lt;&lt; row &lt;&lt; <span class=\"string\">&quot; Col: &quot;</span> &lt;&lt; col &lt;&lt; std::endl;  </span><br><span class=\"line\">        std::cout &lt;&lt; <span class=\"string\">&quot;Max Rows: &quot;</span> &lt;&lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() &lt;&lt; <span class=\"string\">&quot; Max Cols: &quot;</span> &lt;&lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() &lt;&lt; std::endl;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Row or column out of range&quot;</span>);  </span><br><span class=\"line\">  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data[row][col] = value;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Formatted-Output\"><a href=\"#Formatted-Output\" class=\"headerlink\" title=\"Formatted Output\"></a>Formatted Output</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// To string  </span></span><br><span class=\"line\">std::ostream &amp;<span class=\"keyword\">operator</span>&lt;&lt;(std::ostream &amp;os, <span class=\"type\">const</span> Matrix &amp;matrix) &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; matrix.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; matrix.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            os &lt;&lt; matrix.data[i][j] &lt;&lt; <span class=\"string\">&quot;, &quot;</span>;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        os &lt;&lt; std::endl;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> os;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::print</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;row : data) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;col : row) &#123;  </span><br><span class=\"line\">            std::cout &lt;&lt; col &lt;&lt; <span class=\"string\">&quot; &quot;</span>;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        std::cout &lt;&lt; std::endl;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">std::string <span class=\"title\">Matrix::printableShape</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot;(&quot;</span> + std::<span class=\"built_in\">to_string</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>()) + <span class=\"string\">&quot;, &quot;</span> + std::<span class=\"built_in\">to_string</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>()) + <span class=\"string\">&quot;)&quot;</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Simple-check\"><a href=\"#Simple-check\" class=\"headerlink\" title=\"Simple check\"></a>Simple check</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">Matrix::isScalar</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == <span class=\"number\">1</span> &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == <span class=\"number\">1</span>;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<p>And we’re done here, the build of the neural network will be in part 2</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"Why-even-bother-writing-any-neural-networks-in-C\"><a href=\"#Why-even-bother-writing-any-neural-networks-in-C\" class=\"headerlink\" title=\"Why even bother writing any neural networks in C++?\"></a>Why even bother writing any neural networks in C++?</h1><p>For production purposes, C++ implementations are assumed to be faster since it can utilize the  hardware to their limit if coded properly. But in most cases, python is powerful enough, especially when the python community is much larger.<br>But sometime I wonder if I rely on those beautiful toolkits a bit too much. Imagine someday the python community collapses (purely fictional). Will it be awesome if you know how to code things in those “ancient” languages?</p>\n<h1 id=\"A-2D-Matrix-Class\"><a href=\"#A-2D-Matrix-Class\" class=\"headerlink\" title=\"A 2D Matrix Class\"></a>A 2D Matrix Class</h1><p>The fundamental of any higher dimensional computation is matrix. There’re actually a lot of implementation, but I still want to reinvent the wheel just for fun.</p>\n<p>Let’s create one </p>\n<ol>\n<li>to reduce the typing workload, I will overload most of the algebraic operators.</li>\n<li>I need the dot operation and transpose</li>\n<li>I also need a few getters and setters</li>\n<li>I will put everything in a 2D <code>std::vector</code>","more":"<blockquote>\n<p>It might be to avoid std container if I know how to handle the power of dynamic memory.</p>\n</blockquote>\n</li>\n</ol>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Matrix</span> &#123;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">Matrix</span><span class=\"params\">(std::vector&lt;std::vector&lt;<span class=\"type\">double</span>&gt;&gt;)</span></span>;  </span><br><span class=\"line\">    <span class=\"built_in\">Matrix</span>(<span class=\"type\">int</span> rows, <span class=\"type\">int</span> cols);  </span><br><span class=\"line\">    <span class=\"built_in\">Matrix</span>(Matrix <span class=\"type\">const</span> &amp;matrix) <span class=\"keyword\">noexcept</span>;  </span><br><span class=\"line\">    ~<span class=\"built_in\">Matrix</span>();  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Operators overloading  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">double</span> &amp;<span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>+(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>+(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>-(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>*(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>*(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix <span class=\"keyword\">operator</span>/(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span>;  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>+=(<span class=\"type\">const</span> Matrix &amp;other);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>-=(<span class=\"type\">const</span> Matrix &amp;other);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>*=(<span class=\"type\">double</span> scalar);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>/=(<span class=\"type\">double</span> scalar);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>=(<span class=\"type\">const</span> Matrix &amp;other);  </span><br><span class=\"line\">    Matrix &amp;<span class=\"keyword\">operator</span>=(Matrix &amp;&amp;other) <span class=\"keyword\">noexcept</span>; </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Other computations  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">dot</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">transpose</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"comment\">//Fetching Operations  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getRow</span><span class=\"params\">(<span class=\"type\">int</span> row)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getCol</span><span class=\"params\">(<span class=\"type\">int</span> col)</span> <span class=\"type\">const</span></span>; </span><br><span class=\"line\">    <span class=\"comment\">// Getter  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">size</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">rows</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">cols</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">sum</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">mean</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>; </span><br><span class=\"line\">    <span class=\"function\">std::pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; <span class=\"title\">shape</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">isScalar</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Setter  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">changeAt</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col, <span class=\"type\">double</span> value)</span></span>;  </span><br><span class=\"line\">    <span class=\"comment\">// Manipulation  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">addRow</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;row)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">setRow</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">const</span> Matrix &amp;newRow)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Static functions  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">EmptyMatrix</span><span class=\"params\">()</span> </span>&#123; <span class=\"keyword\">return</span> &#123;<span class=\"number\">0</span>, <span class=\"number\">0</span>&#125;; &#125;;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Print  </span></span><br><span class=\"line\">    <span class=\"keyword\">friend</span> std::ostream &amp;<span class=\"keyword\">operator</span>&lt;&lt;(std::ostream &amp;os, <span class=\"type\">const</span> Matrix &amp;matrix);  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">print</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">std::string <span class=\"title\">printableShape</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">private</span>:  </span><br><span class=\"line\">    std::vector&lt;std::vector&lt;<span class=\"type\">double</span>&gt;&gt; data;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">checkIfSameDimension</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">checkForDotMultiplication</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;;  </span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">endif</span> <span class=\"comment\">//NAIVE_NN_MATRIX_H</span></span></span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"Constructors\"><a href=\"#Constructors\" class=\"headerlink\" title=\"Constructors\"></a>Constructors</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Matrix.h&quot;</span>  </span></span><br><span class=\"line\">Matrix::<span class=\"built_in\">Matrix</span>(<span class=\"type\">int</span> rows, <span class=\"type\">int</span> cols)&#123;  </span><br><span class=\"line\">    <span class=\"comment\">// initialize the vector  </span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data = std::vector&lt;std::vector&lt;<span class=\"type\">double</span>&gt;&gt;(rows, std::<span class=\"built_in\">vector</span>&lt;<span class=\"type\">double</span>&gt;(cols, <span class=\"number\">0</span>));  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix::<span class=\"built_in\">Matrix</span>(std::vector&lt;std::vector&lt;<span class=\"type\">double</span>&gt;&gt; data) &#123;  </span><br><span class=\"line\">    <span class=\"comment\">// Copy data from the array to the matrix  </span></span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data = std::<span class=\"built_in\">move</span>(data);  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Matrix::<span class=\"built_in\">Matrix</span>(Matrix <span class=\"type\">const</span> &amp;matrix) <span class=\"keyword\">noexcept</span> &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data = matrix.data;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">Matrix::~<span class=\"built_in\">Matrix</span>() = <span class=\"keyword\">default</span>;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Operators-overloading\"><a href=\"#Operators-overloading\" class=\"headerlink\" title=\"Operators overloading\"></a>Operators overloading</h1><ul>\n<li>For the <code>*</code> multiplication, it is the similar to the broadcasting logic in numpy</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> &amp;<span class=\"title\">Matrix::operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> data[row][col];  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">Matrix::operator</span><span class=\"params\">()</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> data[row][col];  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>+(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkIfSameDimension</span>(other);  </span><br><span class=\"line\">    <span class=\"comment\">// Add two matrices  </span></span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(other.<span class=\"built_in\">rows</span>(), other.<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] + <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>+(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] + scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125; </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>-(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkIfSameDimension</span>(other);  </span><br><span class=\"line\">  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(other.<span class=\"built_in\">rows</span>(), other.<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] - <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>*(<span class=\"type\">const</span> Matrix &amp;other) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    <span class=\"comment\">//Broadcasting  </span></span><br><span class=\"line\">    <span class=\"comment\">// If one of the matrix is a scalar, then we can just multiply the scalar with the other matrix    if (this-&gt;isScalar()) &#123;  </span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> other * <span class=\"keyword\">this</span>-&gt;data[<span class=\"number\">0</span>][<span class=\"number\">0</span>];  </span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (other.<span class=\"built_in\">isScalar</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span> * other.data[<span class=\"number\">0</span>][<span class=\"number\">0</span>];  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// If the two matrices has the same dimension, then we can just multiply them  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == other.<span class=\"built_in\">rows</span>() &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == other.<span class=\"built_in\">cols</span>()) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] * <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// If the two matrices has the same number of rows and one of them has only one column, then we can just multiply them element-wise  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == other.<span class=\"built_in\">rows</span>() &amp;&amp; other.<span class=\"built_in\">cols</span>() == <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] * <span class=\"built_in\">other</span>(i, <span class=\"number\">0</span>);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == other.<span class=\"built_in\">rows</span>() &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), other.<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][<span class=\"number\">0</span>] * <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// If the two matrices has the same number of columns and one of them has only one row, then we can just multiply them element-wise  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == other.<span class=\"built_in\">cols</span>() &amp;&amp; other.<span class=\"built_in\">rows</span>() == <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] * <span class=\"built_in\">other</span>(<span class=\"number\">0</span>, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == other.<span class=\"built_in\">cols</span>() &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        Matrix result = <span class=\"built_in\">Matrix</span>(other.<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[<span class=\"number\">0</span>][j] * <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    std::cerr &lt;&lt; <span class=\"string\">&quot;Shape mismatch: &quot;</span> &lt;&lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">printableShape</span>() &lt;&lt; <span class=\"string\">&quot; and &quot;</span> &lt;&lt; other.<span class=\"built_in\">printableShape</span>() &lt;&lt; std::endl;  </span><br><span class=\"line\">    <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;The two matrices cannot be multiplied&quot;</span>);  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>*(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] * scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix Matrix::<span class=\"keyword\">operator</span>/(<span class=\"type\">double</span> scalar) <span class=\"type\">const</span> &#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = <span class=\"keyword\">this</span>-&gt;data[i][j] / scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>+=(<span class=\"type\">const</span> Matrix &amp;other) &#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkIfSameDimension</span>(other);  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;data[i][j] += <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>-=(<span class=\"type\">const</span> Matrix &amp;other) &#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkIfSameDimension</span>(other);  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; other.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;data[i][j] -= <span class=\"built_in\">other</span>(i, j);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>*=(<span class=\"type\">double</span> scalar) &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;data[i][j] *= scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>/=(<span class=\"type\">double</span> scalar) &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;data[i][j] /= scalar;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>=(<span class=\"type\">const</span> Matrix &amp;other) = <span class=\"keyword\">default</span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix &amp;Matrix::<span class=\"keyword\">operator</span>=(Matrix &amp;&amp;other) <span class=\"keyword\">noexcept</span> &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data = std::<span class=\"built_in\">move</span>(other.data);  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Some-helper-methods\"><a href=\"#Some-helper-methods\" class=\"headerlink\" title=\"Some helper methods\"></a>Some helper methods</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::checkIfSameDimension</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() != other.<span class=\"built_in\">rows</span>() || <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() != other.<span class=\"built_in\">cols</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Matrix dimensions do not match&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::checkForDotMultiplication</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() != other.<span class=\"built_in\">rows</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Matrix dimensions do not match&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Dot-and-transpose\"><a href=\"#Dot-and-transpose\" class=\"headerlink\" title=\"Dot and transpose\"></a>Dot and transpose</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::dot</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;other)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"built_in\">checkForDotMultiplication</span>(other);  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), other.<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; other.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"type\">double</span> sum = <span class=\"number\">0</span>;  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> k = <span class=\"number\">0</span>; k &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); k++) &#123;  </span><br><span class=\"line\">                sum += <span class=\"keyword\">this</span>-&gt;data[i][k] * <span class=\"built_in\">other</span>(k, j);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(i, j) = sum;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125; </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::transpose</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(), <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">result</span>(j, i) = <span class=\"keyword\">this</span>-&gt;data[i][j];  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Basic-stats\"><a href=\"#Basic-stats\" class=\"headerlink\" title=\"Basic stats\"></a>Basic stats</h1><ul>\n<li>sum</li>\n<li>mean</li>\n<li>size</li>\n<li>shape</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">Matrix::sum</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"type\">double</span> sum = <span class=\"number\">0</span>;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            sum += <span class=\"keyword\">this</span>-&gt;data[i][j];  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> sum;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">Matrix::mean</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">sum</span>() / <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">size</span>();  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Matrix::size</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() * <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>();  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Matrix::rows</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"type\">int</span>)data.<span class=\"built_in\">size</span>();  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Matrix::cols</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (data.<span class=\"built_in\">empty</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"type\">int</span>)data[<span class=\"number\">0</span>].<span class=\"built_in\">size</span>();  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">std::pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; <span class=\"title\">Matrix::shape</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> &#123;<span class=\"built_in\">rows</span>(), <span class=\"built_in\">cols</span>()&#125;;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Row-and-column-operations\"><a href=\"#Row-and-column-operations\" class=\"headerlink\" title=\"Row and column operations\"></a>Row and column operations</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::getRow</span><span class=\"params\">(<span class=\"type\">int</span> row)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"number\">1</span>, <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>());  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"built_in\">result</span>(<span class=\"number\">0</span>, i) = <span class=\"keyword\">this</span>-&gt;data[row][i];  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::getCol</span><span class=\"params\">(<span class=\"type\">int</span> col)</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    Matrix result = <span class=\"built_in\">Matrix</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(), <span class=\"number\">1</span>);  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"built_in\">result</span>(i, <span class=\"number\">0</span>) = <span class=\"keyword\">this</span>-&gt;data[i][col];  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::addRow</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;row)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> ((row.<span class=\"built_in\">cols</span>() != <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() &gt; <span class=\"number\">0</span>)|| row.<span class=\"built_in\">rows</span>() &lt; <span class=\"number\">1</span>) &#123;  </span><br><span class=\"line\">        std::cerr &lt;&lt; <span class=\"string\">&quot;Shape mismatch: &quot;</span> &lt;&lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">printableShape</span>() &lt;&lt; <span class=\"string\">&quot; and &quot;</span> &lt;&lt; row.<span class=\"built_in\">printableShape</span>() &lt;&lt; std::endl;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Row dimensions do not match&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data.<span class=\"built_in\">push_back</span>(row.data[<span class=\"number\">0</span>]);  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::setRow</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">const</span> Matrix &amp;newRow)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (newRow.<span class=\"built_in\">cols</span>() != <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Row dimensions do not match&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;data[row][j] = <span class=\"built_in\">newRow</span>(<span class=\"number\">0</span>, j);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Matrix::changeAt</span><span class=\"params\">(<span class=\"type\">int</span> row, <span class=\"type\">int</span> col, <span class=\"type\">double</span> value)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"comment\">// check if row and col are in range  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (row &gt;= <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() || col &gt;= <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"comment\">// print range  </span></span><br><span class=\"line\">        std::cout &lt;&lt; <span class=\"string\">&quot;Row: &quot;</span> &lt;&lt; row &lt;&lt; <span class=\"string\">&quot; Col: &quot;</span> &lt;&lt; col &lt;&lt; std::endl;  </span><br><span class=\"line\">        std::cout &lt;&lt; <span class=\"string\">&quot;Max Rows: &quot;</span> &lt;&lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() &lt;&lt; <span class=\"string\">&quot; Max Cols: &quot;</span> &lt;&lt; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() &lt;&lt; std::endl;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;Row or column out of range&quot;</span>);  </span><br><span class=\"line\">  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;data[row][col] = value;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> *<span class=\"keyword\">this</span>;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Formatted-Output\"><a href=\"#Formatted-Output\" class=\"headerlink\" title=\"Formatted Output\"></a>Formatted Output</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// To string  </span></span><br><span class=\"line\">std::ostream &amp;<span class=\"keyword\">operator</span>&lt;&lt;(std::ostream &amp;os, <span class=\"type\">const</span> Matrix &amp;matrix) &#123;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; matrix.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; matrix.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            os &lt;&lt; matrix.data[i][j] &lt;&lt; <span class=\"string\">&quot;, &quot;</span>;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        os &lt;&lt; std::endl;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> os;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Matrix::print</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;row : data) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;col : row) &#123;  </span><br><span class=\"line\">            std::cout &lt;&lt; col &lt;&lt; <span class=\"string\">&quot; &quot;</span>;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">        std::cout &lt;&lt; std::endl;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">std::string <span class=\"title\">Matrix::printableShape</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">&quot;(&quot;</span> + std::<span class=\"built_in\">to_string</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>()) + <span class=\"string\">&quot;, &quot;</span> + std::<span class=\"built_in\">to_string</span>(<span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>()) + <span class=\"string\">&quot;)&quot;</span>;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Simple-check\"><a href=\"#Simple-check\" class=\"headerlink\" title=\"Simple check\"></a>Simple check</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">Matrix::isScalar</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">rows</span>() == <span class=\"number\">1</span> &amp;&amp; <span class=\"keyword\">this</span>-&gt;<span class=\"built_in\">cols</span>() == <span class=\"number\">1</span>;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<p>And we’re done here, the build of the neural network will be in part 2</p>"},{"title":"Very Naive Neural Network in C++ Part 2 Activation functions","date":"2021-01-06T03:00:01.000Z","_content":"\nNow we've created the `Matrix`, we should build the layer that contains the neurons. But before that we need the activation functions. I'm gonna create a class that contains some static activation functions and their derivative forms (relu, sigmoid, tanh and softmax). These activation functions will take the input matrices and output the  matrices after applying the corresponding functions.\n\n<!-- more -->\n\n```cpp\n#include \"Matrix.h\"  \n#include <cmath>  \n\n//Let's put everything to an enum for better organization\nenum ActivationFunctionType {  \n    RELU,  \n    SIGMOID,  \n    TANH,  \n    SOFTMAX,  \n    NONE  \n};  \n  \nclass ActivationFunction {  \npublic:  \n\t//The activation and derivative methods will serve as proxies to call the required function types\n    static Matrix activation(const Matrix& input, ActivationFunctionType activationFunctionType);  \n    static Matrix derivative(const Matrix& input, ActivationFunctionType activationFunctionType);  \n  \nprivate:  \n    static Matrix reluActivation(Matrix input);  \n    static Matrix reluDerivative(Matrix input);  \n    static Matrix sigmoidActivation(Matrix input);  \n    static Matrix sigmoidDerivative(Matrix input);  \n    static Matrix tanhActivation(Matrix input);  \n    static Matrix tanhDerivative(Matrix input);  \n    static Matrix softmaxActivation(Matrix input);  \n    static Matrix softmaxDerivative(Matrix input);  \n};\n```\n\n\n```cpp\n#include \"ActivationFunction.h\"  \n  \nMatrix ActivationFunction::activation(const Matrix& input, ActivationFunctionType activationFunctionType) {  \n//    std::cout << \"ActivationFunction::activation\" << std::endl;  \n    switch (activationFunctionType) {  \n        case RELU:  \n            return ActivationFunction::reluActivation(input);  \n        case SIGMOID:  \n            return ActivationFunction::sigmoidActivation(input);  \n        case TANH:  \n            return ActivationFunction::tanhActivation(input);  \n        case SOFTMAX:  \n            return ActivationFunction::softmaxActivation(input);  \n        default:  \n            return input;  \n    }  \n}  \n  \nMatrix ActivationFunction::derivative(const Matrix& input, ActivationFunctionType activationFunctionType) {  \n    switch (activationFunctionType) {  \n        case RELU:  \n            return ActivationFunction::reluDerivative(input);  \n        case SIGMOID:  \n            return ActivationFunction::sigmoidDerivative(input);  \n        case TANH:  \n            return ActivationFunction::tanhDerivative(input);  \n        case SOFTMAX:  \n            return ActivationFunction::softmaxDerivative(input);  \n        default:  \n            return input;  \n    }  \n}  \n```\n# Relu\n```cpp \nMatrix ActivationFunction::reluActivation(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            if (input(i, j) < 0) {  \n                output(i, j) = 0;  \n            }  \n        }  \n    }  \n    return output;  \n}  \n  \nMatrix ActivationFunction::reluDerivative(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            if (input(i, j) < 0) {  \n                output(i, j) = 0;  \n            } else {  \n                output(i, j) = 1;  \n            }  \n        }  \n    }  \n    return output;  \n}  \n```\n\n# Softmax\n```cpp  \nMatrix ActivationFunction::softmaxActivation(Matrix input) {  \n    Matrix output = input;  \n    double sum = 0;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            sum += exp(input(i, j));  \n        }  \n    }  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = exp(input(i, j)) / sum;  \n        }  \n    }  \n    return output;  \n}  \n  \nMatrix ActivationFunction::softmaxDerivative(Matrix input) {  \n    Matrix output = input;  \n    double sum = 0;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            sum += exp(input(i, j));  \n        }  \n    }  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = exp(input(i, j)) / sum * (1 - exp(input(i, j)) / sum);  \n        }  \n    }  \n    return output;  \n}  \n  ```\n# Sigmoid \n```\nMatrix ActivationFunction::sigmoidActivation(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = 1 / (1 + exp(-input(i, j)));  \n        }  \n    }  \n    return output;  \n}  \n  \nMatrix ActivationFunction::sigmoidDerivative(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = exp(-input(i, j)) / pow(1 + exp(-input(i, j)), 2);  \n        }  \n    }  \n    return output;  \n}  \n```\n\n# Tanh\n```  \nMatrix ActivationFunction::tanhActivation(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = (exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j)));  \n        }  \n    }  \n    return output;  \n}  \n  \nMatrix ActivationFunction::tanhDerivative(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = 1 - pow((exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j))), 2);  \n        }  \n    }  \n    return output;  \n}\n```","source":"_posts/Code/Naive-Neural-Network-in-Cpp-2.md","raw":"---\ntitle: Very Naive Neural Network in C++ Part 2 Activation functions\ntags:\n  - c++\n  - deep-learning\n  - machine-learning\n  - neural-network\ncategories:\n  - Code\ndate: 2021-01-05 19:00:01\n---\n\nNow we've created the `Matrix`, we should build the layer that contains the neurons. But before that we need the activation functions. I'm gonna create a class that contains some static activation functions and their derivative forms (relu, sigmoid, tanh and softmax). These activation functions will take the input matrices and output the  matrices after applying the corresponding functions.\n\n<!-- more -->\n\n```cpp\n#include \"Matrix.h\"  \n#include <cmath>  \n\n//Let's put everything to an enum for better organization\nenum ActivationFunctionType {  \n    RELU,  \n    SIGMOID,  \n    TANH,  \n    SOFTMAX,  \n    NONE  \n};  \n  \nclass ActivationFunction {  \npublic:  \n\t//The activation and derivative methods will serve as proxies to call the required function types\n    static Matrix activation(const Matrix& input, ActivationFunctionType activationFunctionType);  \n    static Matrix derivative(const Matrix& input, ActivationFunctionType activationFunctionType);  \n  \nprivate:  \n    static Matrix reluActivation(Matrix input);  \n    static Matrix reluDerivative(Matrix input);  \n    static Matrix sigmoidActivation(Matrix input);  \n    static Matrix sigmoidDerivative(Matrix input);  \n    static Matrix tanhActivation(Matrix input);  \n    static Matrix tanhDerivative(Matrix input);  \n    static Matrix softmaxActivation(Matrix input);  \n    static Matrix softmaxDerivative(Matrix input);  \n};\n```\n\n\n```cpp\n#include \"ActivationFunction.h\"  \n  \nMatrix ActivationFunction::activation(const Matrix& input, ActivationFunctionType activationFunctionType) {  \n//    std::cout << \"ActivationFunction::activation\" << std::endl;  \n    switch (activationFunctionType) {  \n        case RELU:  \n            return ActivationFunction::reluActivation(input);  \n        case SIGMOID:  \n            return ActivationFunction::sigmoidActivation(input);  \n        case TANH:  \n            return ActivationFunction::tanhActivation(input);  \n        case SOFTMAX:  \n            return ActivationFunction::softmaxActivation(input);  \n        default:  \n            return input;  \n    }  \n}  \n  \nMatrix ActivationFunction::derivative(const Matrix& input, ActivationFunctionType activationFunctionType) {  \n    switch (activationFunctionType) {  \n        case RELU:  \n            return ActivationFunction::reluDerivative(input);  \n        case SIGMOID:  \n            return ActivationFunction::sigmoidDerivative(input);  \n        case TANH:  \n            return ActivationFunction::tanhDerivative(input);  \n        case SOFTMAX:  \n            return ActivationFunction::softmaxDerivative(input);  \n        default:  \n            return input;  \n    }  \n}  \n```\n# Relu\n```cpp \nMatrix ActivationFunction::reluActivation(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            if (input(i, j) < 0) {  \n                output(i, j) = 0;  \n            }  \n        }  \n    }  \n    return output;  \n}  \n  \nMatrix ActivationFunction::reluDerivative(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            if (input(i, j) < 0) {  \n                output(i, j) = 0;  \n            } else {  \n                output(i, j) = 1;  \n            }  \n        }  \n    }  \n    return output;  \n}  \n```\n\n# Softmax\n```cpp  \nMatrix ActivationFunction::softmaxActivation(Matrix input) {  \n    Matrix output = input;  \n    double sum = 0;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            sum += exp(input(i, j));  \n        }  \n    }  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = exp(input(i, j)) / sum;  \n        }  \n    }  \n    return output;  \n}  \n  \nMatrix ActivationFunction::softmaxDerivative(Matrix input) {  \n    Matrix output = input;  \n    double sum = 0;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            sum += exp(input(i, j));  \n        }  \n    }  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = exp(input(i, j)) / sum * (1 - exp(input(i, j)) / sum);  \n        }  \n    }  \n    return output;  \n}  \n  ```\n# Sigmoid \n```\nMatrix ActivationFunction::sigmoidActivation(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = 1 / (1 + exp(-input(i, j)));  \n        }  \n    }  \n    return output;  \n}  \n  \nMatrix ActivationFunction::sigmoidDerivative(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = exp(-input(i, j)) / pow(1 + exp(-input(i, j)), 2);  \n        }  \n    }  \n    return output;  \n}  \n```\n\n# Tanh\n```  \nMatrix ActivationFunction::tanhActivation(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = (exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j)));  \n        }  \n    }  \n    return output;  \n}  \n  \nMatrix ActivationFunction::tanhDerivative(Matrix input) {  \n    Matrix output = input;  \n    for (int i = 0; i < input.rows(); i++) {  \n        for (int j = 0; j < input.cols(); j++) {  \n            output(i, j) = 1 - pow((exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j))), 2);  \n        }  \n    }  \n    return output;  \n}\n```","slug":"Code/Naive-Neural-Network-in-Cpp-2","published":1,"updated":"2022-12-16T23:57:40.452Z","_id":"clbr3qe7p0000fjsba2v697fw","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Now we’ve created the <code>Matrix</code>, we should build the layer that contains the neurons. But before that we need the activation functions. I’m gonna create a class that contains some static activation functions and their derivative forms (relu, sigmoid, tanh and softmax). These activation functions will take the input matrices and output the  matrices after applying the corresponding functions.</p>\n<span id=\"more\"></span>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Matrix.h&quot;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;cmath&gt;</span>  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Let&#x27;s put everything to an enum for better organization</span></span><br><span class=\"line\"><span class=\"keyword\">enum</span> <span class=\"title class_\">ActivationFunctionType</span> &#123;  </span><br><span class=\"line\">    RELU,  </span><br><span class=\"line\">    SIGMOID,  </span><br><span class=\"line\">    TANH,  </span><br><span class=\"line\">    SOFTMAX,  </span><br><span class=\"line\">    NONE  </span><br><span class=\"line\">&#125;;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ActivationFunction</span> &#123;  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">\t<span class=\"comment\">//The activation and derivative methods will serve as proxies to call the required function types</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">activation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">derivative</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">private</span>:  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">reluActivation</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">reluDerivative</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">sigmoidActivation</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">sigmoidDerivative</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">tanhActivation</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">tanhDerivative</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">softmaxActivation</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">softmaxDerivative</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;ActivationFunction.h&quot;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::activation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span> </span>&#123;  </span><br><span class=\"line\"><span class=\"comment\">//    std::cout &lt;&lt; &quot;ActivationFunction::activation&quot; &lt;&lt; std::endl;  </span></span><br><span class=\"line\">    <span class=\"keyword\">switch</span> (activationFunctionType) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> RELU:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">reluActivation</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> SIGMOID:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">sigmoidActivation</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> TANH:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">tanhActivation</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> SOFTMAX:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">softmaxActivation</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">default</span>:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> input;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::derivative</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">switch</span> (activationFunctionType) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> RELU:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">reluDerivative</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> SIGMOID:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">sigmoidDerivative</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> TANH:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">tanhDerivative</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> SOFTMAX:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">softmaxDerivative</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">default</span>:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> input;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Relu\"><a href=\"#Relu\" class=\"headerlink\" title=\"Relu\"></a>Relu</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::reluActivation</span><span class=\"params\">(Matrix input)</span> </span>&#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">input</span>(i, j) &lt; <span class=\"number\">0</span>) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">output</span>(i, j) = <span class=\"number\">0</span>;  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> output;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::reluDerivative</span><span class=\"params\">(Matrix input)</span> </span>&#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">input</span>(i, j) &lt; <span class=\"number\">0</span>) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">output</span>(i, j) = <span class=\"number\">0</span>;  </span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">output</span>(i, j) = <span class=\"number\">1</span>;  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> output;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Softmax\"><a href=\"#Softmax\" class=\"headerlink\" title=\"Softmax\"></a>Softmax</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::softmaxActivation</span><span class=\"params\">(Matrix input)</span> </span>&#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    <span class=\"type\">double</span> sum = <span class=\"number\">0</span>;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            sum += <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">output</span>(i, j) = <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j)) / sum;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> output;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::softmaxDerivative</span><span class=\"params\">(Matrix input)</span> </span>&#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    <span class=\"type\">double</span> sum = <span class=\"number\">0</span>;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            sum += <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">output</span>(i, j) = <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j)) / sum * (<span class=\"number\">1</span> - <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j)) / sum);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> output;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Matrix ActivationFunction::sigmoidActivation(Matrix input) &#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class=\"line\">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class=\"line\">            output(i, j) = 1 / (1 + exp(-input(i, j)));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    return output;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix ActivationFunction::sigmoidDerivative(Matrix input) &#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class=\"line\">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class=\"line\">            output(i, j) = exp(-input(i, j)) / pow(1 + exp(-input(i, j)), 2);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    return output;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Tanh\"><a href=\"#Tanh\" class=\"headerlink\" title=\"Tanh\"></a>Tanh</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Matrix ActivationFunction::tanhActivation(Matrix input) &#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class=\"line\">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class=\"line\">            output(i, j) = (exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j)));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    return output;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix ActivationFunction::tanhDerivative(Matrix input) &#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class=\"line\">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class=\"line\">            output(i, j) = 1 - pow((exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j))), 2);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    return output;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>Now we’ve created the <code>Matrix</code>, we should build the layer that contains the neurons. But before that we need the activation functions. I’m gonna create a class that contains some static activation functions and their derivative forms (relu, sigmoid, tanh and softmax). These activation functions will take the input matrices and output the  matrices after applying the corresponding functions.</p>","more":"<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Matrix.h&quot;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;cmath&gt;</span>  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//Let&#x27;s put everything to an enum for better organization</span></span><br><span class=\"line\"><span class=\"keyword\">enum</span> <span class=\"title class_\">ActivationFunctionType</span> &#123;  </span><br><span class=\"line\">    RELU,  </span><br><span class=\"line\">    SIGMOID,  </span><br><span class=\"line\">    TANH,  </span><br><span class=\"line\">    SOFTMAX,  </span><br><span class=\"line\">    NONE  </span><br><span class=\"line\">&#125;;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ActivationFunction</span> &#123;  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">\t<span class=\"comment\">//The activation and derivative methods will serve as proxies to call the required function types</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">activation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">derivative</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">private</span>:  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">reluActivation</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">reluDerivative</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">sigmoidActivation</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">sigmoidDerivative</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">tanhActivation</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">tanhDerivative</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">softmaxActivation</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">softmaxDerivative</span><span class=\"params\">(Matrix input)</span></span>;  </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;ActivationFunction.h&quot;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::activation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span> </span>&#123;  </span><br><span class=\"line\"><span class=\"comment\">//    std::cout &lt;&lt; &quot;ActivationFunction::activation&quot; &lt;&lt; std::endl;  </span></span><br><span class=\"line\">    <span class=\"keyword\">switch</span> (activationFunctionType) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> RELU:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">reluActivation</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> SIGMOID:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">sigmoidActivation</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> TANH:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">tanhActivation</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> SOFTMAX:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">softmaxActivation</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">default</span>:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> input;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::derivative</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; input, ActivationFunctionType activationFunctionType)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">switch</span> (activationFunctionType) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> RELU:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">reluDerivative</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> SIGMOID:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">sigmoidDerivative</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> TANH:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">tanhDerivative</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">case</span> SOFTMAX:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">softmaxDerivative</span>(input);  </span><br><span class=\"line\">        <span class=\"keyword\">default</span>:  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> input;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Relu\"><a href=\"#Relu\" class=\"headerlink\" title=\"Relu\"></a>Relu</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::reluActivation</span><span class=\"params\">(Matrix input)</span> </span>&#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">input</span>(i, j) &lt; <span class=\"number\">0</span>) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">output</span>(i, j) = <span class=\"number\">0</span>;  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> output;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::reluDerivative</span><span class=\"params\">(Matrix input)</span> </span>&#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">input</span>(i, j) &lt; <span class=\"number\">0</span>) &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">output</span>(i, j) = <span class=\"number\">0</span>;  </span><br><span class=\"line\">            &#125; <span class=\"keyword\">else</span> &#123;  </span><br><span class=\"line\">                <span class=\"built_in\">output</span>(i, j) = <span class=\"number\">1</span>;  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> output;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Softmax\"><a href=\"#Softmax\" class=\"headerlink\" title=\"Softmax\"></a>Softmax</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::softmaxActivation</span><span class=\"params\">(Matrix input)</span> </span>&#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    <span class=\"type\">double</span> sum = <span class=\"number\">0</span>;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            sum += <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">output</span>(i, j) = <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j)) / sum;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> output;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">ActivationFunction::softmaxDerivative</span><span class=\"params\">(Matrix input)</span> </span>&#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    <span class=\"type\">double</span> sum = <span class=\"number\">0</span>;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            sum += <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; input.<span class=\"built_in\">rows</span>(); i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; input.<span class=\"built_in\">cols</span>(); j++) &#123;  </span><br><span class=\"line\">            <span class=\"built_in\">output</span>(i, j) = <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j)) / sum * (<span class=\"number\">1</span> - <span class=\"built_in\">exp</span>(<span class=\"built_in\">input</span>(i, j)) / sum);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> output;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Sigmoid\"><a href=\"#Sigmoid\" class=\"headerlink\" title=\"Sigmoid\"></a>Sigmoid</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Matrix ActivationFunction::sigmoidActivation(Matrix input) &#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class=\"line\">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class=\"line\">            output(i, j) = 1 / (1 + exp(-input(i, j)));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    return output;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix ActivationFunction::sigmoidDerivative(Matrix input) &#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class=\"line\">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class=\"line\">            output(i, j) = exp(-input(i, j)) / pow(1 + exp(-input(i, j)), 2);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    return output;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Tanh\"><a href=\"#Tanh\" class=\"headerlink\" title=\"Tanh\"></a>Tanh</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Matrix ActivationFunction::tanhActivation(Matrix input) &#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class=\"line\">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class=\"line\">            output(i, j) = (exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j)));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    return output;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Matrix ActivationFunction::tanhDerivative(Matrix input) &#123;  </span><br><span class=\"line\">    Matrix output = input;  </span><br><span class=\"line\">    for (int i = 0; i &lt; input.rows(); i++) &#123;  </span><br><span class=\"line\">        for (int j = 0; j &lt; input.cols(); j++) &#123;  </span><br><span class=\"line\">            output(i, j) = 1 - pow((exp(input(i, j)) - exp(-input(i, j))) / (exp(input(i, j)) + exp(-input(i, j))), 2);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    return output;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Very Naive Neural Network in C++ Part 3 Layers","date":"2021-01-06T03:15:34.000Z","_content":"Okay, now we have matrices and the activation functions, it's time to build up the layers.\nFor simplicity and purpose of the project, I will not gives the layers more abstraction by making it an interface and builds the subsequent layer above that. For now, I will put activation and non-activation layers together.\n\n<!-- more -->\n\n```cpp\n#include \"../Math/ActivationFunction.h\"  \n#include <random>  \n#include <memory>  \n  \nclass Layer {  \npublic:  \n    // Constructor for input layer, which has no weights and biases  \n    explicit Layer(int inputSize, int outputSize);  \n    explicit Layer(int inputSize, int outputSize, Matrix weights, Matrix biases);  \n    explicit Layer(ActivationFunctionType activationFunctionType);  \n\n\t//Forward propagation\n    Matrix forwardPropagation(const Matrix& inputData);  \n  \n    // Backward propagation  \n    Matrix backwardPropagation(const Matrix& error, double learningRate);  \n  \n    // Getters  \n    Matrix getWeights();  \n    Matrix getBiases();  \n    Matrix getInput();  \n  \n    int getInputSize() const;  \n    int getOutputSize() const;  \n  \n    bool isActivationLayer() const;  \n    void setActivationType(ActivationFunctionType activationType);  \n  \n  \nprivate:  \n    Matrix weights;  \n    Matrix biases;  \n    ActivationFunctionType activationFunctionType;  \n    Matrix input;  \n    int inputSize;  \n    int outputSize;  \n  \n    ActivationFunctionType getActivationType(ActivationFunctionType activationType);  \n};\n```\n# Initialization of weights and bias\nSo here we will generate the weights and bias randomly by a normal distribution\nThe initial values are actually crucial to our naive design\n\n```cpp\n#include \"Layer.h\"  \nLayer::Layer(ActivationFunctionType activationFunctionType) : weights(Matrix::EmptyMatrix()),  \n                                                              biases(Matrix::EmptyMatrix()),  \n                                                              input(Matrix::EmptyMatrix()), inputSize(0), outputSize(0),  \n                                                              activationFunctionType(activationFunctionType) {}  \n  \nLayer::Layer(int inputSize, int outputSize) : weights(inputSize, outputSize),  \n                                              biases(1, outputSize),  \n                                              input(Matrix::EmptyMatrix()),  \n                                              inputSize(inputSize),  \n                                              outputSize(outputSize),  \n                                              activationFunctionType(ActivationFunctionType::NONE) {  \n    weights = Matrix(inputSize, outputSize);  \n  \n    // Set up random number generator  \n    std::random_device rd;  \n    std::mt19937 gen(rd());  \n    std::normal_distribution<double> weights_distribution(inputSize, outputSize);  \n    std::normal_distribution<double> bias_distribution(1, outputSize);  \n  \n    // Initialize weights and biases with random values from normal distribution  \n    for (int i = 0; i < inputSize; i++) {  \n        for (int j = 0; j < outputSize; j++) {  \n            this->weights.changeAt(i, j, weights_distribution(gen)/ sqrt(inputSize+outputSize));  \n        }  \n    }  \n    for (int i = 0; i < outputSize; i++) {  \n        this->biases.changeAt(0, i, bias_distribution(gen)/ sqrt(inputSize+outputSize));  \n    }  \n    this->weights.print();  \n    this->biases.print();  \n}  \n```\n# Forward propagation\n- For non-activation layer, we will simply sum up the product of each x and weights and plus the bias. So the information will be changed and transformed into the dimension of the inputSize times the output size\n$$Y=XW+B$$\n- For activation layer, we just apply the activation function and return the tensor to next layer\n```cpp  \nMatrix Layer::forwardPropagation(const Matrix &inputData) {  \n  \n    this->input = inputData;  \n    if (this->activationFunctionType == ActivationFunctionType::NONE) {  \n        return this->biases + this->input.dot(this->weights);  \n    } else {  \n        Matrix results = ActivationFunction::activation(inputData, activationFunctionType);  \n        return results;  \n    }  \n  \n}  \n\n```\n\n# Backward propagation\nThe clear part of the neural network is to find a set of weights for each layer that minimize the error, and the most common way is to find the gradient and move the weights and biases bits by bits based on the gradients.\n\nWe have to use chain rule to get the information from the previous layer (assume has `n` nodes)\n$$\n\\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial w_{ij}} = \\sum_{k = 1}^{n} \\frac{\\partial E}{\\partial k_j} \\frac{\\partial y_k}{\\partial w_{ij}} =\\sum_{k = 1}^{n} \\frac{\\partial E}{\\partial y_j} x_{i}=X^t \\frac{\\partial E}{\\partial Y}\n$$\n\n$$\n\\frac{\\partial E}{\\partial B_i} = \\sum_{j = 1}^{n} \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial b_i} =\\sum_{j = 1}^{n} \\frac{\\partial E}{\\partial y_j} = \\frac{\\partial E}{\\partial Y}\n$$\n\n$$\n\\frac{\\partial E}{\\partial X_i} = \\sum_{j = 1}^{n} \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} =\\sum_{j = 1}^{n} \\frac{\\partial E}{\\partial y_j} w_{ij}\n$$\n\n$$\n\\frac{\\partial E}{\\partial X} = \\frac{\\partial E}{\\partial Y}W^t\n$$\n\n```cpp\n// error is dE/DY\nMatrix Layer::backwardPropagation(const Matrix &error, double learningRate) {  \n  \n    // If the layer is not the activation layer  \n    if (this->activationFunctionType == ActivationFunctionType::NONE) {  \n\t\t// dE/dX\n        Matrix previousError = error.dot(this->weights.transpose());  \n  \n        // Calculate gradient of weights and biases  \n        // dE/dW\n        Matrix gradientOfWeights = this->input.transpose().dot(error);  \n  \n        // dE/dB, just for better understanding  \n        const Matrix &gradientOfBiases = error;  \n  \n        // Update weights and biases  \n        this->weights -= gradientOfWeights * learningRate;  \n        this->biases -= gradientOfBiases * learningRate;  \n        return previousError;  \n    } else {  \n        return ActivationFunction::derivative(this->input, this->activationFunctionType) * error;  \n    }  \n  \n}  \n\n```\n# Other methods\n```cpp\nMatrix Layer::getWeights() {  \n    return this->weights;  \n}  \n  \nMatrix Layer::getBiases() {  \n    return this->biases;  \n}  \n  \nMatrix Layer::getInput() {  \n    return this->input;  \n}  \n  \nint Layer::getInputSize() const {  \n    return inputSize;  \n}  \n  \nint Layer::getOutputSize() const {  \n    return outputSize;  \n}  \n  \nbool Layer::isActivationLayer() const {  \n    return this->activationFunctionType != ActivationFunctionType::NONE;  \n}  \n  \nvoid Layer::setActivationType(ActivationFunctionType activationType) {  \n    this->activationFunctionType = activationType;  \n}  \n  \nActivationFunctionType Layer::getActivationType(ActivationFunctionType activationType) {  \n    return this->activationFunctionType;  \n}  \n  \nLayer::Layer(int inputSize, int outputSize, Matrix weights, Matrix biases):  \n        weights(weights), biases(biases), input(Matrix::EmptyMatrix()), inputSize(inputSize), outputSize(outputSize),  \n        activationFunctionType(ActivationFunctionType::NONE) {}\n```","source":"_posts/Code/Naive-Neural-Network-in-Cpp-3.md","raw":"---\ntitle: Very Naive Neural Network in C++ Part 3 Layers\ntags:\n  - c++\n  - deep-learning\n  - machine-learning\n  - neural-network\ncategories:\n  - Code\ndate: 2021-01-05 19:15:34\n---\nOkay, now we have matrices and the activation functions, it's time to build up the layers.\nFor simplicity and purpose of the project, I will not gives the layers more abstraction by making it an interface and builds the subsequent layer above that. For now, I will put activation and non-activation layers together.\n\n<!-- more -->\n\n```cpp\n#include \"../Math/ActivationFunction.h\"  \n#include <random>  \n#include <memory>  \n  \nclass Layer {  \npublic:  \n    // Constructor for input layer, which has no weights and biases  \n    explicit Layer(int inputSize, int outputSize);  \n    explicit Layer(int inputSize, int outputSize, Matrix weights, Matrix biases);  \n    explicit Layer(ActivationFunctionType activationFunctionType);  \n\n\t//Forward propagation\n    Matrix forwardPropagation(const Matrix& inputData);  \n  \n    // Backward propagation  \n    Matrix backwardPropagation(const Matrix& error, double learningRate);  \n  \n    // Getters  \n    Matrix getWeights();  \n    Matrix getBiases();  \n    Matrix getInput();  \n  \n    int getInputSize() const;  \n    int getOutputSize() const;  \n  \n    bool isActivationLayer() const;  \n    void setActivationType(ActivationFunctionType activationType);  \n  \n  \nprivate:  \n    Matrix weights;  \n    Matrix biases;  \n    ActivationFunctionType activationFunctionType;  \n    Matrix input;  \n    int inputSize;  \n    int outputSize;  \n  \n    ActivationFunctionType getActivationType(ActivationFunctionType activationType);  \n};\n```\n# Initialization of weights and bias\nSo here we will generate the weights and bias randomly by a normal distribution\nThe initial values are actually crucial to our naive design\n\n```cpp\n#include \"Layer.h\"  \nLayer::Layer(ActivationFunctionType activationFunctionType) : weights(Matrix::EmptyMatrix()),  \n                                                              biases(Matrix::EmptyMatrix()),  \n                                                              input(Matrix::EmptyMatrix()), inputSize(0), outputSize(0),  \n                                                              activationFunctionType(activationFunctionType) {}  \n  \nLayer::Layer(int inputSize, int outputSize) : weights(inputSize, outputSize),  \n                                              biases(1, outputSize),  \n                                              input(Matrix::EmptyMatrix()),  \n                                              inputSize(inputSize),  \n                                              outputSize(outputSize),  \n                                              activationFunctionType(ActivationFunctionType::NONE) {  \n    weights = Matrix(inputSize, outputSize);  \n  \n    // Set up random number generator  \n    std::random_device rd;  \n    std::mt19937 gen(rd());  \n    std::normal_distribution<double> weights_distribution(inputSize, outputSize);  \n    std::normal_distribution<double> bias_distribution(1, outputSize);  \n  \n    // Initialize weights and biases with random values from normal distribution  \n    for (int i = 0; i < inputSize; i++) {  \n        for (int j = 0; j < outputSize; j++) {  \n            this->weights.changeAt(i, j, weights_distribution(gen)/ sqrt(inputSize+outputSize));  \n        }  \n    }  \n    for (int i = 0; i < outputSize; i++) {  \n        this->biases.changeAt(0, i, bias_distribution(gen)/ sqrt(inputSize+outputSize));  \n    }  \n    this->weights.print();  \n    this->biases.print();  \n}  \n```\n# Forward propagation\n- For non-activation layer, we will simply sum up the product of each x and weights and plus the bias. So the information will be changed and transformed into the dimension of the inputSize times the output size\n$$Y=XW+B$$\n- For activation layer, we just apply the activation function and return the tensor to next layer\n```cpp  \nMatrix Layer::forwardPropagation(const Matrix &inputData) {  \n  \n    this->input = inputData;  \n    if (this->activationFunctionType == ActivationFunctionType::NONE) {  \n        return this->biases + this->input.dot(this->weights);  \n    } else {  \n        Matrix results = ActivationFunction::activation(inputData, activationFunctionType);  \n        return results;  \n    }  \n  \n}  \n\n```\n\n# Backward propagation\nThe clear part of the neural network is to find a set of weights for each layer that minimize the error, and the most common way is to find the gradient and move the weights and biases bits by bits based on the gradients.\n\nWe have to use chain rule to get the information from the previous layer (assume has `n` nodes)\n$$\n\\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial w_{ij}} = \\sum_{k = 1}^{n} \\frac{\\partial E}{\\partial k_j} \\frac{\\partial y_k}{\\partial w_{ij}} =\\sum_{k = 1}^{n} \\frac{\\partial E}{\\partial y_j} x_{i}=X^t \\frac{\\partial E}{\\partial Y}\n$$\n\n$$\n\\frac{\\partial E}{\\partial B_i} = \\sum_{j = 1}^{n} \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial b_i} =\\sum_{j = 1}^{n} \\frac{\\partial E}{\\partial y_j} = \\frac{\\partial E}{\\partial Y}\n$$\n\n$$\n\\frac{\\partial E}{\\partial X_i} = \\sum_{j = 1}^{n} \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} =\\sum_{j = 1}^{n} \\frac{\\partial E}{\\partial y_j} w_{ij}\n$$\n\n$$\n\\frac{\\partial E}{\\partial X} = \\frac{\\partial E}{\\partial Y}W^t\n$$\n\n```cpp\n// error is dE/DY\nMatrix Layer::backwardPropagation(const Matrix &error, double learningRate) {  \n  \n    // If the layer is not the activation layer  \n    if (this->activationFunctionType == ActivationFunctionType::NONE) {  \n\t\t// dE/dX\n        Matrix previousError = error.dot(this->weights.transpose());  \n  \n        // Calculate gradient of weights and biases  \n        // dE/dW\n        Matrix gradientOfWeights = this->input.transpose().dot(error);  \n  \n        // dE/dB, just for better understanding  \n        const Matrix &gradientOfBiases = error;  \n  \n        // Update weights and biases  \n        this->weights -= gradientOfWeights * learningRate;  \n        this->biases -= gradientOfBiases * learningRate;  \n        return previousError;  \n    } else {  \n        return ActivationFunction::derivative(this->input, this->activationFunctionType) * error;  \n    }  \n  \n}  \n\n```\n# Other methods\n```cpp\nMatrix Layer::getWeights() {  \n    return this->weights;  \n}  \n  \nMatrix Layer::getBiases() {  \n    return this->biases;  \n}  \n  \nMatrix Layer::getInput() {  \n    return this->input;  \n}  \n  \nint Layer::getInputSize() const {  \n    return inputSize;  \n}  \n  \nint Layer::getOutputSize() const {  \n    return outputSize;  \n}  \n  \nbool Layer::isActivationLayer() const {  \n    return this->activationFunctionType != ActivationFunctionType::NONE;  \n}  \n  \nvoid Layer::setActivationType(ActivationFunctionType activationType) {  \n    this->activationFunctionType = activationType;  \n}  \n  \nActivationFunctionType Layer::getActivationType(ActivationFunctionType activationType) {  \n    return this->activationFunctionType;  \n}  \n  \nLayer::Layer(int inputSize, int outputSize, Matrix weights, Matrix biases):  \n        weights(weights), biases(biases), input(Matrix::EmptyMatrix()), inputSize(inputSize), outputSize(outputSize),  \n        activationFunctionType(ActivationFunctionType::NONE) {}\n```","slug":"Code/Naive-Neural-Network-in-Cpp-3","published":1,"updated":"2022-12-16T23:58:48.323Z","_id":"clbr3s8ga0002fzsbc55o7yvj","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Okay, now we have matrices and the activation functions, it’s time to build up the layers.<br>For simplicity and purpose of the project, I will not gives the layers more abstraction by making it an interface and builds the subsequent layer above that. For now, I will put activation and non-activation layers together.</p>\n<span id=\"more\"></span>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;../Math/ActivationFunction.h&quot;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;random&gt;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;memory&gt;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Layer</span> &#123;  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">    <span class=\"comment\">// Constructor for input layer, which has no weights and biases  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">Layer</span><span class=\"params\">(<span class=\"type\">int</span> inputSize, <span class=\"type\">int</span> outputSize)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">Layer</span><span class=\"params\">(<span class=\"type\">int</span> inputSize, <span class=\"type\">int</span> outputSize, Matrix weights, Matrix biases)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">Layer</span><span class=\"params\">(ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">//Forward propagation</span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">forwardPropagation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; inputData)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Backward propagation  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">backwardPropagation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; error, <span class=\"type\">double</span> learningRate)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Getters  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getWeights</span><span class=\"params\">()</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getBiases</span><span class=\"params\">()</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getInput</span><span class=\"params\">()</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">getInputSize</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">getOutputSize</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">isActivationLayer</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">setActivationType</span><span class=\"params\">(ActivationFunctionType activationType)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">private</span>:  </span><br><span class=\"line\">    Matrix weights;  </span><br><span class=\"line\">    Matrix biases;  </span><br><span class=\"line\">    ActivationFunctionType activationFunctionType;  </span><br><span class=\"line\">    Matrix input;  </span><br><span class=\"line\">    <span class=\"type\">int</span> inputSize;  </span><br><span class=\"line\">    <span class=\"type\">int</span> outputSize;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\">ActivationFunctionType <span class=\"title\">getActivationType</span><span class=\"params\">(ActivationFunctionType activationType)</span></span>;  </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Initialization-of-weights-and-bias\"><a href=\"#Initialization-of-weights-and-bias\" class=\"headerlink\" title=\"Initialization of weights and bias\"></a>Initialization of weights and bias</h1><p>So here we will generate the weights and bias randomly by a normal distribution<br>The initial values are actually crucial to our naive design</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Layer.h&quot;</span>  </span></span><br><span class=\"line\">Layer::<span class=\"built_in\">Layer</span>(ActivationFunctionType activationFunctionType) : <span class=\"built_in\">weights</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()),  </span><br><span class=\"line\">                                                              <span class=\"built_in\">biases</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()),  </span><br><span class=\"line\">                                                              <span class=\"built_in\">input</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()), <span class=\"built_in\">inputSize</span>(<span class=\"number\">0</span>), <span class=\"built_in\">outputSize</span>(<span class=\"number\">0</span>),  </span><br><span class=\"line\">                                                              <span class=\"built_in\">activationFunctionType</span>(activationFunctionType) &#123;&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Layer::<span class=\"built_in\">Layer</span>(<span class=\"type\">int</span> inputSize, <span class=\"type\">int</span> outputSize) : <span class=\"built_in\">weights</span>(inputSize, outputSize),  </span><br><span class=\"line\">                                              <span class=\"built_in\">biases</span>(<span class=\"number\">1</span>, outputSize),  </span><br><span class=\"line\">                                              <span class=\"built_in\">input</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()),  </span><br><span class=\"line\">                                              <span class=\"built_in\">inputSize</span>(inputSize),  </span><br><span class=\"line\">                                              <span class=\"built_in\">outputSize</span>(outputSize),  </span><br><span class=\"line\">                                              <span class=\"built_in\">activationFunctionType</span>(ActivationFunctionType::NONE) &#123;  </span><br><span class=\"line\">    weights = <span class=\"built_in\">Matrix</span>(inputSize, outputSize);  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Set up random number generator  </span></span><br><span class=\"line\">    std::random_device rd;  </span><br><span class=\"line\">    <span class=\"function\">std::mt19937 <span class=\"title\">gen</span><span class=\"params\">(rd())</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">std::normal_distribution&lt;<span class=\"type\">double</span>&gt; <span class=\"title\">weights_distribution</span><span class=\"params\">(inputSize, outputSize)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">std::normal_distribution&lt;<span class=\"type\">double</span>&gt; <span class=\"title\">bias_distribution</span><span class=\"params\">(<span class=\"number\">1</span>, outputSize)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Initialize weights and biases with random values from normal distribution  </span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; inputSize; i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; outputSize; j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;weights.<span class=\"built_in\">changeAt</span>(i, j, <span class=\"built_in\">weights_distribution</span>(gen)/ <span class=\"built_in\">sqrt</span>(inputSize+outputSize));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; outputSize; i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;biases.<span class=\"built_in\">changeAt</span>(<span class=\"number\">0</span>, i, <span class=\"built_in\">bias_distribution</span>(gen)/ <span class=\"built_in\">sqrt</span>(inputSize+outputSize));  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;weights.<span class=\"built_in\">print</span>();  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;biases.<span class=\"built_in\">print</span>();  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Forward-propagation\"><a href=\"#Forward-propagation\" class=\"headerlink\" title=\"Forward propagation\"></a>Forward propagation</h1><ul>\n<li>For non-activation layer, we will simply sum up the product of each x and weights and plus the bias. So the information will be changed and transformed into the dimension of the inputSize times the output size<br>$$Y&#x3D;XW+B$$</li>\n<li>For activation layer, we just apply the activation function and return the tensor to next layer<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::forwardPropagation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;inputData)</span> </span>&#123;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;input = inputData;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;activationFunctionType == ActivationFunctionType::NONE) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;biases + <span class=\"keyword\">this</span>-&gt;input.<span class=\"built_in\">dot</span>(<span class=\"keyword\">this</span>-&gt;weights);  </span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;  </span><br><span class=\"line\">        Matrix results = ActivationFunction::<span class=\"built_in\">activation</span>(inputData, activationFunctionType);  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> results;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h1 id=\"Backward-propagation\"><a href=\"#Backward-propagation\" class=\"headerlink\" title=\"Backward propagation\"></a>Backward propagation</h1><p>The clear part of the neural network is to find a set of weights for each layer that minimize the error, and the most common way is to find the gradient and move the weights and biases bits by bits based on the gradients.</p>\n<p>We have to use chain rule to get the information from the previous layer (assume has <code>n</code> nodes)<br>$$<br>\\frac{\\partial E}{\\partial W} &#x3D; \\frac{\\partial E}{\\partial w_{ij}} &#x3D; \\sum_{k &#x3D; 1}^{n} \\frac{\\partial E}{\\partial k_j} \\frac{\\partial y_k}{\\partial w_{ij}} &#x3D;\\sum_{k &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} x_{i}&#x3D;X^t \\frac{\\partial E}{\\partial Y}<br>$$</p>\n<p>$$<br>\\frac{\\partial E}{\\partial B_i} &#x3D; \\sum_{j &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial b_i} &#x3D;\\sum_{j &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} &#x3D; \\frac{\\partial E}{\\partial Y}<br>$$</p>\n<p>$$<br>\\frac{\\partial E}{\\partial X_i} &#x3D; \\sum_{j &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} &#x3D;\\sum_{j &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} w_{ij}<br>$$</p>\n<p>$$<br>\\frac{\\partial E}{\\partial X} &#x3D; \\frac{\\partial E}{\\partial Y}W^t<br>$$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// error is dE/DY</span></span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::backwardPropagation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;error, <span class=\"type\">double</span> learningRate)</span> </span>&#123;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// If the layer is not the activation layer  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;activationFunctionType == ActivationFunctionType::NONE) &#123;  </span><br><span class=\"line\">\t\t<span class=\"comment\">// dE/dX</span></span><br><span class=\"line\">        Matrix previousError = error.<span class=\"built_in\">dot</span>(<span class=\"keyword\">this</span>-&gt;weights.<span class=\"built_in\">transpose</span>());  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\">// Calculate gradient of weights and biases  </span></span><br><span class=\"line\">        <span class=\"comment\">// dE/dW</span></span><br><span class=\"line\">        Matrix gradientOfWeights = <span class=\"keyword\">this</span>-&gt;input.<span class=\"built_in\">transpose</span>().<span class=\"built_in\">dot</span>(error);  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\">// dE/dB, just for better understanding  </span></span><br><span class=\"line\">        <span class=\"type\">const</span> Matrix &amp;gradientOfBiases = error;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\">// Update weights and biases  </span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;weights -= gradientOfWeights * learningRate;  </span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;biases -= gradientOfBiases * learningRate;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> previousError;  </span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">derivative</span>(<span class=\"keyword\">this</span>-&gt;input, <span class=\"keyword\">this</span>-&gt;activationFunctionType) * error;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::getWeights</span><span class=\"params\">()</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;weights;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::getBiases</span><span class=\"params\">()</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;biases;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::getInput</span><span class=\"params\">()</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;input;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Layer::getInputSize</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> inputSize;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Layer::getOutputSize</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> outputSize;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">Layer::isActivationLayer</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;activationFunctionType != ActivationFunctionType::NONE;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Layer::setActivationType</span><span class=\"params\">(ActivationFunctionType activationType)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;activationFunctionType = activationType;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">ActivationFunctionType <span class=\"title\">Layer::getActivationType</span><span class=\"params\">(ActivationFunctionType activationType)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;activationFunctionType;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Layer::<span class=\"built_in\">Layer</span>(<span class=\"type\">int</span> inputSize, <span class=\"type\">int</span> outputSize, Matrix weights, Matrix biases):  </span><br><span class=\"line\">        <span class=\"built_in\">weights</span>(weights), <span class=\"built_in\">biases</span>(biases), <span class=\"built_in\">input</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()), <span class=\"built_in\">inputSize</span>(inputSize), <span class=\"built_in\">outputSize</span>(outputSize),  </span><br><span class=\"line\">        <span class=\"built_in\">activationFunctionType</span>(ActivationFunctionType::NONE) &#123;&#125;</span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"<p>Okay, now we have matrices and the activation functions, it’s time to build up the layers.<br>For simplicity and purpose of the project, I will not gives the layers more abstraction by making it an interface and builds the subsequent layer above that. For now, I will put activation and non-activation layers together.</p>","more":"<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;../Math/ActivationFunction.h&quot;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;random&gt;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;memory&gt;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Layer</span> &#123;  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">    <span class=\"comment\">// Constructor for input layer, which has no weights and biases  </span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">Layer</span><span class=\"params\">(<span class=\"type\">int</span> inputSize, <span class=\"type\">int</span> outputSize)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">Layer</span><span class=\"params\">(<span class=\"type\">int</span> inputSize, <span class=\"type\">int</span> outputSize, Matrix weights, Matrix biases)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">explicit</span> <span class=\"title\">Layer</span><span class=\"params\">(ActivationFunctionType activationFunctionType)</span></span>;  </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"comment\">//Forward propagation</span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">forwardPropagation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; inputData)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Backward propagation  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">backwardPropagation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; error, <span class=\"type\">double</span> learningRate)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Getters  </span></span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getWeights</span><span class=\"params\">()</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getBiases</span><span class=\"params\">()</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">getInput</span><span class=\"params\">()</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">getInputSize</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">getOutputSize</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">isActivationLayer</span><span class=\"params\">()</span> <span class=\"type\">const</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">setActivationType</span><span class=\"params\">(ActivationFunctionType activationType)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">private</span>:  </span><br><span class=\"line\">    Matrix weights;  </span><br><span class=\"line\">    Matrix biases;  </span><br><span class=\"line\">    ActivationFunctionType activationFunctionType;  </span><br><span class=\"line\">    Matrix input;  </span><br><span class=\"line\">    <span class=\"type\">int</span> inputSize;  </span><br><span class=\"line\">    <span class=\"type\">int</span> outputSize;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\">ActivationFunctionType <span class=\"title\">getActivationType</span><span class=\"params\">(ActivationFunctionType activationType)</span></span>;  </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Initialization-of-weights-and-bias\"><a href=\"#Initialization-of-weights-and-bias\" class=\"headerlink\" title=\"Initialization of weights and bias\"></a>Initialization of weights and bias</h1><p>So here we will generate the weights and bias randomly by a normal distribution<br>The initial values are actually crucial to our naive design</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Layer.h&quot;</span>  </span></span><br><span class=\"line\">Layer::<span class=\"built_in\">Layer</span>(ActivationFunctionType activationFunctionType) : <span class=\"built_in\">weights</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()),  </span><br><span class=\"line\">                                                              <span class=\"built_in\">biases</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()),  </span><br><span class=\"line\">                                                              <span class=\"built_in\">input</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()), <span class=\"built_in\">inputSize</span>(<span class=\"number\">0</span>), <span class=\"built_in\">outputSize</span>(<span class=\"number\">0</span>),  </span><br><span class=\"line\">                                                              <span class=\"built_in\">activationFunctionType</span>(activationFunctionType) &#123;&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Layer::<span class=\"built_in\">Layer</span>(<span class=\"type\">int</span> inputSize, <span class=\"type\">int</span> outputSize) : <span class=\"built_in\">weights</span>(inputSize, outputSize),  </span><br><span class=\"line\">                                              <span class=\"built_in\">biases</span>(<span class=\"number\">1</span>, outputSize),  </span><br><span class=\"line\">                                              <span class=\"built_in\">input</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()),  </span><br><span class=\"line\">                                              <span class=\"built_in\">inputSize</span>(inputSize),  </span><br><span class=\"line\">                                              <span class=\"built_in\">outputSize</span>(outputSize),  </span><br><span class=\"line\">                                              <span class=\"built_in\">activationFunctionType</span>(ActivationFunctionType::NONE) &#123;  </span><br><span class=\"line\">    weights = <span class=\"built_in\">Matrix</span>(inputSize, outputSize);  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Set up random number generator  </span></span><br><span class=\"line\">    std::random_device rd;  </span><br><span class=\"line\">    <span class=\"function\">std::mt19937 <span class=\"title\">gen</span><span class=\"params\">(rd())</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">std::normal_distribution&lt;<span class=\"type\">double</span>&gt; <span class=\"title\">weights_distribution</span><span class=\"params\">(inputSize, outputSize)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">std::normal_distribution&lt;<span class=\"type\">double</span>&gt; <span class=\"title\">bias_distribution</span><span class=\"params\">(<span class=\"number\">1</span>, outputSize)</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// Initialize weights and biases with random values from normal distribution  </span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; inputSize; i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; outputSize; j++) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">this</span>-&gt;weights.<span class=\"built_in\">changeAt</span>(i, j, <span class=\"built_in\">weights_distribution</span>(gen)/ <span class=\"built_in\">sqrt</span>(inputSize+outputSize));  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; outputSize; i++) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;biases.<span class=\"built_in\">changeAt</span>(<span class=\"number\">0</span>, i, <span class=\"built_in\">bias_distribution</span>(gen)/ <span class=\"built_in\">sqrt</span>(inputSize+outputSize));  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;weights.<span class=\"built_in\">print</span>();  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;biases.<span class=\"built_in\">print</span>();  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Forward-propagation\"><a href=\"#Forward-propagation\" class=\"headerlink\" title=\"Forward propagation\"></a>Forward propagation</h1><ul>\n<li>For non-activation layer, we will simply sum up the product of each x and weights and plus the bias. So the information will be changed and transformed into the dimension of the inputSize times the output size<br>$$Y&#x3D;XW+B$$</li>\n<li>For activation layer, we just apply the activation function and return the tensor to next layer<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::forwardPropagation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;inputData)</span> </span>&#123;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;input = inputData;  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;activationFunctionType == ActivationFunctionType::NONE) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;biases + <span class=\"keyword\">this</span>-&gt;input.<span class=\"built_in\">dot</span>(<span class=\"keyword\">this</span>-&gt;weights);  </span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;  </span><br><span class=\"line\">        Matrix results = ActivationFunction::<span class=\"built_in\">activation</span>(inputData, activationFunctionType);  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> results;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h1 id=\"Backward-propagation\"><a href=\"#Backward-propagation\" class=\"headerlink\" title=\"Backward propagation\"></a>Backward propagation</h1><p>The clear part of the neural network is to find a set of weights for each layer that minimize the error, and the most common way is to find the gradient and move the weights and biases bits by bits based on the gradients.</p>\n<p>We have to use chain rule to get the information from the previous layer (assume has <code>n</code> nodes)<br>$$<br>\\frac{\\partial E}{\\partial W} &#x3D; \\frac{\\partial E}{\\partial w_{ij}} &#x3D; \\sum_{k &#x3D; 1}^{n} \\frac{\\partial E}{\\partial k_j} \\frac{\\partial y_k}{\\partial w_{ij}} &#x3D;\\sum_{k &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} x_{i}&#x3D;X^t \\frac{\\partial E}{\\partial Y}<br>$$</p>\n<p>$$<br>\\frac{\\partial E}{\\partial B_i} &#x3D; \\sum_{j &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial b_i} &#x3D;\\sum_{j &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} &#x3D; \\frac{\\partial E}{\\partial Y}<br>$$</p>\n<p>$$<br>\\frac{\\partial E}{\\partial X_i} &#x3D; \\sum_{j &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} &#x3D;\\sum_{j &#x3D; 1}^{n} \\frac{\\partial E}{\\partial y_j} w_{ij}<br>$$</p>\n<p>$$<br>\\frac{\\partial E}{\\partial X} &#x3D; \\frac{\\partial E}{\\partial Y}W^t<br>$$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// error is dE/DY</span></span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::backwardPropagation</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;error, <span class=\"type\">double</span> learningRate)</span> </span>&#123;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"comment\">// If the layer is not the activation layer  </span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>-&gt;activationFunctionType == ActivationFunctionType::NONE) &#123;  </span><br><span class=\"line\">\t\t<span class=\"comment\">// dE/dX</span></span><br><span class=\"line\">        Matrix previousError = error.<span class=\"built_in\">dot</span>(<span class=\"keyword\">this</span>-&gt;weights.<span class=\"built_in\">transpose</span>());  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\">// Calculate gradient of weights and biases  </span></span><br><span class=\"line\">        <span class=\"comment\">// dE/dW</span></span><br><span class=\"line\">        Matrix gradientOfWeights = <span class=\"keyword\">this</span>-&gt;input.<span class=\"built_in\">transpose</span>().<span class=\"built_in\">dot</span>(error);  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\">// dE/dB, just for better understanding  </span></span><br><span class=\"line\">        <span class=\"type\">const</span> Matrix &amp;gradientOfBiases = error;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"comment\">// Update weights and biases  </span></span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;weights -= gradientOfWeights * learningRate;  </span><br><span class=\"line\">        <span class=\"keyword\">this</span>-&gt;biases -= gradientOfBiases * learningRate;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> previousError;  </span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">return</span> ActivationFunction::<span class=\"built_in\">derivative</span>(<span class=\"keyword\">this</span>-&gt;input, <span class=\"keyword\">this</span>-&gt;activationFunctionType) * error;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h1 id=\"Other-methods\"><a href=\"#Other-methods\" class=\"headerlink\" title=\"Other methods\"></a>Other methods</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::getWeights</span><span class=\"params\">()</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;weights;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::getBiases</span><span class=\"params\">()</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;biases;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Layer::getInput</span><span class=\"params\">()</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;input;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Layer::getInputSize</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> inputSize;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Layer::getOutputSize</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> outputSize;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">Layer::isActivationLayer</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;activationFunctionType != ActivationFunctionType::NONE;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Layer::setActivationType</span><span class=\"params\">(ActivationFunctionType activationType)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">this</span>-&gt;activationFunctionType = activationType;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">ActivationFunctionType <span class=\"title\">Layer::getActivationType</span><span class=\"params\">(ActivationFunctionType activationType)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"keyword\">this</span>-&gt;activationFunctionType;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">Layer::<span class=\"built_in\">Layer</span>(<span class=\"type\">int</span> inputSize, <span class=\"type\">int</span> outputSize, Matrix weights, Matrix biases):  </span><br><span class=\"line\">        <span class=\"built_in\">weights</span>(weights), <span class=\"built_in\">biases</span>(biases), <span class=\"built_in\">input</span>(Matrix::<span class=\"built_in\">EmptyMatrix</span>()), <span class=\"built_in\">inputSize</span>(inputSize), <span class=\"built_in\">outputSize</span>(outputSize),  </span><br><span class=\"line\">        <span class=\"built_in\">activationFunctionType</span>(ActivationFunctionType::NONE) &#123;&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Very Naive Neural Network in C++ Part 4 Network","date":"2021-01-06T03:32:23.000Z","_content":"Ok this is the final bit. We need the network to wrap things up and do the training\n\nBut again, we need some loss functions to determine the errors we gonna put into back propagation. I will use a simple MSE here. it's simply the sum of square of the difference between predicted value and training value\n$$MSE = \\frac{\\sum^n_{i=1} (Y_i - \\hat Y_i)^2}{n}$$\n\nAnd I guess you can directly see the derivative from here\n<!-- more -->\n\n```cpp\n#include \"Matrix.h\"  \nclass Loss {  \npublic:  \n  \n    static double meanSquaredError(const Matrix &predicted, const Matrix &actual);  \n    static Matrix meanSquaredErrorDerivative(const Matrix &predicted, const Matrix &actual);  \n};\n```\n\n```cpp\n#include \"Loss.h\"  \n  \ndouble Loss::meanSquaredError(const Matrix& predicted, const Matrix& actual) {  \n    return ((predicted - actual) * (predicted - actual)).mean();  \n}  \n  \nMatrix Loss::meanSquaredErrorDerivative(const Matrix& predicted, const Matrix& actual) {  \n//    std::cout<< \"Predicted: \" << predicted << std::endl;  \n//    std::cout<< \"Actual: \" << actual << std::endl;  \n//    std::cout<< \"Actual size\" << actual.size() << std::endl;  \n    return (predicted - actual)*2/actual.size();  \n}\n```\n\n# Network\n\nNow finally, the ending bits. \n- The network is gonna take layers and stack them in order\n- The network is gonna fit(train), aka change the weights and bias, based on input data\n- and it's need to predict\n\n```cpp\n#include <vector>  \n#include \"../Layers/Layer.h\"  \n#include \"../Math/Loss.h\"  \n  \nclass Network {  \npublic:  \n    Network(): layers(std::vector<Layer>()) {};  \n    ~Network();  \n  \n    void addLayer(const Layer& layer);  \n    void fit(const Matrix &xTrain, const Matrix &yTrain, int numberOfEpoch, double learningRate);  \n    Matrix predict(const Matrix& xTest);  \n    int getOutputSize();  \n  \nprivate:  \n    std::vector<Layer> layers;  \n  \n};\n```\n\n# Add layers\n```cpp\n#include \"Network.h\"  \n  \nNetwork::~Network() = default;\n  \nvoid Network::addLayer(const Layer &layer) {  \n    layers.push_back(layer);  \n}  \n```\n\nThe fit method is gonna\n- take input consisting of actual value (Y) and its label (X)\n- the training time, epoch number\n- the learning rate, how much of the gradient we apply to the weights and bias\n\n\n\n```cpp\nvoid Network::fit(const Matrix &xTrain, const Matrix &yTrain, int numberOfEpoch, double learningRate) {  \n    // Check if the number of training data matches the number of labels  \n    int trainingSize = xTrain.rows();  \n  \n    if (trainingSize != yTrain.rows()) {  \n        throw std::invalid_argument(\"The number of training data does not match the number of labels\");  \n    }  \n  \n    for (int epoch = 0; epoch < numberOfEpoch; epoch++) {  \n        double mse = 0;  \n        for (int i = 0; i < trainingSize; ++i) {  \n            Matrix output = xTrain.getRow(i);  \n            for (auto &layer: layers) {  \n                output = layer.forwardPropagation(output);  \n            }  \n\t\t\t// We actually don't need this for computation, but we can print it to see if the training is doing properly\n            mse += Loss::meanSquaredError(output, yTrain.getRow(i));  \n\t\n\t\t\t// The dE/dY\n            Matrix err = Loss::meanSquaredErrorDerivative(output, yTrain.getRow(i));  \n\n\t\t\t//remeber it's back not forward, so we do it in reverse\n            for (auto it = layers.rbegin(); it != layers.rend(); ++it) {  \n                err = it->backwardPropagation(err, learningRate);  \n            }  \n        }  \n\t\t\n        mse /= trainingSize;  \n        std::cout << \"Epoch \" << epoch << \" MSE: \" << mse << std::endl;  \n    }  \n}  \n```\n# Predict\nPrediction is rather easy, just propagate through all the layer and you will have the answer.\n\n```cpp\nMatrix Network::predict(const Matrix& xTest) {  \n    int testingSize = xTest.rows();  \n    Matrix result(0, getOutputSize());  \n    for (int i = 0; i < testingSize; ++i) {  \n        Matrix output = xTest.getRow(i);  \n  \n        for (auto &layer: layers) {  \n            output = layer.forwardPropagation(output);  \n        }  \n  \n        result.addRow(output);  \n    }  \n    return result;  \n}  \n  \nint Network::getOutputSize() {  \n    // check if the layer is not an activation layer reversely  \n    for (auto it = layers.rbegin(); it != layers.rend(); ++it) {  \n        if (!it->isActivationLayer()) {  \n            return it->getOutputSize();  \n        }  \n    }  \n}\n```\n\n# Test the code\n```cpp\n#include <iostream>  \n#include \"Networks/Network.h\"\n\nint main() {\n\t//XOR gate\n\tauto xTrain = Matrix({{0,0}, {0,1}, {1,0}, {1,1}});  \n\tauto yTrain = Matrix({{0}, {1}, {1}, {0}});  \n\n\tNetwork network = Network();  \n\tnetwork.addLayer(Layer(2, 3));  \n\tnetwork.addLayer(Layer(ActivationFunctionType::TANH));  \n\tnetwork.addLayer(Layer(3,1));  \n\tnetwork.addLayer(Layer(ActivationFunctionType::TANH));  \n\tnetwork.fit(xTrain, yTrain, 100, 0.1);  \n\n\tauto result = network.predict(xTrain);  \n\tstd::cout << result << std::endl;\n\treturn 0\n}\n```\n\nThe output\n```\nEpoch 1 MSE: 0.48139\n...\n...\n...\nEpoch 99 MSE: 0.0133901\n0.0314563, \n0.886822, \n0.819525, \n-0.00120216, \n```\n\nHere we go, we get the fairly accurate output with a small epoch number. \n\nAnd we can also use the model on 2D images like the famous mnist dataset. The idea is convert the 2d image into a 1D array and create the initial activation layer with input size equals to the image size. However, it requires some image processing code which we will be doing in another day.","source":"_posts/Code/Naive-Neural-Network-in-Cpp-4 Network.md","raw":"---\ntitle: Very Naive Neural Network in C++ Part 4 Network\ntags:\n  - c++\n  - deep-learning\n  - machine-learning\n  - neural-network\ncategories:\n  - Code\ndate: 2021-01-05 19:32:23\n---\nOk this is the final bit. We need the network to wrap things up and do the training\n\nBut again, we need some loss functions to determine the errors we gonna put into back propagation. I will use a simple MSE here. it's simply the sum of square of the difference between predicted value and training value\n$$MSE = \\frac{\\sum^n_{i=1} (Y_i - \\hat Y_i)^2}{n}$$\n\nAnd I guess you can directly see the derivative from here\n<!-- more -->\n\n```cpp\n#include \"Matrix.h\"  \nclass Loss {  \npublic:  \n  \n    static double meanSquaredError(const Matrix &predicted, const Matrix &actual);  \n    static Matrix meanSquaredErrorDerivative(const Matrix &predicted, const Matrix &actual);  \n};\n```\n\n```cpp\n#include \"Loss.h\"  \n  \ndouble Loss::meanSquaredError(const Matrix& predicted, const Matrix& actual) {  \n    return ((predicted - actual) * (predicted - actual)).mean();  \n}  \n  \nMatrix Loss::meanSquaredErrorDerivative(const Matrix& predicted, const Matrix& actual) {  \n//    std::cout<< \"Predicted: \" << predicted << std::endl;  \n//    std::cout<< \"Actual: \" << actual << std::endl;  \n//    std::cout<< \"Actual size\" << actual.size() << std::endl;  \n    return (predicted - actual)*2/actual.size();  \n}\n```\n\n# Network\n\nNow finally, the ending bits. \n- The network is gonna take layers and stack them in order\n- The network is gonna fit(train), aka change the weights and bias, based on input data\n- and it's need to predict\n\n```cpp\n#include <vector>  \n#include \"../Layers/Layer.h\"  \n#include \"../Math/Loss.h\"  \n  \nclass Network {  \npublic:  \n    Network(): layers(std::vector<Layer>()) {};  \n    ~Network();  \n  \n    void addLayer(const Layer& layer);  \n    void fit(const Matrix &xTrain, const Matrix &yTrain, int numberOfEpoch, double learningRate);  \n    Matrix predict(const Matrix& xTest);  \n    int getOutputSize();  \n  \nprivate:  \n    std::vector<Layer> layers;  \n  \n};\n```\n\n# Add layers\n```cpp\n#include \"Network.h\"  \n  \nNetwork::~Network() = default;\n  \nvoid Network::addLayer(const Layer &layer) {  \n    layers.push_back(layer);  \n}  \n```\n\nThe fit method is gonna\n- take input consisting of actual value (Y) and its label (X)\n- the training time, epoch number\n- the learning rate, how much of the gradient we apply to the weights and bias\n\n\n\n```cpp\nvoid Network::fit(const Matrix &xTrain, const Matrix &yTrain, int numberOfEpoch, double learningRate) {  \n    // Check if the number of training data matches the number of labels  \n    int trainingSize = xTrain.rows();  \n  \n    if (trainingSize != yTrain.rows()) {  \n        throw std::invalid_argument(\"The number of training data does not match the number of labels\");  \n    }  \n  \n    for (int epoch = 0; epoch < numberOfEpoch; epoch++) {  \n        double mse = 0;  \n        for (int i = 0; i < trainingSize; ++i) {  \n            Matrix output = xTrain.getRow(i);  \n            for (auto &layer: layers) {  \n                output = layer.forwardPropagation(output);  \n            }  \n\t\t\t// We actually don't need this for computation, but we can print it to see if the training is doing properly\n            mse += Loss::meanSquaredError(output, yTrain.getRow(i));  \n\t\n\t\t\t// The dE/dY\n            Matrix err = Loss::meanSquaredErrorDerivative(output, yTrain.getRow(i));  \n\n\t\t\t//remeber it's back not forward, so we do it in reverse\n            for (auto it = layers.rbegin(); it != layers.rend(); ++it) {  \n                err = it->backwardPropagation(err, learningRate);  \n            }  \n        }  \n\t\t\n        mse /= trainingSize;  \n        std::cout << \"Epoch \" << epoch << \" MSE: \" << mse << std::endl;  \n    }  \n}  \n```\n# Predict\nPrediction is rather easy, just propagate through all the layer and you will have the answer.\n\n```cpp\nMatrix Network::predict(const Matrix& xTest) {  \n    int testingSize = xTest.rows();  \n    Matrix result(0, getOutputSize());  \n    for (int i = 0; i < testingSize; ++i) {  \n        Matrix output = xTest.getRow(i);  \n  \n        for (auto &layer: layers) {  \n            output = layer.forwardPropagation(output);  \n        }  \n  \n        result.addRow(output);  \n    }  \n    return result;  \n}  \n  \nint Network::getOutputSize() {  \n    // check if the layer is not an activation layer reversely  \n    for (auto it = layers.rbegin(); it != layers.rend(); ++it) {  \n        if (!it->isActivationLayer()) {  \n            return it->getOutputSize();  \n        }  \n    }  \n}\n```\n\n# Test the code\n```cpp\n#include <iostream>  \n#include \"Networks/Network.h\"\n\nint main() {\n\t//XOR gate\n\tauto xTrain = Matrix({{0,0}, {0,1}, {1,0}, {1,1}});  \n\tauto yTrain = Matrix({{0}, {1}, {1}, {0}});  \n\n\tNetwork network = Network();  \n\tnetwork.addLayer(Layer(2, 3));  \n\tnetwork.addLayer(Layer(ActivationFunctionType::TANH));  \n\tnetwork.addLayer(Layer(3,1));  \n\tnetwork.addLayer(Layer(ActivationFunctionType::TANH));  \n\tnetwork.fit(xTrain, yTrain, 100, 0.1);  \n\n\tauto result = network.predict(xTrain);  \n\tstd::cout << result << std::endl;\n\treturn 0\n}\n```\n\nThe output\n```\nEpoch 1 MSE: 0.48139\n...\n...\n...\nEpoch 99 MSE: 0.0133901\n0.0314563, \n0.886822, \n0.819525, \n-0.00120216, \n```\n\nHere we go, we get the fairly accurate output with a small epoch number. \n\nAnd we can also use the model on 2D images like the famous mnist dataset. The idea is convert the 2d image into a 1D array and create the initial activation layer with input size equals to the image size. However, it requires some image processing code which we will be doing in another day.","slug":"Code/Naive-Neural-Network-in-Cpp-4 Network","published":1,"updated":"2022-12-16T23:58:17.462Z","_id":"clbr5wgj40000aosb5ltu6ydw","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Ok this is the final bit. We need the network to wrap things up and do the training</p>\n<p>But again, we need some loss functions to determine the errors we gonna put into back propagation. I will use a simple MSE here. it’s simply the sum of square of the difference between predicted value and training value<br>$$MSE &#x3D; \\frac{\\sum^n_{i&#x3D;1} (Y_i - \\hat Y_i)^2}{n}$$</p>\n<p>And I guess you can directly see the derivative from here</p>\n<span id=\"more\"></span>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Matrix.h&quot;</span>  </span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Loss</span> &#123;  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> <span class=\"type\">double</span> <span class=\"title\">meanSquaredError</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;predicted, <span class=\"type\">const</span> Matrix &amp;actual)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">meanSquaredErrorDerivative</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;predicted, <span class=\"type\">const</span> Matrix &amp;actual)</span></span>;  </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Loss.h&quot;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">Loss::meanSquaredError</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; predicted, <span class=\"type\">const</span> Matrix&amp; actual)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> ((predicted - actual) * (predicted - actual)).<span class=\"built_in\">mean</span>();  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Loss::meanSquaredErrorDerivative</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; predicted, <span class=\"type\">const</span> Matrix&amp; actual)</span> </span>&#123;  </span><br><span class=\"line\"><span class=\"comment\">//    std::cout&lt;&lt; &quot;Predicted: &quot; &lt;&lt; predicted &lt;&lt; std::endl;  </span></span><br><span class=\"line\"><span class=\"comment\">//    std::cout&lt;&lt; &quot;Actual: &quot; &lt;&lt; actual &lt;&lt; std::endl;  </span></span><br><span class=\"line\"><span class=\"comment\">//    std::cout&lt;&lt; &quot;Actual size&quot; &lt;&lt; actual.size() &lt;&lt; std::endl;  </span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (predicted - actual)*<span class=\"number\">2</span>/actual.<span class=\"built_in\">size</span>();  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Network\"><a href=\"#Network\" class=\"headerlink\" title=\"Network\"></a>Network</h1><p>Now finally, the ending bits. </p>\n<ul>\n<li>The network is gonna take layers and stack them in order</li>\n<li>The network is gonna fit(train), aka change the weights and bias, based on input data</li>\n<li>and it’s need to predict</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;vector&gt;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;../Layers/Layer.h&quot;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;../Math/Loss.h&quot;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span> &#123;  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">    <span class=\"built_in\">Network</span>(): <span class=\"built_in\">layers</span>(std::<span class=\"built_in\">vector</span>&lt;Layer&gt;()) &#123;&#125;;  </span><br><span class=\"line\">    ~<span class=\"built_in\">Network</span>();  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">addLayer</span><span class=\"params\">(<span class=\"type\">const</span> Layer&amp; layer)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">fit</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;xTrain, <span class=\"type\">const</span> Matrix &amp;yTrain, <span class=\"type\">int</span> numberOfEpoch, <span class=\"type\">double</span> learningRate)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">predict</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; xTest)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">getOutputSize</span><span class=\"params\">()</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">private</span>:  </span><br><span class=\"line\">    std::vector&lt;Layer&gt; layers;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Add-layers\"><a href=\"#Add-layers\" class=\"headerlink\" title=\"Add layers\"></a>Add layers</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Network.h&quot;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\">Network::~<span class=\"built_in\">Network</span>() = <span class=\"keyword\">default</span>;</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Network::addLayer</span><span class=\"params\">(<span class=\"type\">const</span> Layer &amp;layer)</span> </span>&#123;  </span><br><span class=\"line\">    layers.<span class=\"built_in\">push_back</span>(layer);  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<p>The fit method is gonna</p>\n<ul>\n<li>take input consisting of actual value (Y) and its label (X)</li>\n<li>the training time, epoch number</li>\n<li>the learning rate, how much of the gradient we apply to the weights and bias</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Network::fit</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;xTrain, <span class=\"type\">const</span> Matrix &amp;yTrain, <span class=\"type\">int</span> numberOfEpoch, <span class=\"type\">double</span> learningRate)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"comment\">// Check if the number of training data matches the number of labels  </span></span><br><span class=\"line\">    <span class=\"type\">int</span> trainingSize = xTrain.<span class=\"built_in\">rows</span>();  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (trainingSize != yTrain.<span class=\"built_in\">rows</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;The number of training data does not match the number of labels&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> epoch = <span class=\"number\">0</span>; epoch &lt; numberOfEpoch; epoch++) &#123;  </span><br><span class=\"line\">        <span class=\"type\">double</span> mse = <span class=\"number\">0</span>;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; trainingSize; ++i) &#123;  </span><br><span class=\"line\">            Matrix output = xTrain.<span class=\"built_in\">getRow</span>(i);  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;layer: layers) &#123;  </span><br><span class=\"line\">                output = layer.forwardPropagation(output);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">\t\t\t<span class=\"comment\">// We actually don&#x27;t need this for computation, but we can print it to see if the training is doing properly</span></span><br><span class=\"line\">            mse += Loss::<span class=\"built_in\">meanSquaredError</span>(output, yTrain.<span class=\"built_in\">getRow</span>(i));  </span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// The dE/dY</span></span><br><span class=\"line\">            Matrix err = Loss::<span class=\"built_in\">meanSquaredErrorDerivative</span>(output, yTrain.<span class=\"built_in\">getRow</span>(i));  </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//remeber it&#x27;s back not forward, so we do it in reverse</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> it = layers.<span class=\"built_in\">rbegin</span>(); it != layers.<span class=\"built_in\">rend</span>(); ++it) &#123;  </span><br><span class=\"line\">                err = it-&gt;<span class=\"built_in\">backwardPropagation</span>(err, learningRate);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">\t\t</span><br><span class=\"line\">        mse /= trainingSize;  </span><br><span class=\"line\">        std::cout &lt;&lt; <span class=\"string\">&quot;Epoch &quot;</span> &lt;&lt; epoch &lt;&lt; <span class=\"string\">&quot; MSE: &quot;</span> &lt;&lt; mse &lt;&lt; std::endl;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Predict\"><a href=\"#Predict\" class=\"headerlink\" title=\"Predict\"></a>Predict</h1><p>Prediction is rather easy, just propagate through all the layer and you will have the answer.</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Network::predict</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; xTest)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"type\">int</span> testingSize = xTest.<span class=\"built_in\">rows</span>();  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">result</span><span class=\"params\">(<span class=\"number\">0</span>, getOutputSize())</span></span>;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; testingSize; ++i) &#123;  </span><br><span class=\"line\">        Matrix output = xTest.<span class=\"built_in\">getRow</span>(i);  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;layer: layers) &#123;  </span><br><span class=\"line\">            output = layer.forwardPropagation(output);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        result.<span class=\"built_in\">addRow</span>(output);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Network::getOutputSize</span><span class=\"params\">()</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"comment\">// check if the layer is not an activation layer reversely  </span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> it = layers.<span class=\"built_in\">rbegin</span>(); it != layers.<span class=\"built_in\">rend</span>(); ++it) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!it-&gt;<span class=\"built_in\">isActivationLayer</span>()) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> it-&gt;<span class=\"built_in\">getOutputSize</span>();  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Test-the-code\"><a href=\"#Test-the-code\" class=\"headerlink\" title=\"Test the code\"></a>Test the code</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Networks/Network.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">//XOR gate</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> xTrain = <span class=\"built_in\">Matrix</span>(&#123;&#123;<span class=\"number\">0</span>,<span class=\"number\">0</span>&#125;, &#123;<span class=\"number\">0</span>,<span class=\"number\">1</span>&#125;, &#123;<span class=\"number\">1</span>,<span class=\"number\">0</span>&#125;, &#123;<span class=\"number\">1</span>,<span class=\"number\">1</span>&#125;&#125;);  </span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> yTrain = <span class=\"built_in\">Matrix</span>(&#123;&#123;<span class=\"number\">0</span>&#125;, &#123;<span class=\"number\">1</span>&#125;, &#123;<span class=\"number\">1</span>&#125;, &#123;<span class=\"number\">0</span>&#125;&#125;);  </span><br><span class=\"line\"></span><br><span class=\"line\">\tNetwork network = <span class=\"built_in\">Network</span>();  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">addLayer</span>(<span class=\"built_in\">Layer</span>(<span class=\"number\">2</span>, <span class=\"number\">3</span>));  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">addLayer</span>(<span class=\"built_in\">Layer</span>(ActivationFunctionType::TANH));  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">addLayer</span>(<span class=\"built_in\">Layer</span>(<span class=\"number\">3</span>,<span class=\"number\">1</span>));  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">addLayer</span>(<span class=\"built_in\">Layer</span>(ActivationFunctionType::TANH));  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">fit</span>(xTrain, yTrain, <span class=\"number\">100</span>, <span class=\"number\">0.1</span>);  </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> result = network.<span class=\"built_in\">predict</span>(xTrain);  </span><br><span class=\"line\">\tstd::cout &lt;&lt; result &lt;&lt; std::endl;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>The output</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Epoch 1 MSE: 0.48139</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">Epoch 99 MSE: 0.0133901</span><br><span class=\"line\">0.0314563, </span><br><span class=\"line\">0.886822, </span><br><span class=\"line\">0.819525, </span><br><span class=\"line\">-0.00120216, </span><br></pre></td></tr></table></figure>\n\n<p>Here we go, we get the fairly accurate output with a small epoch number. </p>\n<p>And we can also use the model on 2D images like the famous mnist dataset. The idea is convert the 2d image into a 1D array and create the initial activation layer with input size equals to the image size. However, it requires some image processing code which we will be doing in another day.</p>\n","site":{"data":{}},"excerpt":"<p>Ok this is the final bit. We need the network to wrap things up and do the training</p>\n<p>But again, we need some loss functions to determine the errors we gonna put into back propagation. I will use a simple MSE here. it’s simply the sum of square of the difference between predicted value and training value<br>$$MSE &#x3D; \\frac{\\sum^n_{i&#x3D;1} (Y_i - \\hat Y_i)^2}{n}$$</p>\n<p>And I guess you can directly see the derivative from here</p>","more":"<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Matrix.h&quot;</span>  </span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Loss</span> &#123;  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> <span class=\"type\">double</span> <span class=\"title\">meanSquaredError</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;predicted, <span class=\"type\">const</span> Matrix &amp;actual)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">static</span> Matrix <span class=\"title\">meanSquaredErrorDerivative</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;predicted, <span class=\"type\">const</span> Matrix &amp;actual)</span></span>;  </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Loss.h&quot;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">double</span> <span class=\"title\">Loss::meanSquaredError</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; predicted, <span class=\"type\">const</span> Matrix&amp; actual)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> ((predicted - actual) * (predicted - actual)).<span class=\"built_in\">mean</span>();  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Loss::meanSquaredErrorDerivative</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; predicted, <span class=\"type\">const</span> Matrix&amp; actual)</span> </span>&#123;  </span><br><span class=\"line\"><span class=\"comment\">//    std::cout&lt;&lt; &quot;Predicted: &quot; &lt;&lt; predicted &lt;&lt; std::endl;  </span></span><br><span class=\"line\"><span class=\"comment\">//    std::cout&lt;&lt; &quot;Actual: &quot; &lt;&lt; actual &lt;&lt; std::endl;  </span></span><br><span class=\"line\"><span class=\"comment\">//    std::cout&lt;&lt; &quot;Actual size&quot; &lt;&lt; actual.size() &lt;&lt; std::endl;  </span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (predicted - actual)*<span class=\"number\">2</span>/actual.<span class=\"built_in\">size</span>();  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Network\"><a href=\"#Network\" class=\"headerlink\" title=\"Network\"></a>Network</h1><p>Now finally, the ending bits. </p>\n<ul>\n<li>The network is gonna take layers and stack them in order</li>\n<li>The network is gonna fit(train), aka change the weights and bias, based on input data</li>\n<li>and it’s need to predict</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;vector&gt;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;../Layers/Layer.h&quot;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;../Math/Loss.h&quot;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Network</span> &#123;  </span><br><span class=\"line\"><span class=\"keyword\">public</span>:  </span><br><span class=\"line\">    <span class=\"built_in\">Network</span>(): <span class=\"built_in\">layers</span>(std::<span class=\"built_in\">vector</span>&lt;Layer&gt;()) &#123;&#125;;  </span><br><span class=\"line\">    ~<span class=\"built_in\">Network</span>();  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">addLayer</span><span class=\"params\">(<span class=\"type\">const</span> Layer&amp; layer)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">fit</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;xTrain, <span class=\"type\">const</span> Matrix &amp;yTrain, <span class=\"type\">int</span> numberOfEpoch, <span class=\"type\">double</span> learningRate)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">predict</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; xTest)</span></span>;  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">getOutputSize</span><span class=\"params\">()</span></span>;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"keyword\">private</span>:  </span><br><span class=\"line\">    std::vector&lt;Layer&gt; layers;  </span><br><span class=\"line\">  </span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Add-layers\"><a href=\"#Add-layers\" class=\"headerlink\" title=\"Add layers\"></a>Add layers</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Network.h&quot;</span>  </span></span><br><span class=\"line\">  </span><br><span class=\"line\">Network::~<span class=\"built_in\">Network</span>() = <span class=\"keyword\">default</span>;</span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Network::addLayer</span><span class=\"params\">(<span class=\"type\">const</span> Layer &amp;layer)</span> </span>&#123;  </span><br><span class=\"line\">    layers.<span class=\"built_in\">push_back</span>(layer);  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<p>The fit method is gonna</p>\n<ul>\n<li>take input consisting of actual value (Y) and its label (X)</li>\n<li>the training time, epoch number</li>\n<li>the learning rate, how much of the gradient we apply to the weights and bias</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">Network::fit</span><span class=\"params\">(<span class=\"type\">const</span> Matrix &amp;xTrain, <span class=\"type\">const</span> Matrix &amp;yTrain, <span class=\"type\">int</span> numberOfEpoch, <span class=\"type\">double</span> learningRate)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"comment\">// Check if the number of training data matches the number of labels  </span></span><br><span class=\"line\">    <span class=\"type\">int</span> trainingSize = xTrain.<span class=\"built_in\">rows</span>();  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (trainingSize != yTrain.<span class=\"built_in\">rows</span>()) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">throw</span> std::<span class=\"built_in\">invalid_argument</span>(<span class=\"string\">&quot;The number of training data does not match the number of labels&quot;</span>);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> epoch = <span class=\"number\">0</span>; epoch &lt; numberOfEpoch; epoch++) &#123;  </span><br><span class=\"line\">        <span class=\"type\">double</span> mse = <span class=\"number\">0</span>;  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; trainingSize; ++i) &#123;  </span><br><span class=\"line\">            Matrix output = xTrain.<span class=\"built_in\">getRow</span>(i);  </span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;layer: layers) &#123;  </span><br><span class=\"line\">                output = layer.forwardPropagation(output);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">\t\t\t<span class=\"comment\">// We actually don&#x27;t need this for computation, but we can print it to see if the training is doing properly</span></span><br><span class=\"line\">            mse += Loss::<span class=\"built_in\">meanSquaredError</span>(output, yTrain.<span class=\"built_in\">getRow</span>(i));  </span><br><span class=\"line\">\t</span><br><span class=\"line\">\t\t\t<span class=\"comment\">// The dE/dY</span></span><br><span class=\"line\">            Matrix err = Loss::<span class=\"built_in\">meanSquaredErrorDerivative</span>(output, yTrain.<span class=\"built_in\">getRow</span>(i));  </span><br><span class=\"line\"></span><br><span class=\"line\">\t\t\t<span class=\"comment\">//remeber it&#x27;s back not forward, so we do it in reverse</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> it = layers.<span class=\"built_in\">rbegin</span>(); it != layers.<span class=\"built_in\">rend</span>(); ++it) &#123;  </span><br><span class=\"line\">                err = it-&gt;<span class=\"built_in\">backwardPropagation</span>(err, learningRate);  </span><br><span class=\"line\">            &#125;  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">\t\t</span><br><span class=\"line\">        mse /= trainingSize;  </span><br><span class=\"line\">        std::cout &lt;&lt; <span class=\"string\">&quot;Epoch &quot;</span> &lt;&lt; epoch &lt;&lt; <span class=\"string\">&quot; MSE: &quot;</span> &lt;&lt; mse &lt;&lt; std::endl;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<h1 id=\"Predict\"><a href=\"#Predict\" class=\"headerlink\" title=\"Predict\"></a>Predict</h1><p>Prediction is rather easy, just propagate through all the layer and you will have the answer.</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">Matrix <span class=\"title\">Network::predict</span><span class=\"params\">(<span class=\"type\">const</span> Matrix&amp; xTest)</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"type\">int</span> testingSize = xTest.<span class=\"built_in\">rows</span>();  </span><br><span class=\"line\">    <span class=\"function\">Matrix <span class=\"title\">result</span><span class=\"params\">(<span class=\"number\">0</span>, getOutputSize())</span></span>;  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; testingSize; ++i) &#123;  </span><br><span class=\"line\">        Matrix output = xTest.<span class=\"built_in\">getRow</span>(i);  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> &amp;layer: layers) &#123;  </span><br><span class=\"line\">            output = layer.forwardPropagation(output);  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\">        result.<span class=\"built_in\">addRow</span>(output);  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> result;  </span><br><span class=\"line\">&#125;  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">Network::getOutputSize</span><span class=\"params\">()</span> </span>&#123;  </span><br><span class=\"line\">    <span class=\"comment\">// check if the layer is not an activation layer reversely  </span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> it = layers.<span class=\"built_in\">rbegin</span>(); it != layers.<span class=\"built_in\">rend</span>(); ++it) &#123;  </span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!it-&gt;<span class=\"built_in\">isActivationLayer</span>()) &#123;  </span><br><span class=\"line\">            <span class=\"keyword\">return</span> it-&gt;<span class=\"built_in\">getOutputSize</span>();  </span><br><span class=\"line\">        &#125;  </span><br><span class=\"line\">    &#125;  </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Test-the-code\"><a href=\"#Test-the-code\" class=\"headerlink\" title=\"Test the code\"></a>Test the code</h1><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span>  </span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&quot;Networks/Network.h&quot;</span></span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span> </span>&#123;</span><br><span class=\"line\">\t<span class=\"comment\">//XOR gate</span></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> xTrain = <span class=\"built_in\">Matrix</span>(&#123;&#123;<span class=\"number\">0</span>,<span class=\"number\">0</span>&#125;, &#123;<span class=\"number\">0</span>,<span class=\"number\">1</span>&#125;, &#123;<span class=\"number\">1</span>,<span class=\"number\">0</span>&#125;, &#123;<span class=\"number\">1</span>,<span class=\"number\">1</span>&#125;&#125;);  </span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> yTrain = <span class=\"built_in\">Matrix</span>(&#123;&#123;<span class=\"number\">0</span>&#125;, &#123;<span class=\"number\">1</span>&#125;, &#123;<span class=\"number\">1</span>&#125;, &#123;<span class=\"number\">0</span>&#125;&#125;);  </span><br><span class=\"line\"></span><br><span class=\"line\">\tNetwork network = <span class=\"built_in\">Network</span>();  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">addLayer</span>(<span class=\"built_in\">Layer</span>(<span class=\"number\">2</span>, <span class=\"number\">3</span>));  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">addLayer</span>(<span class=\"built_in\">Layer</span>(ActivationFunctionType::TANH));  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">addLayer</span>(<span class=\"built_in\">Layer</span>(<span class=\"number\">3</span>,<span class=\"number\">1</span>));  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">addLayer</span>(<span class=\"built_in\">Layer</span>(ActivationFunctionType::TANH));  </span><br><span class=\"line\">\tnetwork.<span class=\"built_in\">fit</span>(xTrain, yTrain, <span class=\"number\">100</span>, <span class=\"number\">0.1</span>);  </span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">auto</span> result = network.<span class=\"built_in\">predict</span>(xTrain);  </span><br><span class=\"line\">\tstd::cout &lt;&lt; result &lt;&lt; std::endl;</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>The output</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Epoch 1 MSE: 0.48139</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">...</span><br><span class=\"line\">Epoch 99 MSE: 0.0133901</span><br><span class=\"line\">0.0314563, </span><br><span class=\"line\">0.886822, </span><br><span class=\"line\">0.819525, </span><br><span class=\"line\">-0.00120216, </span><br></pre></td></tr></table></figure>\n\n<p>Here we go, we get the fairly accurate output with a small epoch number. </p>\n<p>And we can also use the model on 2D images like the famous mnist dataset. The idea is convert the 2d image into a 1D array and create the initial activation layer with input size equals to the image size. However, it requires some image processing code which we will be doing in another day.</p>"}],"PostAsset":[{"_id":"source/_posts/Gallery/Cat/Funny/cateye.jpg","slug":"cateye.jpg","post":"clanvcfhm000o7bsb8zmz38mg","modified":0,"renderable":0},{"_id":"source/_posts/Gallery/Cat/Funny/greasycat.jpg","slug":"greasycat.jpg","post":"clanvcfhm000o7bsb8zmz38mg","modified":0,"renderable":0}],"PostCategory":[{"post_id":"clanvcfhe00017bsbfe457cxc","category_id":"clanvcfhh00047bsb47q9auiv","_id":"clanvcfhk000b7bsbemjo43jm"},{"post_id":"clanvcfhg00037bsb15ui6tgh","category_id":"clanvcfhh00047bsb47q9auiv","_id":"clanvcfhk000d7bsb3h5xci5t"},{"post_id":"clanvcfhi00077bsb4vg47tup","category_id":"clanvcfhh00047bsb47q9auiv","_id":"clanvcfhl000g7bsb4g75dbg2"},{"post_id":"clanvcfhm000o7bsb8zmz38mg","category_id":"clanvcfhm000p7bsbashy10a6","_id":"clanvcfhn000r7bsb55t95tl8"},{"post_id":"clanvcfhm000o7bsb8zmz38mg","category_id":"clanvcfhn000q7bsb6dnha1fg","_id":"clanvcfhn000s7bsbcsba79ac"},{"post_id":"clbo42yi10000o2sbhexn3dd3","category_id":"clbo3zivi0001jhsba6ho9lbw","_id":"clbo42yi50003o2sb9or8doe4"},{"post_id":"clbr3qe7p0000fjsba2v697fw","category_id":"clbo3zivi0001jhsba6ho9lbw","_id":"clbr3qe7r0003fjsb4jwh96ws"},{"post_id":"clbr3s8ga0002fzsbc55o7yvj","category_id":"clbo3zivi0001jhsba6ho9lbw","_id":"clbr3s8pd0003fzsb9rjgda9m"},{"post_id":"clbr5wgj40000aosb5ltu6ydw","category_id":"clbo3zivi0001jhsba6ho9lbw","_id":"clbr5wgj50004aosb5x456xcx"}],"PostTag":[{"post_id":"clanvcfhe00017bsbfe457cxc","tag_id":"clanvcfhi00057bsb70fb5r9v","_id":"clanvcfhl000f7bsbezo30hq5"},{"post_id":"clanvcfhe00017bsbfe457cxc","tag_id":"clanvcfhj00097bsb7j3j8i0l","_id":"clanvcfhl000h7bsb5qzf25ls"},{"post_id":"clanvcfhe00017bsbfe457cxc","tag_id":"clanvcfhk000c7bsbejyd07rw","_id":"clanvcfhl000j7bsb80io18aw"},{"post_id":"clanvcfhi00077bsb4vg47tup","tag_id":"clanvcfhi00057bsb70fb5r9v","_id":"clanvcfhl000l7bsbcvk20ypi"},{"post_id":"clanvcfhi00077bsb4vg47tup","tag_id":"clanvcfhl000i7bsb8atobv1c","_id":"clanvcfhl000m7bsbbuundm2m"},{"post_id":"clanvcfhi00077bsb4vg47tup","tag_id":"clanvcfhl000k7bsb4jul7fx4","_id":"clanvcfhl000n7bsb4uq76dvv"},{"post_id":"clbo42yi10000o2sbhexn3dd3","tag_id":"clbo3zivj0002jhsbb2nb2qvm","_id":"clbo42yi40001o2sb1ij55eik"},{"post_id":"clbo42yi10000o2sbhexn3dd3","tag_id":"clanvcfhk000c7bsbejyd07rw","_id":"clbo42yi50002o2sb74offf7j"},{"post_id":"clbo42yi10000o2sbhexn3dd3","tag_id":"clbo3zivj0003jhsb4tuu93os","_id":"clbo42yi60004o2sbalzz8b50"},{"post_id":"clbo42yi10000o2sbhexn3dd3","tag_id":"clbo3zivk0005jhsbe2ymhnxw","_id":"clbo42yi60005o2sbd7b01jzi"},{"post_id":"clbr3qe7p0000fjsba2v697fw","tag_id":"clbo3zivj0002jhsbb2nb2qvm","_id":"clbr3qe7r0001fjsb66dlhxpw"},{"post_id":"clbr3qe7p0000fjsba2v697fw","tag_id":"clanvcfhk000c7bsbejyd07rw","_id":"clbr3qe7r0002fjsb1se10x1p"},{"post_id":"clbr3qe7p0000fjsba2v697fw","tag_id":"clbo3zivj0003jhsb4tuu93os","_id":"clbr3qe7r0004fjsbebov309x"},{"post_id":"clbr3qe7p0000fjsba2v697fw","tag_id":"clbo3zivk0005jhsbe2ymhnxw","_id":"clbr3qe7r0005fjsbc6f149kh"},{"post_id":"clbr3s8ga0002fzsbc55o7yvj","tag_id":"clbo3zivj0002jhsbb2nb2qvm","_id":"clbr5wgj50001aosb4py0ec2z"},{"post_id":"clbr3s8ga0002fzsbc55o7yvj","tag_id":"clanvcfhk000c7bsbejyd07rw","_id":"clbr5wgj50002aosbcaa27j7s"},{"post_id":"clbr3s8ga0002fzsbc55o7yvj","tag_id":"clbo3zivj0003jhsb4tuu93os","_id":"clbr5wgj50003aosbefvke4x7"},{"post_id":"clbr3s8ga0002fzsbc55o7yvj","tag_id":"clbo3zivk0005jhsbe2ymhnxw","_id":"clbr5wgj60005aosbf2n11p3o"},{"post_id":"clbr5wgj40000aosb5ltu6ydw","tag_id":"clbo3zivj0002jhsbb2nb2qvm","_id":"clbr5wgj60006aosb6cnj5x3p"},{"post_id":"clbr5wgj40000aosb5ltu6ydw","tag_id":"clanvcfhk000c7bsbejyd07rw","_id":"clbr5wgj60007aosb5f91aqmj"},{"post_id":"clbr5wgj40000aosb5ltu6ydw","tag_id":"clbo3zivj0003jhsb4tuu93os","_id":"clbr5wgj60008aosbec9te54b"},{"post_id":"clbr5wgj40000aosb5ltu6ydw","tag_id":"clbo3zivk0005jhsbe2ymhnxw","_id":"clbr5wgj60009aosb2z1ybnxy"}],"Tag":[{"name":"archlinux","_id":"clanvcfhi00057bsb70fb5r9v"},{"name":"installation","_id":"clanvcfhj00097bsb7j3j8i0l"},{"name":"deep-learning","_id":"clanvcfhk000c7bsbejyd07rw"},{"name":"zoom","_id":"clanvcfhl000i7bsb8atobv1c"},{"name":"solved","_id":"clanvcfhl000k7bsb4jul7fx4"},{"name":"c++","_id":"clbo3zivj0002jhsbb2nb2qvm"},{"name":"machine-learning","_id":"clbo3zivj0003jhsb4tuu93os"},{"name":"neural-network","_id":"clbo3zivk0005jhsbe2ymhnxw"}]}}